{"doi": "https://doi.org/10.3847/2041-8213/ab50c5", "title": "PSR J0030+0451 Mass and Radius from NICER Data and Implications for the Properties of Neutron Star Matter", "language": "en", "created_date": "2019-12-26T00:00:00", "publication_year": 2019, "cited_by_count": 1577, "authors": ["Bogdanov S", "Y. Soong", "G. Prigozhin", "T. Enoto", "Paul S. Ray", "S. Guillot", "Z. Arzoumanian", "R. Foster", "S. M. Morsink", "K S Wood", "T. E. Strohmayer", "F. K. Lamb", "S. Mahmoodifar", "A. J. Dittmann", "K. C. Gendreau", "M.C. Miller", "J.M. Lattimer", "T Okajima", "W.C.G. Ho", "A. K. Harding", "Renee M. Ludlam"], "abstract": "Neutron stars are not only of astrophysical interest, but are also of great interest to nuclear physicists, because their attributes can be used to determine the properties of the dense matter in their cores. One of the most informative approaches for determining the equation of state of this dense matter is to measure both a star's equatorial circumferential radius $R_e$ and its gravitational mass $M$. Here we report estimates of the mass and radius of the isolated 205.53 Hz millisecond pulsar PSR J0030+0451 obtained using a Bayesian inference approach to analyze its energy-dependent thermal X-ray waveform, which was observed using the Neutron Star Interior Composition Explorer (NICER). This approach is thought to be less subject to systematic errors than other approaches for estimating neutron star radii. We explored a variety of emission patterns on the stellar surface. Our best-fit model has three oval, uniform-temperature emitting spots and provides an excellent description of the pulse waveform observed using NICER. The radius and mass estimates given by this model are $R_e = 13.02^{+1.24}_{-1.06}$ km and $M = 1.44^{+0.15}_{-0.14}\\ M_\\odot$ (68%). The independent analysis reported in the companion paper by Riley et al. (2019) explores different emitting spot models, but finds spot shapes and locations and estimates of $R_e$ and $M$ that are consistent with those found in this work. We show that our measurements of $R_e$ and $M$ for PSR J0030$+$0451 improve the astrophysical constraints on the equation of state of cold, catalyzed matter above nuclear saturation density.", "arxiv_id": "1912.05705v1", "research_question": "How do simultaneous measurements of a neutron star’s mass and radius constrain the equation of state of dense nuclear matter?"}
{"doi": "https://doi.org/10.1038/s41568-019-0216-7", "title": "The PI3K–AKT network at the interface of oncogenic signalling and cancer metabolism", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1948, "authors": ["Brendan D. Manning", "Gerta Hoxhaj"], "abstract": "Although cancer is known to be characterized by several unifying biological hallmarks, systems biology has had limited success in identifying molecular signatures present in in all types of cancer. The current availability of rich data sets from many different cancer types provides an opportunity for thorough computational data mining in search of such common patterns. Here we report the identification of 18 \"pan-cancer\" molecular signatures resulting from analysis of data sets containing values from mRNA expression, microRNA expression, DNA methylation, and protein activity, from twelve different cancer types. The membership of many of these signatures points to particular biological mechanisms related to cancer progression, suggesting that they represent important attributes of cancer in need of being elucidated for potential applications in diagnostic, prognostic and therapeutic products applicable to multiple cancer types.", "arxiv_id": "1306.2584v2", "research_question": "What criteria and validation steps are used to determine whether a molecular signature is truly common across many cancer types and clinically useful for diagnosis or prognosis?"}
{"doi": "https://doi.org/10.1016/s1474-4422(19)30356-4", "title": "The gut microbiome in neurological disorders", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1186, "authors": ["Kenneth J. O’Riordan", "Veronica L. Peterson", "John F. Cryan", "Timothy G. Dinan", "Kiran Sandhu"], "abstract": "Recent attacks of various viruses with having deep and extensive impact at a global scale has warranted that microbiome be studied extensively and in a robust analytic framework. Microbiome typically refers to the collective genomes of such organisms, although it could also refer to the collection of the organisms by themselves. Here we provide an overview of statistical techniques that are useful in analysing such data.", "arxiv_id": "2303.16722v1", "research_question": "What are the main statistical challenges unique to analyzing microbiome sequencing data, and what best-practice approaches are recommended to address them?"}
{"doi": "https://doi.org/10.1103/physrevx.9.041015", "title": "Symmetry and Topology in Non-Hermitian Physics", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1214, "authors": ["Ken Shiozaki", "Masatoshi Sato", "Kohei Kawabata", "Masahito Ueda"], "abstract": "The multipartite non-Hermitian Su-Schrieffer-Heeger model is explored as a prototypical example of one-dimensional systems with several sublattice sites for unveiling intriguing insulating and metallic phases with no Hermitian counterparts. These phases are characterized by composite cyclic loops of multiple complex-energy bands encircling single or multiple exceptional points (EPs) on the parametric space of real and imaginary energy. We show the topology of these composite loops is similar to well-known topological objects like Möbius strips and Penrose triangles, and can be quantified by a nonadiabatic cyclic geometric phase which includes contributions only from the participating bands. We analytically derive a complete phase diagram with the phase boundaries of the model. We further examine the connection between the encircling of multiple EPs by complex-energy bands on parametric space and associated topology.", "arxiv_id": "2201.12297v2", "research_question": "How can encircling multiple exceptional points and the resulting nonadiabatic cyclic geometric phases be detected experimentally in one-dimensional non-Hermitian systems?"}
{"doi": "https://doi.org/10.1186/s13059-019-1727-y", "title": "Performance of neural network basecalling tools for Oxford Nanopore sequencing", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 3064, "authors": ["Ryan R. Wick", "Kathryn E. Holt", "Louise M. Judd"], "abstract": "Random Neural Networks (RNNs) are a class of Neural Networks (NNs) that can also be seen as a specific type of queuing network. They have been successfully used in several domains during the last 25 years, as queuing networks to analyze the performance of resource sharing in many engineering areas, as learning tools and in combinatorial optimization, where they are seen as neural systems, and also as models of neurological aspects of living beings. In this article we focus on their learning capabilities, and more specifically, we present a practical guide for using the RNN to solve supervised learning problems. We give a general description of these models using almost indistinctly the terminology of Queuing Theory and the neural one. We present the standard learning procedures used by RNNs, adapted from similar well-established improvements in the standard NN field. We describe in particular a set of learning algorithms covering techniques based on the use of first order and, then, of second order derivatives. We also discuss some issues related to these objects and present new perspectives about their use in supervised learning problems. The tutorial describes their most relevant applications, and also provides a large bibliography.", "arxiv_id": "1609.04846v1", "research_question": "How do Random Neural Networks differ from conventional neural network models in their architecture and training dynamics, and in what kinds of supervised learning problems are they most advantageous to use?"}
{"doi": "https://doi.org/10.1016/s0140-6736(19)31149-3", "title": "Dulaglutide and cardiovascular outcomes in type 2 diabetes (REWIND): a double-blind, randomised placebo-controlled trial", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 2683, "authors": ["Rao Ps", "Paulina Virginia Lanchiotti", "Miguel Hominal", "Luis R Cartasegna", "Leanne Dyal", "Alejandra Izzicupo", "Eduardo Amoroso", "Alejandra Camino", "C. Perez Garcia", "Ricardo López Santi", "Fernando Laņas", "Stephanie Hall", "Theodora Temelkova‐Kurktschiev", "Lars Rydén", "Miguel Farah", "Patricio López‐Jaramillo", "Nana Pogosova", "Álvaro Avezum", "Rafael Díaz", "Pedro Godoy Bolzán", "Ernesto Muñoz", "Alberto Ocaña", "Claudio Gerbaudo", "Cristian Guridi", "María del Rocío Cantero", "Maria Langhe", "Mercedes Abella", "Jonathan E. Shaw", "Marcelo Garrido", "Mátyás Keltai", "Osvaldo Costamagna", "Gabriel Bolobanich", "Paula Anadon", "María Antonietta Barbieri", "Nicolae Hâncu", "José Antonio Fuentealba", "Miguel Bustamante Labarta", "G.C. Wong", "Adrian D. Hrabar", "Ines Bartolacci", "Petr Janský", "Raúl Barcudi", "Claudio Fabián Dituro", "Carlos Alberto Cuneo", "Helen M. Colhoun", "Valdis Pīrāgs", "Namsik Chung", "Jeffrey S. Riesmeyer", "Victoria Conosciuto", "Mark Lakshmanan", "Adrián Ingaramo", "Sofia Dean", "Adriana Cristina Ferrari", "Florencia Fernandez", "Sandra Cusimano", "M Hanefeld", "Eduardo Hasbani", "Mario Krynski", "William C. Cushman", "Maria Carignano", "Oscar Gómez Vilamajó", "José Cuello-Navarro", "Patricia Flammia", "Ana Paula Giotto", "Edward Franek", "Narcisa Gutierrez Garrido", "Shaun Holt", "Prem Pais", "Sandra Almagro", "Mariana Lagrutta", "Charles Atisso", "Ignacio Conget", "Fernando Guerlloy", "Elizabeth Andreu", "Víctor Commendatore", "Ruben Garcia Duran", "Guillermo Aristimuño", "Rodolfo Gavícola", "Matthew C. Riddle", "Karina Beatriz Gallardo", "Andrea Alebuena", "Wayne Huey‐Herng Sheu", "Javier Llanos", "Hertzel C. Gerstein", "Gilles R. Dagenais", "Denis Xavier", "Maria Arzadun", "Peter Raubenheimer", "Andrea Domínguez", "Claudia A. Crespo", "Marcela Cipullo", "Veronica Leonard", "Jan Basile", "B. Bustos", "Lawrence A. Leiter", "Anselmo P. Bordonava", "Alberto Caccavo", "Graciela Gilli", "Jeffrey L. Probstfield", "Sonia Hermida"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of the branching fractions for rare B meson decays into muon pairs constrain potential extensions of the Standard Model, and what types of new-physics scenarios are most affected by these measurements?"}
{"doi": "https://doi.org/10.18653/v1/d19-1371", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 2777, "authors": ["Kyle Lo", "Iz Beltagy", "Arman Cohan"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare decays such as B_s^0 → μ^+μ^- constrain theories beyond the Standard Model, and which classes of new-physics models are most strongly affected by these measurements?"}
{"doi": "https://doi.org/10.1109/iccv.2019.00612", "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 4293, "authors": ["Sangdoo Yun", "Youngjoon Yoo", "Dongyoon Han", "Junsuk Choe", "Seong Joon Oh", "Sanghyuk Chun"], "abstract": "The 2023-2032 Planetary Science and Astrobiology Decadal Survey Origins, Worlds, and Life recommended that \"NASA develop scientific exploration strategies, as it has for Mars, in areas of broad scientific importance, e.g., Venus... that have an increasing number of U.S. missions and international collaboration opportunities\" (OWL, p.22-10). In NASA's initial responses to that Decadal Survey, the agency asserted that \"...specific scientific exploration strategies should be community generated by bodies such as the Analysis Groups,\" thus placing the onus on the planetary community to generate and support these exploration strategies. In late 2022, the Venus Exploration Analysis Group began a project to develop a new exploration strategy for Venus, reflecting the 2021 selections of the VERITAS, DAVINCI, and EnVision missions and the sweeping comparative planetology recommendations relevant to Venus in Origins, Worlds, and Life.\n  This is that strategy.\n  Taking a broad look at the scientific, technological, and programmatic advances required to address the key outstanding questions that Venus poses, and predicated on VERITAS, DAVINCI, and EnVision flying as planned in the early 2030s, this report outlines a set of actions available to NASA, VEXAG, and the planetary science community at large to establish a sustained program of Venus exploration in the years and decades ahead. Key to this approach is recognizing Venus as a unique setting where multiple, cross-disciplinary, Decadal-level planetary, Earth, heliophysics, and exoplanet science questions can be addressed, as well as being a worthy target of exploration in its own right.\n  This report offers Assessments of the current state of Venus exploration, and Actions for the U.S. and international Venus community, as well as NASA, to consider. This strategy is a living document and should be updated as warranted.", "arxiv_id": "2412.06830v1", "research_question": "What are the major technological and engineering challenges to conducting sustained, long-term exploration of Venus, and what approaches or technologies are being considered to address them?"}
{"doi": "https://doi.org/10.2112/coas", "title": "Journal of Coastal Research", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1863, "authors": [], "abstract": "Vibrio cholerae is a globally distributed water-borne pathogen that causes severe diarrheal disease and mortality, with current outbreaks as part of the seventh pandemic. Further understanding of the role of environmental factors in potential pathogen distribution and corresponding V. cholerae disease transmission over time and space is urgently needed to target surveillance of cholera and other climate and water-sensitive diseases. We used an ecological niche model (ENM) to identify environmental variables associated with V. cholerae presence in marine environments, to project a global model of V. cholerae distribution in ocean waters under current and future climate scenarios. We generated an ENM using published reports of V. cholerae in seawater and freely available remotely sensed imagery. Models indicated that factors associated with V. cholerae presence included chlorophyll-a, pH, and sea surface temperature (SST), with chlorophyll-a demonstrating the greatest explanatory power from variables selected for model calibration. We identified specific geographic areas for potential V. cholerae distribution. Coastal Bangladesh, where cholera is endemic, was found to be environmentally similar to coastal areas in Latin America. In a conservative climate change scenario, we observed a predicted increase in areas with environmental conditions suitable for V. cholerae. Findings highlight the potential for vulnerability maps to inform cholera surveillance, early warning systems, and disease prevention and control.", "arxiv_id": "1506.01757v1", "research_question": "How can environmental and satellite-derived data be used to build operational early-warning systems for cholera and other waterborne diseases, and what are the main limitations and challenges in implementing such systems in low-resource settings?"}
{"doi": "https://doi.org/10.1215/9781478007098", "title": "Intersectionality as Critical Social Theory", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1011, "authors": ["Patrícia Hill Collins"], "abstract": "All online sharing systems gather data that reflects users' collective behaviour and their shared activities. This data can be used to extract different kinds of relationships, which can be grouped into layers, and which are basic components of the multidimensional social network proposed in the paper. The layers are created on the basis of two types of relations between humans, i.e. direct and object-based ones which respectively correspond to either social or semantic links between individuals. For better understanding of the complexity of the social network structure, layers and their profiles were identified and studied on two, spanned in time, snapshots of the Flickr population. Additionally, for each layer, a separate strength measure was proposed. The experiments on the Flickr photo sharing system revealed that the relationships between users result either from semantic links between objects they operate on or from social connections of these users. Moreover, the density of the social network increases in time. The second part of the study is devoted to building a social recommender system that supports the creation of new relations between users in a multimedia sharing system. Its main goal is to generate personalized suggestions that are continuously adapted to users' needs depending on the personal weights assigned to each layer in the multidimensional social network. The conducted experiments confirmed the usefulness of the proposed model.", "arxiv_id": "1303.0093v1", "research_question": "How can a recommender system effectively combine multiple types of social and semantic relationships (different network layers) to generate personalized suggestions, and how are the relative importance or weights of those relationship types determined and adapted over time?"}
{"doi": "https://doi.org/10.1007/s00018-019-03351-7", "title": "Tumor angiogenesis: causes, consequences, challenges and opportunities", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1766, "authors": ["Mohanraj Ramachandran", "Roberta Lugano", "Anna Dimberg"], "abstract": "The Lustre parallel file system has been widely adopted by high-performance computing (HPC) centers as an effective system for managing large-scale storage resources. Lustre achieves unprecedented aggregate performance by parallelizing I/O over file system clients and storage targets at extreme scales. Today, 7 out of 10 fastest supercomputers in the world use Lustre for high-performance storage. To date, Lustre development has focused on improving the performance and scalability of large-scale scientific workloads. In particular, large-scale checkpoint storage and retrieval, which is characterized by bursty I/O from coordinated parallel clients, has been the primary driver of Lustre development over the last decade. With the advent of extreme scale computing and Big Data computing, many HPC centers are seeing increased user interest in running diverse workloads that place new demands on Lustre. In March 2015, the International Workshop on the Lustre Ecosystem: Challenges and Opportunities was held in Annapolis, Maryland at the Historic Inns of Annapolis Governor Calvert House. This workshop series is intended to help explore improvements in the performance and flexibility of Lustre for supporting diverse application workloads. The 2015 workshop was the inaugural edition, and the goal was to initiate a discussion on the open challenges associated with enhancing Lustre for diverse applications, the technological advances necessary, and the associated impacts to the Lustre ecosystem. The workshop program featured a day of tutorials and a day of technical paper presentations.", "arxiv_id": "1506.05323v1", "research_question": "What are the main challenges in adapting a large-scale parallel file system to efficiently support both traditional bursty HPC checkpointing workloads and modern diverse Big Data analytics workloads?"}
{"doi": "https://doi.org/10.1126/science.aau2027", "title": "Grand challenges in the science of wind energy", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1084, "authors": ["Daniel Laird", "Ola Carlson", "Charles Meneveau", "Julie K. Lundquist", "Lucy Y. Pao", "Katherine Dykes", "Joshua Paquette", "Amy Robertson", "Ryan Wiser", "Anna Maria Sempreviva", "James F. Manwell", "Xabier Munduate", "Andrew Clifton", "Ville Lehtomäki", "Peter F. Green", "Javier Sanz Rodrigo", "Michael Muskulus", "Johney B. Green", "Eric Lantz", "Patrick Moriarty", "Melinda Marquis", "Hannele Holttinen", "Jonathan Naughton", "J. Charles Smith", "Paul Veers", "Carlo L. Bottasso", "Joachim Peinke", "Aidan Tuohy", "Stephan Barth"], "abstract": "This white paper describes the LSST Dark Energy Science Collaboration (DESC), whose goal is the study of dark energy and related topics in fundamental physics with data from the Large Synoptic Survey Telescope (LSST). It provides an overview of dark energy science and describes the current and anticipated state of the field. It makes the case for the DESC by laying out a robust analytical framework for dark energy science that has been defined by its members and the comprehensive three-year work plan they have developed for implementing that framework. The analysis working groups cover five key probes of dark energy: weak lensing, large scale structure, galaxy clusters, Type Ia supernovae, and strong lensing. The computing working groups span cosmological simulations, galaxy catalogs, photon simulations and a systematic software and computational framework for LSST dark energy data analysis. The technical working groups make the connection between dark energy science and the LSST system. The working groups have close linkages, especially through the use of the photon simulations to study the impact of instrument design and survey strategy on analysis methodology and cosmological parameter estimation. The white paper describes several high priority tasks identified by each of the 16 working groups. Over the next three years these tasks will help prepare for LSST analysis, make synergistic connections with ongoing cosmological surveys and provide the dark energy community with state of the art analysis tools. Members of the community are invited to join the LSST DESC, according to the membership policies described in the white paper. Applications to sign up for associate membership may be made by submitting the Web form at http://www.slac.stanford.edu/exp/lsst/desc/signup.html with a short statement of the work they wish to pursue that is relevant to the LSST DESC.", "arxiv_id": "1211.0310v1", "research_question": "How do the main cosmological probes—weak gravitational lensing, large-scale structure, galaxy clusters, Type Ia supernovae, and strong lensing—complement one another in constraining dark energy, and what are the primary systematic uncertainties unique to each probe?"}
{"doi": "https://doi.org/10.1016/s0140-6736(18)32822-8", "title": "The Global Syndemic of Obesity, Undernutrition, and Climate Change: The Lancet Commission report", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 2750, "authors": ["Steven Allender", "Mario Herrero", "Boyd Swinburn", "Corinna Hawkes", "Gareth J. Morgan", "Ali Tootee", "An Pan", "Victor Keihan Rodrigues Matsudo", "Meera Shekar", "Phillip Baker", "Matt Kasman", "Sharon Friel", "Alejandro Calvillo", "Warren Smit", "Shifalika Goenka", "Gary Sacks", "William H. Dietz", "Harriet V. Kuhnlein", "Ross A. Hammond", "Tim Lobstein", "Bagher Larijani", "Wilma Waterlander", "Peter S. Hovmand", "Vincent Atkins", "G Simmons", "Olivier De Schutter", "Raji Devarajan", "Hannah Brinsden", "David Patterson", "Gerard Hastings", "Susanna Mills", "Vivica I. Kraak", "Mark Howden", "Ariadne Beatrice Kapetanaki", "Jessica Bogard", "Michael W. Long", "Majid Ezzati", "Alexandra B. Morshed", "Stefanie Vandevijvere", "Lindsay M. Jaacks", "Luke Wolfenden", "Patricia Nece", "Shiriki Kumanyika"], "abstract": "Assessments of impacts of climate change and future projections over the Indian region, have so far relied on a single regional climate model (RCM) - eg., the PRECIS RCM of the Hadley Centre, UK. While these assessments have provided inputs to various reports (e.g., INCCA 2010; NATCOMM2 2012), it is important to have an ensemble of climate projections drawn from multiple RCMs due to large uncertainties in regional-scale climate projections. Ensembles of multi-RCM projections driven under different perceivable socio-economic scenarios are required to capture the probable path of growth, and provide the behavior of future climate and impacts on various biophysical systems and economic sectors dependent on such systems.\n  The Centre for Climate Change Research, Indian Institute of Tropical Meteorology (CCCR-IITM) has generated an ensemble of high resolution downscaled projections of regional climate and monsoon over South Asia until 2100 for the Intergovernmental Panel for Climate Change (IPCC)using a RCM (ICTP-RegCM4) at 50 km horizontal resolution, by driving the regional model with lateral and lower boundary conditions from multiple global atmosphere-ocean coupled models from the Coupled Model Intercomparison Project Phase 5 (CMIP5). The future projections are based on three Representation Concentration Pathway (RCP) scenarios (viz., RCP2.6, RCP4.5, RCP8.5) of the IPCC.", "arxiv_id": "2012.10386v1", "research_question": "How do ensembles of regional climate model simulations reduce uncertainty in regional climate projections compared with relying on a single regional model, and what are the main remaining sources of uncertainty?"}
{"doi": "https://doi.org/10.1186/s12943-018-0928-4", "title": "Role of the tumor microenvironment in PD-L1/PD-1-mediated tumor immune escape", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1392, "authors": ["Fang Xiong", "Jie Wang", "Xiaoling Li", "Yong Li", "Bo Xiang", "Junshang Ge", "Xu Wu", "Xianjie Jiang", "Can Guo", "Ming Zhou", "Guiyuan Li", "Xiangying Deng", "Wei Xiong", "Jian Ma", "Zhaoyang Zeng"], "abstract": "Automated segmentation of kidneys and kidney tumors is an important step in quantifying the tumor's morphometrical details to monitor the progression of the disease and accurately compare decisions regarding the kidney tumor treatment. Manual delineation techniques are often tedious, error-prone and require expert knowledge for creating unambiguous representation of kidneys and kidney tumors segmentation. In this work, we propose an end-to-end boundary aware fully Convolutional Neural Networks (CNNs) for reliable kidney and kidney tumor semantic segmentation from arterial phase abdominal 3D CT scans. We propose a segmentation network consisting of an encoder-decoder architecture that specifically accounts for organ and tumor edge information by devising a dedicated boundary branch supervised by edge-aware loss terms. We have evaluated our model on 2019 MICCAI KiTS Kidney Tumor Segmentation Challenge dataset and our method has achieved dice scores of 0.9742 and 0.8103 for kidney and tumor repetitively and an overall composite dice score of 0.8923.", "arxiv_id": "1909.06684v1", "research_question": "What are the main challenges to deploying automated kidney and kidney tumor segmentation tools in routine clinical practice, and how can these tools be validated to ensure reliability across different scanners and patient populations?"}
{"doi": "https://doi.org/10.1016/j.compositesb.2019.107496", "title": "A review of recent research on bio-inspired structures and materials for energy absorption applications", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1024, "authors": ["Ngoc San Ha", "Guoxing Lu"], "abstract": "In the rapidly expanding field of two-dimensional materials, magnetic monolayers show great promise for the future applications in nanoelectronics, data storage, and sensing. The research in intrinsically magnetic two-dimensional materials mainly focuses on synthetic iodide and telluride based compounds, which inherently suffer from the lack of ambient stability. So far, naturally occurring layered magnetic materials have been vastly overlooked. These minerals offer a unique opportunity to explore air-stable complex layered systems with high concentration of local moment bearing ions. We demonstrate magnetic ordering in iron-rich two-dimensional phyllosilicates, focusing on mineral species of minnesotaite, annite, and biotite. These are naturally occurring van der Waals magnetic materials which integrate local moment baring ions of iron via magnesium/aluminium substitution in their octahedral sites. Due to self-inherent capping by silicate/aluminate tetrahedral groups, ultra-thin layers are air-stable. Chemical characterization, quantitative elemental analysis, and iron oxidation states were determined via Raman spectroscopy, wavelength disperse X-ray spectroscopy, X-ray absorption spectroscopy, and X-ray photoelectron spectroscopy. Superconducting quantum interference device magnetometry measurements were performed to examine the magnetic ordering. These layered materials exhibit paramagnetic or superparamagnetic characteristics at room temperature. At low temperature ferrimagnetic or antiferromagnetic ordering occurs, with the critical ordering temperature of 38.7 K for minnesotaite, 36.1 K for annite, and 4.9 K for biotite. In-field magnetic force microscopy on iron bearing phyllosilicates confirmed the paramagnetic response at room temperature, present down to monolayers.", "arxiv_id": "2304.06533v1", "research_question": "What structural or chemical features make naturally occurring layered minerals more air-stable than many synthetic two-dimensional magnetic compounds, and how do those features help preserve magnetic properties when the materials are thinned to monolayers?"}
{"doi": "https://doi.org/10.1080/13632434.2019.1596077", "title": "Seven strong claims about successful school leadership revisited", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1161, "authors": ["Alma Harris", "David Hopkins", "Kenneth Leithwood"], "abstract": "The recent explosion of false claims in social media and on the Web in general has given rise to a lot of manual fact-checking initiatives. Unfortunately, the number of claims that need to be fact-checked is several orders of magnitude larger than what humans can handle manually. Thus, there has been a lot of research aiming at automating the process. Interestingly, previous work has largely ignored the growing number of claims about images. This is despite the fact that visual imagery is more influential than text and naturally appears alongside fake news. Here we aim at bridging this gap. In particular, we create a new dataset for this problem, and we explore a variety of features modeling the claim, the image, and the relationship between the claim and the image. The evaluation results show sizable improvements over the baseline. We release our dataset, hoping to enable further research on fact-checking claims about images.", "arxiv_id": "1908.11722v1", "research_question": "What are the key challenges and approaches for automatically verifying whether a textual claim is supported, contradicted, or unrelated to a given image?"}
{"doi": "https://doi.org/10.1016/j.promfg.2019.06.089", "title": "An Overview on 3D Printing Technology: Technological, Materials, and Applications", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1809, "authors": ["Lee Te Chuan", "N. Shahrubudin", "Rohaizan Ramlan"], "abstract": "High-precision 3D printing technology opens to almost endless opportunities to design complex shapes present in tailored architected materials. The scope of this work is to review the latest studies regarding 3D printed lattice structures that involve the use of photopolymers fabricated by Material Jetting (MJ), with a focus on the widely used Polyjet and MultiJet techniques. The main aspects governing this printing process are introduced to determine their influence during the fabrication of 3D printed lattices. Performed experimental studies, considered assumptions, and constitutive models for the respective numerical simulations are analyzed. Furthermore, an overview of the latest extensively studied 3D printed architected lattice materials is exposed by emphasizing their achieved mechanical performances through the use of Ashby plots. Then, we highlight the advantages, limitations, and challenges of the material jetting technology to manufacture tunable architected materials for innovative devices, oriented to several engineering applications. Finally, possible approaches for future works and gaps to be covered by further research are indicated, including cost and environmental-related issues.", "arxiv_id": "2301.12634v1", "research_question": "What are the main factors that determine the mechanical performance and manufacturability of high-precision photopolymer 3D-printed lattice structures, and how can designers balance resolution, material properties, and printing constraints when optimizing for stiffness, strength, or energy absorption?"}
{"doi": "https://doi.org/10.1021/acs.chemrev.9b00248", "title": "Recent Advances in Electrocatalytic Hydrogen Evolution Using Nanoparticles", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 2794, "authors": ["Liangsheng Hu", "Lawrence Yoon Suk Lee", "Kwok‐Yin Wong", "Jing Zhu", "Pengxiang Zhao"], "abstract": "Studies of muonic hydrogen atoms and molecules have been performed traditionally in bulk targets of gas, liquid or solid. At TRIUMF, Canada's meson facility, we have developed a new type of target system using multilayer thin films of solid hydrogen, which provides a beam of muonic hydrogen atoms in vacuum. Using the time-of-flight of the muonic atoms, the energy-dependent information of muonic reactions are obtained in direct manner. We discuss some unique measurements enabled by the new technique, with emphasis on processes relevant to muon catalyzed fusion.", "arxiv_id": "0101007v1", "research_question": "What are the main advantages and technical challenges of producing and studying muonic hydrogen atoms in vacuum using thin solid hydrogen layers compared with traditional bulk gas, liquid, or solid targets?"}
{"doi": "https://doi.org/10.4103/aca.aca_157_18", "title": "Descriptive statistics and normality tests for statistical data", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 2242, "authors": ["Sweta Singh", "Anshul Gupta", "Prabhakar Mishra", "Chandra Mani Pandey", "Uttam Singh", "Amit Keshri"], "abstract": "Statistics has moved beyond the frequentist-Bayesian controversies of the past. Where does this leave our ability to interpret results? I suggest that a philosophy compatible with statistical practice, labeled here statistical pragmatism, serves as a foundation for inference. Statistical pragmatism is inclusive and emphasizes the assumptions that connect statistical models with observed data. I argue that introductory courses often mischaracterize the process of statistical inference and I propose an alternative \"big picture\" depiction.", "arxiv_id": "1106.2895v2", "research_question": "How can introductory statistics courses be redesigned to emphasize the role of model assumptions and pragmatic interpretation of results while still teaching core techniques?"}
{"doi": "https://doi.org/10.1038/s41586-019-1237-9", "title": "Multi-omics of the gut microbial ecosystem in inflammatory bowel diseases", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 2768, "authors": ["Janet Jansson", "Ramnik J. Xavier", "Mahadev Prasad", "Hera Vlamakis", "Kathleen D. Lake", "Carol J. Landers", "Colin Brislawn", "Jonathan Braun", "Jenny Sauk", "Subra Kugathasan", "Nadim J. Ajami", "Curtis Huttenhower", "Jason Lloyd‐Price", "Rob Knight", "Himel Mallick", "Elizabeth Andrews", "David Casero", "Ali Rahnavard", "Clary B. Clish", "Harland S. Winter", "Antonio González", "Dermot McGovern", "Lee A. Denson", "Joseph F. Petrosino", "Eric A. Franzosa", "Thaddeus S. Stappenbeck", "Cesar Arze", "Kevin S. Bonham", "Ashwin N. Ananthakrishnan", "Damian R. Plichta", "Yoshiki Vázquez‐Baeza", "A. Brantley Hall", "Dmitry Shungin", "Holly Courtney", "Richard White", "Thomas G. Graeber", "Julián Ávila-Pacheco", "Tiffany Poon", "Melanie Schirmer"], "abstract": "Inflammatory bowel diseases (IBD) are complex diseases in which the gut microbiota is attacked by the immune system of genetically predisposed subjects when they are exposed to yet unclear environmental factors. The complexity of this class of diseases makes them suitable to be represented and studied with network science. In the project, the metagenomic data of the gut microbiota of control, Crohn's disease, and ulcerative colitis subjects were divided in three ranges (prevalent, common, uncommon). Then, correlation networks and co-expression networks were used to represent this data. The former networks involved the calculation of the Pearson's correlation and the use of the percolation threshold to binarize the adjacency matrix, whereas the latter involved the construction of the bipartite networks and the monopartite projection after binarization of the biadjacency matrix. Then, centrality measures and community detection were used on the so-built networks. The main results obtained were about the modules of \"Bacteroides\", which were connected in control subjects' correlation network, \"Faecalibacterium prausnitzii\", where co-enzyme A became central in IBD correlation networks and \"Escherichia coli\", which module has different position in the different diagnoses networks.", "arxiv_id": "2208.07763v1", "research_question": "How can network-based analyses of gut microbiome data help identify microbial interactions or biomarkers that are useful for diagnosing or treating inflammatory bowel diseases?"}
{"doi": "https://doi.org/10.48550/arxiv.1908.02265", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks", "language": "en", "created_date": "2019-08-13T00:00:00", "publication_year": 2019, "cited_by_count": 1672, "authors": ["Devi Parikh", "Stefan Lee", "Jiasen Lu", "Dhruv Batra"], "abstract": "We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks -- visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval -- by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models -- achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.", "arxiv_id": "1908.02265v1", "research_question": "How does pretraining on large image–caption datasets improve performance on downstream vision-and-language tasks, and what are the main challenges when transferring such pretrained multimodal models?"}
{"doi": "https://doi.org/10.1109/iccv.2019.00667", "title": "CenterNet: Keypoint Triplets for Object Detection", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 3214, "authors": ["Kaiwen Duan", "Honggang Qi", "Qi Tian", "Song Bai", "Lingxi Xie", "Qingming Huang"], "abstract": "Human keypoint detection from a single image is very challenging due to occlusion, blur, illumination and scale variance of person instances. In this paper, we find that context information plays an important role in addressing these issues, and propose a novel method named progressive context refinement (PCR) for human keypoint detection. First, we devise a simple but effective context-aware module (CAM) that can efficiently integrate spatial and channel context information to aid feature learning for locating hard keypoints. Then, we construct the PCR model by stacking several CAMs sequentially with shortcuts and employ multi-task learning to progressively refine the context information and predictions. Besides, to maximize PCR's potential for the aforementioned hard case inference, we propose a hard-negative person detection mining strategy together with a joint-training strategy by exploiting the unlabeled coco dataset and external dataset. Extensive experiments on the COCO keypoint detection benchmark demonstrate the superiority of PCR over representative state-of-the-art (SOTA) methods. Our single model achieves comparable performance with the winner of the 2018 COCO Keypoint Detection Challenge. The final ensemble model sets a new SOTA on this benchmark.", "arxiv_id": "1910.12223v1", "research_question": "How can incorporating spatial and channel-wise contextual information improve the accuracy of human keypoint detection in cases of occlusion, blur, and scale variation?"}
{"doi": "https://doi.org/10.1186/s13073-019-0638-6", "title": "Molecular and pharmacological modulators of the tumor immune contexture revealed by deconvolution of RNA-seq data", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1578, "authors": ["Melinda E. Sanders", "Zuzana Loncová", "Gerhard Laschober", "Anne Krogsdam", "Douglas B. Johnson", "Pornpimol Charoentong", "Hubert Hackl", "Mónica V. Estrada", "Thomas P. Brouwer", "Yaomin Xu", "Wilfried Posch", "Justin M. Balko", "Sieghart Sopper", "Clemens Mayer", "Noel F.C.C. de Miranda", "Zlatko Trajanoski", "Christina Plattner", "Marieke E. Ijsselsteijn", "Doris Wilflingseder", "Francesca Finotello", "Paula I. González-Ericsson", "Yu Wang", "Dietmar Rieder"], "abstract": "In a previous paper the authors argued the case for incorporating ideas from innate immunity into artificial immune systems (AISs) and presented an outline for a conceptual framework for such systems. A number of key general properties observed in the biological innate and adaptive immune systems were highlighted, and how such properties might be instantiated in artificial systems was discussed in detail. The next logical step is to take these ideas and build a software system with which AISs with these properties can be implemented and experimentally evaluated. This paper reports on the results of that step - the libtissue system.", "arxiv_id": "1004.2854v1", "research_question": "What practical considerations and challenges should be addressed when developing a software platform to implement and evaluate artificial immune systems inspired by innate immunity?"}
{"doi": "https://doi.org/10.3390/polym11101667", "title": "Fiber-Reinforced Polymer Composites: Manufacturing, Properties, and Applications", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1449, "authors": ["Emanoil Linul", "Durgesh D. Pagar", "Pradeep L. Menezes", "Dipen Kumar Rajak"], "abstract": "When manufacturing parts using material extrusion additive manufacturing and anisotropic polymers, the mechanical properties of a manufactured component are strongly dependent on the print trajectory orientation. We conduct non-planar slicing and optimize the print trajectories to maximize the alignment between the material deposition direction and the stress flow induced by a predefined load case. The trajectory optimization framework considers manufacturability constraints in the form of uniform layer height and line spacing. We demonstrate the method by manufacturing a load bearing mechanical bracket using a 5-axis 3D printer and a liquid crystal polymer material. The failure strength and stiffness of the optimized bracket are improved by a factor of 44 and 6 respectively when compared with conventional printing.", "arxiv_id": "2301.04999v3", "research_question": "How does aligning the extrusion/deposition direction with the principal stress directions of a loaded part improve strength and stiffness in anisotropic polymer prints, and what practical challenges and constraints (e.g., printer kinematics, layer height, line spacing) typically arise when implementing such optimized non-planar printing trajectories?"}
{"doi": "https://doi.org/10.3322/caac.21552", "title": "Artificial intelligence in cancer imaging: Clinical challenges and applications", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1720, "authors": ["Raymond H. Mak", "Alireza Mehrtash", "Lawrence H. Schwartz", "Omar Arnaout", "Ahmed Hosny", "Wenya Linda Bi", "Clare M. Tempany", "Robert J. Gillies", "Christopher Abbosh", "Ian F. Dunn", "Rulla M. Tamimi", "Hugo J.W.L. Aerts", "Udo Hoffmann", "Nicolai J. Birkbak", "Tavis Allison", "Raymond Y. Huang", "Matthew B. Schabath", "Maryellen L. Giger", "Charles Swanton"], "abstract": "Recently, there has been great interest in developing Artificial Intelligence (AI) enabled computer-aided diagnostics solutions for the diagnosis of skin cancer. With the increasing incidence of skin cancers, low awareness among a growing population, and a lack of adequate clinical expertise and services, there is an immediate need for AI systems to assist clinicians in this domain. A large number of skin lesion datasets are available publicly, and researchers have developed AI-based image classification solutions, particularly deep learning algorithms, to distinguish malignant skin lesions from benign lesions in different image modalities such as dermoscopic, clinical, and histopathology images. Despite the various claims of AI systems achieving higher accuracy than dermatologists in the classification of different skin lesions, these AI systems are still in the very early stages of clinical application in terms of being ready to aid clinicians in the diagnosis of skin cancers. In this review, we discuss advancements in the digital image-based AI solutions for the diagnosis of skin cancer, along with some challenges and future opportunities to improve these AI systems to support dermatologists and enhance their ability to diagnose skin cancer.", "arxiv_id": "1911.11872v3", "research_question": "What are the main barriers to safely and effectively integrating AI-based skin cancer diagnostic tools into routine clinical practice, and how can they be addressed?"}
{"doi": "https://doi.org/10.1007/978-3-030-26253-2", "title": "Principles of Distributed Database Systems", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 2338, "authors": ["Patrick Valduriez", "M. TAMER ÖZSU"], "abstract": "This paper presents a novel approach to representing task assignments for partitioned agents (respectively, tasks) in distributed systems. A partition of agents (respectively, tasks) is represented by a Young tableau, which is one of the main tools in studying symmetric groups and combinatorics. In this paper we propose a task, agent, and assignment tableau in order to represent a task assignment for partitioned agents (respectively, tasks) in a distributed system. This paper is concerned with representations of task assignments rather than finding approximate or near optimal solutions for task assignments. A Young tableau approach allows us to raise the expressiveness of partitioned agents (respectively, tasks) and their task assignments.", "arxiv_id": "1012.1288v5", "research_question": "How can combinatorial structures such as Young tableaux be used to represent and analyze task assignments and partitions of agents in distributed systems, and what practical advantages do they offer compared with more traditional representations?"}
{"doi": "https://doi.org/10.1038/s41587-019-0114-2", "title": "Determining cell type abundance and expression from bulk tissues with digital cytometry", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 4439, "authors": ["Aaron M. Newman", "Bogdan Luca", "Andrew J. Gentles", "Ash A. Alizadeh", "Chih Long Liu", "Aadel A. Chaudhuri", "Mohammad Shahrokh Esfahani", "Chloé B. Steen", "David F. Steiner", "Maximilian Diehn", "Michael S. Khodadoust", "Florian Scherer"], "abstract": "Multiplexed immuno-fluorescence tissue imaging, allowing simultaneous detection of molecular properties of cells, is an essential tool for characterizing the complex cellular mechanisms in translational research and clinical practice. New image analysis approaches are needed because tissue section stained with a mixture of protein, DNA and RNA biomarkers are introducing various complexities, including spurious edges due to fluorescent staining artifacts between touching or overlapping cells. We have developed the RRScell method harnessing the stochastic random-reaction-seed (RRS) algorithm and deep neural learning U-net to extract single-cell resolution profiling-map of gene expression over a million cells tissue section accurately and automatically. Furthermore, with the use of manifold learning technique UMAP for cell phenotype cluster analysis, the AI-driven RRScell has equipped with a marker-based image cytometry analysis tool (markerUMAP) in quantifying spatial distribution of cell phenotypes from tissue images with a mixture of biomarkers. The results achieved in this study suggest that RRScell provides a robust enough way for extracting cytometric single cell morphology as well as biomarker content in various tissue types, while the build-in markerUMAP tool secures the efficiency of dimension reduction, making it viable as a general tool in the spatial analysis of high dimensional tissue image.", "arxiv_id": "2011.01002v2", "research_question": "What are common strategies and best practices for handling staining artifacts and spurious edges in multiplexed immunofluorescence images to improve accurate single-cell segmentation and downstream phenotype analysis?"}
{"doi": "https://doi.org/10.5040/9781350025875.ch-001", "title": "An Introduction to Research Methods", "language": "en", "created_date": "2016-06-24T00:00:00", "publication_year": 2019, "cited_by_count": 2377, "authors": ["Roberkt. B Burns"], "abstract": "The purpose of this course is to provide an introduction to Electromagnetic Theory. The foundations of electrodynamics starting from the nature of electrical force up to the level of Maxwell equations solutions are presented. It starts with the introduction of the concept of a field, which plays a very important role in the understanding of electricity and magnetism. In addition, moving electric charge is discussed as a topic of special importance in accelerator physics.", "arxiv_id": "2109.00606v1", "research_question": "How does the concept of an electromagnetic field change our understanding of electric and magnetic interactions compared with the older idea of action-at-a-distance?"}
{"doi": "https://doi.org/10.1109/cvpr.2019.00181", "title": "Toward Convolutional Blind Denoising of Real Photographs", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1085, "authors": ["Kai Zhang", "Shi Guo", "Lei Zhang", "Zifei Yan", "Wangmeng Zuo"], "abstract": "While deep convolutional neural networks (CNNs) have achieved impressive success in image denoising with additive white Gaussian noise (AWGN), their performance remains limited on real-world noisy photographs. The main reason is that their learned models are easy to overfit on the simplified AWGN model which deviates severely from the complicated real-world noise model. In order to improve the generalization ability of deep CNN denoisers, we suggest training a convolutional blind denoising network (CBDNet) with more realistic noise model and real-world noisy-clean image pairs. On the one hand, both signal-dependent noise and in-camera signal processing pipeline is considered to synthesize realistic noisy images. On the other hand, real-world noisy photographs and their nearly noise-free counterparts are also included to train our CBDNet. To further provide an interactive strategy to rectify denoising result conveniently, a noise estimation subnetwork with asymmetric learning to suppress under-estimation of noise level is embedded into CBDNet. Extensive experimental results on three datasets of real-world noisy photographs clearly demonstrate the superior performance of CBDNet over state-of-the-arts in terms of quantitative metrics and visual quality. The code has been made available at https://github.com/GuoShi28/CBDNet.", "arxiv_id": "1807.04686v2", "research_question": "How can I generate realistic synthetic noisy images that mimic real camera noise (including signal-dependent noise and in-camera processing steps) for training deep denoising models?"}
{"doi": "https://doi.org/10.1016/j.neucom.2019.01.078", "title": "Bidirectional LSTM with attention mechanism and convolutional layer for text classification", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1068, "authors": ["Gang Liu", "Jiabao Guo"], "abstract": "This paper focuses on three critical problems on protein classification. Firstly, Carbohydrate-active enzyme (CAZyme) classification can help people to understand the properties of enzymes. However, one CAZyme may belong to several classes. This leads to Multi-label CAZyme classification. Secondly, to capture information from the secondary structure of protein, protein classification is modeled as graph classification problem. Thirdly, compound-protein interactions prediction employs graph learning for compound with sequential embedding for protein. This can be seen as classification task for compound-protein pairs. This paper proposes three models for protein classification. Firstly, this paper proposes a Multi-label CAZyme classification model using CNN-LSTM with Attention mechanism. Secondly, this paper proposes a variational graph autoencoder based subspace learning model for protein graph classification. Thirdly, this paper proposes graph isomorphism networks (GIN) and Attention-based CNN-LSTM for compound-protein interactions prediction, as well as comparing GIN with graph convolution networks (GCN) and graph attention networks (GAT) in this task. The proposed models are effective for protein classification. Source code and data are available at https://github.com/zshicode/GNN-AttCL-protein. Besides, this repository collects and collates the benchmark datasets with respect to above problems, including CAZyme classification, enzyme protein graph classification, compound-protein interactions prediction, drug-target affinities prediction and drug-drug interactions prediction. Hence, the usage for evaluation by benchmark datasets can be more conveniently.", "arxiv_id": "2204.09486v2", "research_question": "What are common ways to represent protein structures as graphs for use with graph neural networks, and what are the main trade-offs (accuracy, computational cost, and ease of construction) between these representations?"}
{"doi": "https://doi.org/10.1038/s41560-019-0326-1", "title": "Economics of converting renewable power to hydrogen", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1143, "authors": ["Stefan Reichelstein", "Gunther Glenk"], "abstract": "Studies of muonic hydrogen atoms and molecules have been performed traditionally in bulk targets of gas, liquid or solid. At TRIUMF, Canada's meson facility, we have developed a new type of target system using multilayer thin films of solid hydrogen, which provides a beam of muonic hydrogen atoms in vacuum. Using the time-of-flight of the muonic atoms, the energy-dependent information of muonic reactions are obtained in direct manner. We discuss some unique measurements enabled by the new technique, with emphasis on processes relevant to muon catalyzed fusion.", "arxiv_id": "0101007v1", "research_question": "What are the main advantages and technical challenges of producing and studying muonic hydrogen atoms in vacuum for investigating energy-dependent muonic reactions and their relevance to muon-catalyzed fusion?"}
{"doi": "https://doi.org/10.1038/s41929-019-0246-2", "title": "Engineering the electronic structure of single atom Ru sites via compressive strain boosts acidic water oxidation electrocatalysis", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1114, "authors": ["Wei‐Xue Li", "Tongwei Yuan", "Bai-Quan Zhu", "Xun Hong", "Zhenggang Xue", "Ruirui Liu", "Zheng‐Qing Huang", "Ketao Zang", "Juncai Dong", "Chun‐Ran Chang", "Shiqiang Wei", "Peter Strasser", "Zhijun Li", "Yanxia Chen", "Wenjuan Yuan", "Jun Luo", "Yancai Yao", "Xiaoqian Wang", "Yuen Wu", "Tao Yao", "Sulei Hu", "Geng Wu", "Xusheng Zheng", "Wei-Chen Wei", "Wenxing Chen", "Yu Wang", "Dongsheng He", "Yadong Li", "Wei Liu"], "abstract": "Co(II) dissolved in acetate buffer at pH 7 is found to be a good water oxidation catalyst (WOC) showing electrocatalytic water oxidation current significantly greater than Co(II) in phosphate buffer under the same conditions owing to the higher solubility of the former. When electrodeposited on ITO/FTO electrodes it forms acetate bound cobalt(II)oxide based material (Co-Ac-WOC) showing catalytic water oxidation current density of 0.1 mA/cm$^{2}$ at 830 mV and 1 mA/cm$^{2}$ at 1 V in a pH 7 buffer solution. The morphology of Co-Ac-OEC is investigated with AFM, HR-TEM and SEM (at different times and electrodeposition potentials). The chemical composition of Co-Ac-OEC is investigated using XPS, EDX, combustion analysis and ATR-FTIR which indicates that this material has a CoO core with chloride and acetate anions bound to the Co center. Sodium is found to be integrated in the Co-Ac-WOC. The presence of the sodium ions and the chloride ions lowers the onset potential for oxygen evolution reaction (OER) by 240 mV relative to the classic Co-Pi at pH 7. The lower onset potential and higher OER current lowers the exchange current density to 10$^{-6.7}$ in Co-Ac-WOC relative to 10$^{-8}$-10$^{-10}$ in Co-Pi and its derivatives.", "arxiv_id": "1402.5491v1", "research_question": "How do different buffer anions and cations present at neutral pH affect the activity, onset potential, and stability of water oxidation catalysts, and what mechanisms explain those effects?"}
{"doi": "https://doi.org/10.1038/s41586-019-1711-4", "title": "Search-and-replace genome editing without double-strand breaks or donor DNA", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 4219, "authors": ["David R. Liu", "Jessie R. Davis", "Peyton B. Randolph", "Andrew V. Anzalone", "Gregory A. Newby", "Aditya Raguram", "Alexander A. Sousa", "Luke W. Koblan", "Jonathan M. Levy", "Christopher Wilson", "Peter J. Chen"], "abstract": "The Critical Assessment of Genome Interpretation (CAGI) aims to advance the state of the art for computational prediction of genetic variant impact, particularly those relevant to disease. The five complete editions of the CAGI community experiment comprised 50 challenges, in which participants made blind predictions of phenotypes from genetic data, and these were evaluated by independent assessors. Overall, results show that while current methods are imperfect, they have major utility for research and clinical applications. Missense variant interpretation methods are able to estimate biochemical effects with increasing accuracy. Performance is particularly strong for clinical pathogenic variants, including some difficult-to-diagnose cases, and extends to interpretation of cancer-related variants. Assessment of methods for regulatory variants and complex trait disease risk is less definitive, and indicates performance potentially suitable for auxiliary use in the clinic. Emerging methods and increasingly large, robust datasets for training and assessment promise further progress ahead.", "arxiv_id": "2205.05897v1", "research_question": "What are the current best practices for integrating computational predictions of genetic variant impact into clinical diagnostic workflows, and how should their uncertainty be communicated to clinicians and patients?"}
{"doi": "https://doi.org/10.1038/s41929-019-0242-6", "title": "Reaction systems for solar hydrogen production via water splitting with particulate semiconductor photocatalysts", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1431, "authors": ["Takashi Hisatomi", "Kazunari Domen"], "abstract": "Studies of muonic hydrogen atoms and molecules have been performed traditionally in bulk targets of gas, liquid or solid. At TRIUMF, Canada's meson facility, we have developed a new type of target system using multilayer thin films of solid hydrogen, which provides a beam of muonic hydrogen atoms in vacuum. Using the time-of-flight of the muonic atoms, the energy-dependent information of muonic reactions are obtained in direct manner. We discuss some unique measurements enabled by the new technique, with emphasis on processes relevant to muon catalyzed fusion.", "arxiv_id": "0101007v1", "research_question": "What are the advantages and challenges of studying muonic hydrogen atoms in vacuum compared with traditional bulk gas, liquid, or solid targets when investigating processes relevant to muon-catalyzed fusion?"}
{"doi": "https://doi.org/10.1109/iccv.2019.00179", "title": "Memorizing Normality to Detect Anomaly: Memory-Augmented Deep Autoencoder for Unsupervised Anomaly Detection", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1528, "authors": ["Lingqiao Liu", "Vuong Le", "Anton van den Hengel", "Moussa Reda Mansour", "Dong Gong", "Svetha Venkatesh", "Budhaditya Saha"], "abstract": "Deep autoencoder has been extensively used for anomaly detection. Training on the normal data, the autoencoder is expected to produce higher reconstruction error for the abnormal inputs than the normal ones, which is adopted as a criterion for identifying anomalies. However, this assumption does not always hold in practice. It has been observed that sometimes the autoencoder \"generalizes\" so well that it can also reconstruct anomalies well, leading to the miss detection of anomalies. To mitigate this drawback for autoencoder based anomaly detector, we propose to augment the autoencoder with a memory module and develop an improved autoencoder called memory-augmented autoencoder, i.e. MemAE. Given an input, MemAE firstly obtains the encoding from the encoder and then uses it as a query to retrieve the most relevant memory items for reconstruction. At the training stage, the memory contents are updated and are encouraged to represent the prototypical elements of the normal data. At the test stage, the learned memory will be fixed, and the reconstruction is obtained from a few selected memory records of the normal data. The reconstruction will thus tend to be close to a normal sample. Thus the reconstructed errors on anomalies will be strengthened for anomaly detection. MemAE is free of assumptions on the data type and thus general to be applied to different tasks. Experiments on various datasets prove the excellent generalization and high effectiveness of the proposed MemAE.", "arxiv_id": "1904.02639v2", "research_question": "How can incorporating an external memory component into an autoencoder improve anomaly detection, and what are the practical trade-offs (such as added complexity, memory size, and risk of overfitting) to consider?"}
{"doi": "https://doi.org/10.1021/acs.chemrev.8b00803", "title": "Quantum Chemistry in the Age of Quantum Computing", "language": "en", "created_date": "2019-01-01T00:00:00", "publication_year": 2019, "cited_by_count": 1257, "authors": ["Ian D. Kivlichan", "Yudong Cao", "Matthias Degroote", "Sukin Sim", "Jonathan P. Olson", "Peter D. Johnson", "Mária Kieferová", "Nicolas P. D. Sawaya", "Tim Menke", "Alán Aspuru-Guzik", "Libor Veis", "Borja Peropadre", "Jonathan Romero"], "abstract": "We present Tierkreis, a higher-order dataflow graph program representation and runtime designed for compositional, quantum-classical hybrid algorithms. The design of the system is motivated by the remote nature of quantum computers, the need for hybrid algorithms to involve cloud and distributed computing, and the long-running nature of these algorithms. The graph-based representation reflects how designers reason about and visualise algorithms, and allows automatic parallelism and asynchronicity. A strong, static type system and higher-order semantics allow for high expressivity and compositionality in the program. The flexible runtime protocol enables third-party developers to add functionality using any language or environment. With Tierkreis, quantum software developers can easily build, visualise, verify, test, and debug complex hybrid workflows, and immediately deploy them to the cloud or a custom distributed environment.", "arxiv_id": "2211.02350v1", "research_question": "What are the advantages and potential challenges of using a higher-order dataflow graph representation and runtime for building, visualizing, and executing quantum-classical hybrid algorithms across remote, cloud, or distributed environments?"}
{"doi": "https://doi.org/10.1111/gcb.14781", "title": "Quantitative assessment of microbial necromass contribution to soil organic matter", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1425, "authors": ["Matthias Kästner", "Johannes Lehmann", "Wulf Amelung", "Chao Liang"], "abstract": "This study investigated the use of portable X-ray fluorescence (PXRF) spectrometry and soil image analysis for rapid soil fertility assessment, with a focus on key indicators such as available boron (B), organic carbon (OC), available manganese (Mn), available sulfur (S), and the sulfur availability index (SAI). A total of 1,133 soil samples from diverse agro-climatic zones in Eastern India were analyzed. The research integrated color and texture features from microscopic soil images, PXRF data, and auxiliary soil variables (AVs) using a Random Forest model. Results showed that combining image features (IFs) with AVs significantly improved prediction accuracy for available B (R2 = 0.80) and OC (R2 = 0.88). A data fusion approach, incorporating IFs, AVs, and PXRF data, further enhanced predictions for available Mn and SAI, with R2 values of 0.72 and 0.70, respectively. The study highlights the potential of integrating these technologies to offer rapid, cost-effective soil testing methods, paving the way for more advanced predictive models and a deeper understanding of soil fertility. Future work should explore the application of deep learning models on a larger dataset, incorporating soils from a wider range of agro-climatic zones under field conditions.", "arxiv_id": "2404.12415v2", "research_question": "How does portable X-ray fluorescence (PXRF) spectrometry compare to conventional laboratory analyses for measuring soil nutrients and elements in terms of accuracy, cost, speed, and field applicability?"}
{"doi": "https://doi.org/10.1038/s41563-018-0275-2", "title": "Advances in magnetoelectric multiferroics", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1603, "authors": ["Nicola A. Spaldin", "R. Ramesh"], "abstract": "Multiferroics, defined for those multifunctional materials in which two or more kinds of fundamental ferroicities coexist, have become one of the hottest topics of condensed matter physics and materials science in recent years. The coexistence of several order parameters in multiferroics brings out novel physical phenomena and offers possibilities for new device functions. The revival of research activities on multiferroics is evidenced by some novel discoveries and concepts, both experimentally and theoretically. In this review article, we outline some of the progressive milestones in this stimulating field, specially for those single phase multiferroics where magnetism and ferroelectricity coexist. Firstly, we will highlight the physical concepts of multiferroicity and the current challenges to integrate the magnetism and ferroelectricity into a single-phase system. Subsequently, we will summarize various strategies used to combine the two types of orders. Special attentions to three novel mechanisms for multiferroicity generation: (1) the ferroelectricity induced by the spin orders such as spiral and E-phase antiferromagnetic spin orders, which break the spatial inversion symmetry, (2) the ferroelectricity originating from the charge ordered states, and (3) the ferrotoroidic system, will be paid. Then, we will address the elementary excitations such as electromagnons, and application potentials of multiferroics. Finally, open questions and opportunities will be prospected.", "arxiv_id": "0908.0662v1", "research_question": "What strategies are most promising for designing or discovering single-phase materials that exhibit both robust ferroelectricity and magnetism at or near room temperature?"}
{"doi": "https://doi.org/10.1109/cvpr.2019.00371", "title": "Actional-Structural Graph Convolutional Networks for Skeleton-Based Action Recognition", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1197, "authors": ["Xu Chen", "Qi Tian", "Maosen Li", "Yanfeng Wang", "Siheng Chen", "Ya Zhang"], "abstract": "In this paper, we study the problem of one-shot skeleton-based action recognition, which poses unique challenges in learning transferable representation from base classes to novel classes, particularly for fine-grained actions. Existing meta-learning frameworks typically rely on the body-level representations in spatial dimension, which limits the generalisation to capture subtle visual differences in the fine-grained label space. To overcome the above limitation, we propose a part-aware prototypical representation for one-shot skeleton-based action recognition. Our method captures skeleton motion patterns at two distinctive spatial levels, one for global contexts among all body joints, referred to as body level, and the other attends to local spatial regions of body parts, referred to as the part level. We also devise a class-agnostic attention mechanism to highlight important parts for each action class. Specifically, we develop a part-aware prototypical graph network consisting of three modules: a cascaded embedding module for our dual-level modelling, an attention-based part fusion module to fuse parts and generate part-aware prototypes, and a matching module to perform classification with the part-aware representations. We demonstrate the effectiveness of our method on two public skeleton-based action recognition datasets: NTU RGB+D 120 and NW-UCLA.", "arxiv_id": "2208.09150v1", "research_question": "How do multi-level (global body vs local body-part) representations and part-aware attention mechanisms improve generalization in one-shot skeleton-based action recognition, especially for distinguishing fine-grained actions?"}
{"doi": "https://doi.org/10.1038/s41586-019-1666-5", "title": "Quantum supremacy using a programmable superconducting processor", "language": "en", "created_date": "2019-11-01T00:00:00", "publication_year": 2019, "cited_by_count": 6340, "authors": ["William Courtney", "Joseph C. Bardin", "Craig Gidney", "Murphy Yuezhen Niu", "Kunal Arya", "Ben Chiaro", "Sergei V. Isakov", "Amit Vainsencher", "Austin Fowler", "Brian Burkett", "Masoud Mohseni", "Anthony Megrant", "Sergio Boixo", "Matthew McEwen", "Kostyantyn Kechedzhi", "Michael J. Hartmann", "Kristel Michielsen", "Matthew P. Harrigan", "Eric Ostby", "Josh Mutus", "Ping Yeh", "Rob Graff", "Steve Habegger", "Jarrod R. McClean", "Fernando G. S. L. Brandao", "Dmitry Lyakh", "Alan Ho", "Alexander Korotkov", "Zhang Jiang", "Daniel Sank", "John M. Martinis", "John C. Platt", "Mike Lindmark", "Salvatore Mandra", "Yu Chen", "Rupak Biswas", "David A. Buell", "Eleanor G. Rieffel", "Evan Jeffrey", "Matthew D. Trevithick", "Travis S. Humble", "Charles Neill", "Marissa Giustina", "Sergey Knysh", "Fedor Kostritsa", "Benjamin Villalonga", "David Landhuis", "Dave Bacon", "Adam Zalcman", "Ofer Naaman", "Xiao Mi", "Matthew Neeley", "Nicholas C. Rubin", "Andrew Dunsworth", "Kevin J. Satzinger", "Frank Arute", "Rami Barends", "Andre Petukhov", "Dvir Kafri", "Roberto Collins", "Brooks Foxen", "Erik Lucero", "Julián Kelly", "Trent Huang", "Zijun Chen", "Theodore White", "Ryan Babbush", "Chris Quintana", "Pedram Roushan", "Paul V. Klimov", "Markus Hoffmann", "Hartmut Neven", "Vadim Smelyanskiy", "Z. Jamie. Yao", "Edward Farhi", "Keith Guerin", "Kevin J. Sung"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B meson decays into muon pairs constrain or rule out different types of theories beyond the Standard Model, and which classes of models are most affected?"}
{"doi": "https://doi.org/10.5195/rt.2019.591", "title": "Universal Declaration of Human Rights", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1087, "authors": ["Janet Zandy"], "abstract": "The Advanced Data Protection Control (ADPC) is a technical specification - and a set of sociotechnical mechanisms surrounding it - that can change the current practice of Internet-based personal data protection and consenting by providing novel and standardized means for the communication of privacy and consenting data, meta-data, information, requests, preferences, and decisions. The ADPC supports humans in practicing their rights to privacy and agency by giving them more human-centric control over the processing of their personal data and consent. It helps the data controllers to improve their users' experiences and provides them with easy-to-adopt means to comply with the relevant legal and ethical requirements and expectations.", "arxiv_id": "2209.09724v1", "research_question": "What are the main technical and social challenges in implementing standardized mechanisms that give individuals more control over their personal data and consent online?"}
{"doi": "https://doi.org/10.18653/v1/d19-1387", "title": "Text Summarization with Pretrained Encoders", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1533, "authors": ["Mirella Lapata", "Yang Liu"], "abstract": "The ILSUM shared task focuses on text summarization for two major Indian languages- Hindi and Gujarati, along with English. In this task, we experiment with various pretrained sequence-to-sequence models to find out the best model for each of the languages. We present a detailed overview of the models and our approaches in this paper. We secure the first rank across all three sub-tasks (English, Hindi and Gujarati). This paper also extensively analyzes the impact of k-fold cross-validation while experimenting with limited data size, and we also perform various experiments with a combination of the original and a filtered version of the data to determine the efficacy of the pretrained models.", "arxiv_id": "2303.14461v1", "research_question": "What are effective strategies for improving abstractive text summarization performance in low-resource languages when only a small amount of labeled training data is available?"}
{"doi": "https://doi.org/10.1101/gr.240663.118", "title": "Benchmark and integration of resources for the estimation of human transcription factor activities", "language": "en", "created_date": "2019-07-30T00:00:00", "publication_year": 2019, "cited_by_count": 1002, "authors": ["Dénes Türei", "Mahmoud M. Ibrahim", "Luz García‐Alonso", "Julio Sáez-Rodríguez", "Christian H. Holland"], "abstract": "Phosphorus (P) is considered to be one of the key elements for life, making it an important element to look for in the abundance analysis of spectra of stellar systems. Yet, there exists only a handful of spectroscopic studies to estimate the P abundances and investigate its trend across a range of metallicities. We have observed full HK band spectra at a spectral resolving power of R=45,000 with IGRINS instrument. Abundances are determined using SME in combination with 1D MARCS stellar atmosphere models. The investigated sample of stars have reliable stellar parameters estimated using optical FIES spectra (GILD; Jönsson et al. in prep.). In order to determine the P abundances from the 16482.92 Angstrom P line, we take special care of the CO($ν=7-4$) blend. We determine the C, N, O abundances from atomic carbon and a range of non-blended molecular lines (CO, CN, OH) which are aplenty in the H band region of K giant stars, assuring an appropriate modelling of the blending CO($ν=7-4$) line. We present [P/Fe] vs [Fe/H] trend for 38 K giant stars in the metallicity range of -1.2 dex $<$ [Fe/H] $<$ 0.4 dex. We find that our trend matches well with the compiled literature sample of prominently dwarf stars and limited number of giant stars. Our trend is found to be higher by $\\sim$ 0.05 - 0.1 dex compared to the theoretical chemical evolution trend in Cescutti et al. 2012 resulting from core collapse supernova (type II) of massive stars with the P yields from Kobayashi et al. (2006) arbitrarily increased by a factor of 2.75. Thus the enhancement factor might need to be $\\sim$ 0.05 - 0.1 dex higher to match our trend. We also find an empirically determined primary behaviour for phosphorus. Furthermore, the phosphorus abundance is found to be elevated by $\\sim$ 0.6 - 0.9 dex in two metal poor s-enriched stars compared to the theoretical chemical evolution trend.", "arxiv_id": "2210.04940v1", "research_question": "What are the main nucleosynthetic sources of phosphorus in the Galaxy, and how do they influence the observed trend of phosphorus abundance with stellar metallicity?"}
{"doi": "https://doi.org/10.1016/j.jretconser.2019.01.011", "title": "Instagram and YouTube bloggers promote it, why should I buy? How credibility and parasocial interaction influence purchase intentions", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1276, "authors": ["Hajer Kéfi", "Karina Sokolova"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare leptonic B-meson decays to muon pairs constrain extensions of the Standard Model, and which classes of new-physics models are most strongly affected?"}
{"doi": "https://doi.org/10.1016/j.immuni.2019.03.021", "title": "The IL-17 Family of Cytokines in Health and Disease", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1299, "authors": ["J. Daniel", "Sarah L. Gaffen", "Mandy J. McGeachy"], "abstract": "Progressive cognitive decline spanning across decades is characteristic of Alzheimer's disease (AD). Various predictive models have been designed to realize its early onset and study the long-term trajectories of cognitive test scores across populations of interest. Research efforts have been geared towards superimposing patients' cognitive test scores with the long-term trajectory denoting gradual cognitive decline, while considering the heterogeneity of AD. Multiple trajectories representing cognitive assessment for the long-term have been developed based on various parameters, highlighting the importance of classifying several groups based on disease progression patterns. In this study, a novel method capable of self-organized prediction, classification, and the overlay of long-term cognitive trajectories based on short-term individual data was developed, based on statistical and differential equation modeling. We validated the predictive accuracy of the proposed method for the long-term trajectory of cognitive test score results on two cohorts: the Alzheimer's Disease Neuroimaging Initiative (ADNI) study and the Japanese ADNI study. We also presented two practical illustrations of the simultaneous evaluation of risk factor associated with both the onset and the longitudinal progression of AD, and an innovative randomized controlled trial design for AD that standardizes the heterogeneity of patients enrolled in a clinical trial. These resources would improve the power of statistical hypothesis testing and help evaluate the therapeutic effect. The application of predicting the trajectory of longitudinal disease progression goes beyond AD, and is especially relevant for progressive and neurodegenerative disorders.", "arxiv_id": "2402.12205v1", "research_question": "What are the main challenges and limitations in using short-term clinical assessments to accurately predict long-term cognitive decline trajectories in progressive neurodegenerative diseases?"}
{"doi": "https://doi.org/10.1126/science.aav0550", "title": "The global soil community and its influence on biogeochemistry", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1077, "authors": ["Ashley D. Keiser", "Daniel S. Maynard", "Melanie A. Mayes", "Lidong Mo", "Joe Wan", "Colin Averill", "Johan van den Hoogen", "Thomas W. Crowther"], "abstract": "This study investigated the use of portable X-ray fluorescence (PXRF) spectrometry and soil image analysis for rapid soil fertility assessment, with a focus on key indicators such as available boron (B), organic carbon (OC), available manganese (Mn), available sulfur (S), and the sulfur availability index (SAI). A total of 1,133 soil samples from diverse agro-climatic zones in Eastern India were analyzed. The research integrated color and texture features from microscopic soil images, PXRF data, and auxiliary soil variables (AVs) using a Random Forest model. Results showed that combining image features (IFs) with AVs significantly improved prediction accuracy for available B (R2 = 0.80) and OC (R2 = 0.88). A data fusion approach, incorporating IFs, AVs, and PXRF data, further enhanced predictions for available Mn and SAI, with R2 values of 0.72 and 0.70, respectively. The study highlights the potential of integrating these technologies to offer rapid, cost-effective soil testing methods, paving the way for more advanced predictive models and a deeper understanding of soil fertility. Future work should explore the application of deep learning models on a larger dataset, incorporating soils from a wider range of agro-climatic zones under field conditions.", "arxiv_id": "2404.12415v2", "research_question": "How do factors like soil moisture, surface roughness, and sample preparation impact the accuracy and reliability of portable X‑ray fluorescence measurements and soil image‑based analyses in rapid field soil fertility assessment?"}
{"doi": "https://doi.org/10.1159/000499361", "title": "European Consensus Guidelines on the Management of Respiratory Distress Syndrome – 2019 Update", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1107, "authors": ["Gerhard H. Visser", "Gorm Greisen", "Henry L. Halliday", "Eren Özek", "Christian P. Speer", "Umberto Siméoni", "David G. Sweet", "Richard Plavka", "Mikko Hallman", "Virgilio Carnielli", "Arjan B. te Pas", "Máximo Vento", "Ola Didrik Saugstad", "Charles Christoph Roehr"], "abstract": "Acute Respiratory Distress Syndrome (ARDS) is a very severe syndrome leading to respiratory failure and subsequent mortality. Sepsis is one of the leading causes of ARDS. Thus, extracellular bacteria play an important role in the pathophysiology of ARDS. Overactivated neutrophils are the major effector cells in ARDS. Thus, extracellular bacteria triggered TH17-like innate immunity with neutrophil activation might accounts for the etiology of ARDS. Here, microarray analysis was employed to describe TH17-like innate immunity-related cytokine including TGF-β and IL-6 up-regulation in whole blood of ARDS patients. It was found that the innate TH17-related TLR1,2,4,5,8, HSP70, G-CSF, GM-CSF, complements, defensin, PMN chemokines, cathepsins, Fc receptors, NCFs, FOS, JunB, CEBPs, NFkB, and leukotriene B4 are all up-regulated. TGF-β secreting Treg cells play important roles in lung fibrosis. Up-regulation of Treg associated STAT5B and TGF-β with down-regulation of MHC genes, TCR genes, and co-stimulation molecule CD86 are noted. Key TH17 transcription factors, STAT3 and RORα, are down-regulated. Thus, the full adaptive TH17 helper CD4 T cells may not be successfully triggered. Many fibrosis promoting genes are also up-regulated including MMP8, MMP9, FGF13, TIMP1, TIMP2, PLOD1, P4HB, P4HA1, PDGFC, HMMR, HS2ST1, CHSY1, and CSGALNACT. Failure to induce successful adaptive immunity could also attribute to ARDS pathogenesis. Thus, ARDS is actually a TH17-like and Treg immune disorder.", "arxiv_id": "1311.4384v1", "research_question": "What are the key differences between TH17-like innate immune responses and conventional adaptive TH17 responses, and how could those differences influence neutrophil-driven lung inflammation and fibrosis in severe respiratory disease?"}
{"doi": "https://doi.org/10.4324/9780367854546", "title": "Cognitive Poetics: An Introduction", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1170, "authors": ["Peter Stockwell"], "abstract": "The purpose of this course is to provide an introduction to Electromagnetic Theory. The foundations of electrodynamics starting from the nature of electrical force up to the level of Maxwell equations solutions are presented. It starts with the introduction of the concept of a field, which plays a very important role in the understanding of electricity and magnetism. In addition, moving electric charge is discussed as a topic of special importance in accelerator physics.", "arxiv_id": "2109.00606v1", "research_question": "What is the physical meaning of an electromagnetic field, and how does introducing the field concept help explain forces between charges and currents?"}
{"doi": "https://doi.org/10.1787/67450d67-en", "title": "The Heavy Burden of Obesity", "language": "en", "created_date": "2022-05-12T00:00:00", "publication_year": 2019, "cited_by_count": 1058, "authors": ["OECD"], "abstract": "This White Paper on Heavy Ion Physics at the LHC was presented at the Town Meeting at Brookhaven National Laboratory, Jan. 21-23, 2001, and made available to NSAC to aid in the long range planning process.", "arxiv_id": "0104014v1", "research_question": "What are the primary scientific goals of studying heavy-ion collisions at very high energies, and how could those experiments advance our understanding of the quark-gluon plasma and the strong interaction?"}
{"doi": "https://doi.org/10.1038/s41586-019-1418-6", "title": "Soil nematode abundance and functional group composition at a global scale", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1099, "authors": ["Stefan Geisen", "Paul Kardol", "Diana H. Wall", "Raquel Campos‐Herrera", "Matthew Magilton", "Jean Trap", "Walter Traunspurger", "Larissa de Brito Caixeta", "Wim H. van der Putten", "Hiroaki Okada", "Mette Vestergård", "Thi Anh Duong Nguyen", "Jiue-in Yang", "Xiaoyun Chen", "Johan van den Hoogen", "Tancredi Caruso", "J. M. da C. e Castro", "El Hassan Mayad", "Thomas O. Powers", "Sergio Rasmann", "Stefan Scheu", "Thomas W. Crowther", "Miguel Escuer", "Juvenil Enrique Cares", "Carmen Gutiérrez", "G.W. Korthals", "Lieven Waeyenberge", "Mariette Marais", "Karin Hohberg", "Michael Bonkowski", "Juan E. Palomares‐Rius", "А. А. Сущук", "Richard D. Bardgett", "Roy Neilson", "D. S. Kalinkina", "José Antonio Rodríguez Martín", "Wasim Ahmad", "Camille Pitteloud", "Júlio Carlos Pereira da Silva", "Walter S. Andriuzzi", "Rachel Creamer", "Marie Dam", "Bryan S. Griffiths", "Peter Mullin", "Casper W. Quist", "Cécile Villenave", "R.G.M. de Goede", "David A. Wardle", "Sofia R. Costa", "Sara Sánchez‐Moreno", "Heikki Setälä", "Alexei V. Tiunov", "Е. М. Матвеева", "Christian Mulder", "Alexey Kudrin", "Valentyna Krashevska", "Vlada Peneva", "Byron J. Adams", "Howard Ferris", "Alan Kergunteuil", "Qi Li", "Uffe N. Nielsen", "Devin Routh", "Rutger A. Wilschut", "Kirsten Powers", "Kaiwen Pan", "Loïc Pellissier", "Daniel G. Wright", "Djibril Djigal", "Wenju Liang"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare neutral B meson decays into muon pairs constrain extensions of the Standard Model, and which types of new-physics scenarios are most strongly affected?"}
{"doi": "https://doi.org/10.1016/j.rser.2019.01.051", "title": "Underground hydrogen storage: Characteristics and prospects", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1009, "authors": ["Radosław Tarkowski"], "abstract": "Studies of muonic hydrogen atoms and molecules have been performed traditionally in bulk targets of gas, liquid or solid. At TRIUMF, Canada's meson facility, we have developed a new type of target system using multilayer thin films of solid hydrogen, which provides a beam of muonic hydrogen atoms in vacuum. Using the time-of-flight of the muonic atoms, the energy-dependent information of muonic reactions are obtained in direct manner. We discuss some unique measurements enabled by the new technique, with emphasis on processes relevant to muon catalyzed fusion.", "arxiv_id": "0101007v1", "research_question": "How does generating muonic hydrogen atoms in a vacuum environment, instead of within bulk gas/liquid/solid targets, change the ability to measure energy-dependent muonic reactions and what experimental challenges does this approach introduce?"}
{"doi": "https://doi.org/10.1126/science.aav7188", "title": "Multiple sclerosis genomic map implicates peripheral immune cells and microglia in susceptibility", "language": "en", "created_date": "2019-11-01T00:00:00", "publication_year": 2019, "cited_by_count": 1305, "authors": ["Onigiusz Zarzycki", "Fredrik Karpe", "Laura Ferrè", "Clara Guaschino", "Vincent Damotte", "Maria Ban", "Cristin McCabe", "Xavier Montalbán", "Lucia Moiola", "Maria Cimpean", "Christiane Graetz", "Bénédicte Dubois", "Pirro G. Hysi", "Verena Grummel", "Frauke Zipp", "Léna Guillot‐Noël", "David Gómez-Cabrero", "Cristina Agliardi", "Luisa Bernardinelli", "Janna Saarela", "Allison Robbins", "Jan Hillert", "Federica Esposito", "Ilijas Jelčić", "Phoebe A. Winn", "Vittorio Martinelli", "Bertrand Fontaine", "Henrik Ullum", "Izaura Lima Bomfim", "Viola Pongratz", "Andrea Zauli", "Roland Martinꝉ", "Isabelle Cournu‐Rebeix", "Paul Molyneux", "Mark Lathrop", "Steffan D. Bos", "Silvia Santoro", "Helle Bach Søndergaard", "Elisabeth Gulowsen Celius", "Nicasio Mancini", "Gıancarlo Comı", "Geneviève Lachance", "Domizia Vecchio", "Julia Mescheriakova", "Antonio Amoroso", "Sergio E. Baranzini", "Melissa Sorosina", "Benjamin Knier", "Christina M. Lill", "Aaron Brandes", "Miriam Zuccalà", "Daniele Cusi", "Ioannis-Pavlos Panteliadis", "Lise Wegner Thørner", "Daniela Galimberti", "Cornelia M. van Duijn", "Fredrik Piehl", "Matthew Neville", "Maria Donata Benedetti", "Marco Salvetti", "Bernhard Hemmer", "Dorothea Buck", "Joseph M. Replogle", "Anne Spurkland", "Seema Kalra", "Amie Baker", "Nikolaos A. Patsopoulos", "Per Soelberg Sorensen", "Parisa Shoostari", "Paola Cavalla", "Muni Hoshi", "Finn Sellebjerg", "Garrett Wong", "Chris Cotsapas", "Ashley Beecham", "Charles C. White", "Maurizio Leone", "Efthimios Dardiotis", "Adam Santaniello", "Clive Hawkins", "Lars Alfredsson", "Achim Berthele", "Christiane Gasperi", "Felix Luessi", "Ferdinando Clarelli", "Maja Jagodic", "Sandra Vukusic", "Ioannis S. Vlachos", "Manuel Comabella", "Mark Mühlau", "Brendan T Keenan", "An Goris", "Thomas Korn", "Tojo James", "Jyoti Khadake", "Nadia Barizzone", "Magdalena Lindén", "Tune H. Pers", "Mirela Sospedra", "Till F. M. Andlauer"], "abstract": "In a previous paper the authors argued the case for incorporating ideas from innate immunity into artificial immune systems (AISs) and presented an outline for a conceptual framework for such systems. A number of key general properties observed in the biological innate and adaptive immune systems were highlighted, and how such properties might be instantiated in artificial systems was discussed in detail. The next logical step is to take these ideas and build a software system with which AISs with these properties can be implemented and experimentally evaluated. This paper reports on the results of that step - the libtissue system.", "arxiv_id": "1004.2854v1", "research_question": "What are the key design principles and components needed to implement an artificial immune system that incorporates innate immunity concepts, and how can such a system be experimentally evaluated in practice?"}
{"doi": "https://doi.org/10.1103/physrevx.9.011001", "title": "Properties of the Binary Neutron Star Merger GW170817", "language": "en", "created_date": "2019-06-27T00:00:00", "publication_year": 2019, "cited_by_count": 1219, "authors": ["C. P. L. Berry", "L. Barsotti", "V. B. Adya", "G. Bergmann", "P. Addesso", "M. Arène", "S. Bae", "C. Beer", "J. C. Bayley", "F. Baldaccini", "P. Bacon", "D. V. Atallah", "A. Ananyeva", "M. Agathos", "P. A. Altin", "Sebastiano Bernuzzi", "N. Aggarwal", "B. Agarwal", "P. Aufmuth", "J. C. Batch", "S. Antier", "S. Ballmer", "M. Bawaj", "F. Aubin", "A. Bertolini", "D. Beniwal", "J. C. Barayoga", "S. A. Bilgili", "M. Bejger", "N. Arnaud", "C. Aulbert", "S. V. Angelova", "M. Barsuglia", "G. Ashton", "M. Bazzan", "K. Barkett", "I. Bartos", "B. Barr", "M. Ast", "T. Adams", "C. Austin", "C. Adams", "S. Appert", "S. Barnum", "T. D. Abbott", "R. Bhandare", "G. Billingsley", "F. Barone", "M. Bensch", "K. Ackley", "M. Á. Aloy", "C. R. Billman", "G. Ballardin", "J. S. Areeda", "D. Barker", "I. Belahcene", "S. Ascenzi", "K. G. Arun", "S. M. Aston", "A. Ain", "K. Arai", "M. C. Araya", "B. Bécsy", "J. J. Bero", "R. Abbott", "J. Bartlett", "S. B. Anderson", "S. Babak", "A. Allocca", "P. Astone", "S. Banagiri", "D. Bersanetti", "O. D. Aguiar", "A. Basti", "L. Aiello", "J. Birch", "D. Barta", "W. G. Anderson", "B. C. Barish", "A. Amato", "I. A. Bilenko", "C. Affeldt", "R. X. Adhikari", "S. E. Barclay", "B. Allen", "R. Bassiri", "K. Agatsuma", "A. S. Bell", "B. K. Berger", "M. K. M. Bader", "F. Badaracco", "P. T. Baker", "K. AultONeal", "J. Betzwieser", "F. Acernese", "P. Ajith", "B. P. Abbott", "A. Avila-Alvarez", "R. Birney"], "abstract": "Finding the electromagnetic (EM) counterpart of binary compact star merger, especially the binary neutron star (BNS) merger, is critically important for gravitational wave (GW) astronomy, cosmology and fundamental physics. On Aug. 17, 2017, Advanced LIGO and \\textit{Fermi}/GBM independently triggered the first BNS merger, GW170817, and its high energy EM counterpart, GRB 170817A, respectively, resulting in a global observation campaign covering gamma-ray, X-ray, UV, optical, IR, radio as well as neutrinos. The High Energy X-ray telescope (HE) onboard \\textit{Insight}-HXMT (Hard X-ray Modulation Telescope) is the unique high-energy gamma-ray telescope that monitored the entire GW localization area and especially the optical counterpart (SSS17a/AT2017gfo) with very large collection area ($\\sim$1000 cm$^2$) and microsecond time resolution in 0.2-5 MeV. In addition, \\textit{Insight}-HXMT quickly implemented a Target of Opportunity (ToO) observation to scan the GW localization area for potential X-ray emission from the GW source. Although it did not detect any significant high energy (0.2-5 MeV) radiation from GW170817, its observation helped to confirm the unexpected weak and soft nature of GRB 170817A. Meanwhile, \\textit{Insight}-HXMT/HE provides one of the most stringent constraints (~10$^{-7}$ to 10$^{-6}$ erg/cm$^2$/s) for both GRB170817A and any other possible precursor or extended emissions in 0.2-5 MeV, which help us to better understand the properties of EM radiation from this BNS merger. Therefore the observation of \\textit{Insight}-HXMT constitutes an important chapter in the full context of multi-wavelength and multi-messenger observation of this historical GW event.", "arxiv_id": "1710.06065v1", "research_question": "How do non-detections or stringent upper limits from high-energy telescopes help constrain models of prompt, precursor, or extended electromagnetic emission from binary neutron star mergers?"}
{"doi": "https://doi.org/10.1038/s41592-019-0403-1", "title": "Deep learning for cellular image analysis", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1185, "authors": ["Markus W. Covert", "Erick Moen", "Dylan Bannon", "William D. Graf", "Takamasa Kudo", "David Van Valen"], "abstract": "Despite the advances of deep learning in specific tasks using images, the principled assessment of image fidelity and similarity is still a critical ability to develop. As it has been shown that Mean Squared Error (MSE) is insufficient for this task, other measures have been developed with one of the most effective being Structural Similarity Index (SSIM). Such measures can be used for subspace learning but existing methods in machine learning, such as Principal Component Analysis (PCA), are based on Euclidean distance or MSE and thus cannot properly capture the structural features of images. In this paper, we define an image structure subspace which discriminates different types of image distortions. We propose Image Structural Component Analysis (ISCA) and also kernel ISCA by using SSIM, rather than Euclidean distance, in the formulation of PCA. This paper provides a bridge between image quality assessment and manifold learning opening a broad new area for future research.", "arxiv_id": "1908.09287v1", "research_question": "How can dimensionality reduction or subspace learning methods be adapted to use perceptual image similarity measures (like SSIM) instead of Euclidean/MSE-based distances, and what are the practical benefits and challenges of doing so?"}
{"doi": "https://doi.org/10.1126/science.aay7044", "title": "Efficient, stable solar cells by using inherent bandgap of α-phase formamidinium lead iodide", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1197, "authors": ["Hanul Min", "Jun Hee Lee", "Maengsuk Kim", "Sang Il Seok", "Hyeonwoo Kim", "Keunsu Choi", "Seungun Lee", "Gwisu Kim"], "abstract": "Recent developments have renewed the demand for solar cells with increased tolerance to radiation damage. To investigate the specific irradiation damage of 1 MeV electron irradiation in GaInAsP lattice matched to InP for varying In and P contents, a simulation based analysis is employed: by fitting the quantum efficiency and open-circuit voltage simultaneously before and after irradiation, the induced changes in lifetime are detected. Furthermore, the reduction of irradiation damage during regeneration under typical satellite operating conditions for GEO missions (60°C and AM0 illumination) is investigated. A clear decrease of the radiation damage is observed after post irradiation regeneration. This regeneration effect is stronger for increasing InP-fraction. It is demonstrated that the irradiation induced defect recombination coefficient for irradiation with 1 MeV electrons after regeneration for 216 hours can be described with a linear function of InP-fraction between 1*10$^{-5}$ cm$^2$/s for GaAs and 7*10$^{-7}$ cm$^2$/s for InP. The results show that GaInAsP is a promising material for radiation hard space solar cells.", "arxiv_id": "2004.00308v2", "research_question": "What physical mechanisms enable thermal and light-assisted regeneration (annealing) of irradiation-induced defects in III–V multinary solar cell materials, and how does alloy composition typically influence the effectiveness of such regeneration?"}
{"doi": "https://doi.org/10.1126/science.aax0848", "title": "The global tree restoration potential", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 2000, "authors": ["Yelena Finegold", "Marcelo Rezende", "Constantin M. Zohner", "Devin Routh", "Claude García", "Danilo Mollicone", "Jean‐François Bastin", "Thomas W. Crowther"], "abstract": "The derivation trees of a tree adjoining grammar provide a first insight into the sentence semantics, and are thus prime targets for generation systems. We define a formalism, feature-based regular tree grammars, and a translation from feature based tree adjoining grammars into this new formalism. The translation preserves the derivation structures of the original grammar, and accounts for feature unification.", "arxiv_id": "0804.4584v1", "research_question": "How do feature-based regular tree grammars compare to traditional tree adjoining grammars in representing derivation structures and handling feature unification, and what practical advantages do they offer for generation systems?"}
{"doi": "https://doi.org/10.1021/acs.est.9b01517", "title": "Human Consumption of Microplastics", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 2277, "authors": ["Garth A. Covernton", "Sarah E. Dudas", "John F. Dower", "Francis Juanes", "Kieran Cox", "Hailey L. Davies"], "abstract": "The progress of some AI paradigms such as deep learning is said to be linked to an exponential growth in the number of parameters. There are many studies corroborating these trends, but does this translate into an exponential increase in energy consumption? In order to answer this question we focus on inference costs rather than training costs, as the former account for most of the computing effort, solely because of the multiplicative factors. Also, apart from algorithmic innovations, we account for more specific and powerful hardware (leading to higher FLOPS) that is usually accompanied with important energy efficiency optimisations. We also move the focus from the first implementation of a breakthrough paper towards the consolidated version of the techniques one or two year later. Under this distinctive and comprehensive perspective, we study relevant models in the areas of computer vision and natural language processing: for a sustained increase in performance we see a much softer growth in energy consumption than previously anticipated. The only caveat is, yet again, the multiplicative factor, as future AI increases penetration and becomes more pervasive.", "arxiv_id": "2109.05472v2", "research_question": "What factors determine whether increasing model size causes higher inference energy consumption, and how can organizations estimate the real-world energy impact of deploying larger models at scale?"}
{"doi": "https://doi.org/10.1038/s41586-019-1710-5", "title": "Dry double-sided tape for adhesion of wet tissues and devices", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1303, "authors": ["Xinyu Mao", "Hyunwoo Yuk", "Robert F. Padera", "Christoph S. Nabzdyk", "Xuanhe Zhao", "Ellen T. Roche", "Claudia E. Varela"], "abstract": "Tree frogs possess soft structured toe pads with channels that help squeeze out fluid from the region between the toe pad and the contacting surfaces. This structure enables them to walk on wet and rough surfaces. We present our preliminary result aiming at understanding the role of surface structure on hydrodynamic drainage forces. We have used the surface forces apparatus to measure the drainage forces (approach and retraction) in silicone oil. We compare the drainage force measured between two smooth PMMA surfaces with the one measure when one of the two surfaces has a structure with channels. We have observed that the presence of channels reduces the hydrodynamic drainage forces when the surfaces come together and reduces the adhesion when the surfaces are pulled apart. Our force measurements were compared to Reynolds theory and we discuss the possible role of elastohydrodynamic deformation in tree frog adhesion.", "arxiv_id": "2204.12491v1", "research_question": "How do micro-scale surface channels alter the hydrodynamic drainage and resulting adhesion between compliant surfaces in viscous fluids, and what role does elastohydrodynamic deformation play in that process?"}
{"doi": "https://doi.org/10.1038/s41586-019-1695-0", "title": "Superconductors, orbital magnets and correlated states in magic-angle bilayer graphene", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1522, "authors": ["Dmitri K. Efetov", "A. H. MacDonald", "Mohammed Ali Aamir", "Ming Xie", "Ipsita Das", "C. Urgell", "Kenji Watanabe", "Adrian Bachtold", "Takashi Taniguchi", "Guangyu Zhang", "Xiaobo Lu", "Petr Stepanov", "Wei Yang"], "abstract": "Twisted bilayer graphene (TBG) is remarkable for its topological flat bands, which drive strongly-interacting physics at integer fillings, and its simple theoretical description facilitated by the Bistritzer-MacDonald Hamiltonian, a continuum model coupling two Dirac fermions. Due to the large moiré unit cell, TBG offers the unprecedented opportunity to observe reentrant Hofstadter phases in laboratory-strength magnetic fields near $25$T. This Letter is devoted to magic angle TBG at $2π$ flux where the magnetic translation group commutes. We use a newly developed gauge-invariant formalism to determine the exact single-particle band structure and topology. We find that the characteristic TBG flat bands reemerge at $2π$ flux, but, due to the magnetic field breaking $C_{2z} \\mathcal{T}$, they split and acquire Chern number $\\pm1$. We show that reentrant correlated insulating states appear at $2π$ flux driven by the Coulomb interaction at integer fillings, and we predict the characteristic Landau fans from their excitation spectrum. We conjecture that superconductivity can also be re-entrant at $2π$ flux.", "arxiv_id": "2111.11434v1", "research_question": "How does applying a strong perpendicular magnetic field to a moiré two-dimensional material alter the topology and correlated phases of its low-energy electronic bands, and what experimental probes would best reveal those changes?"}
{"doi": "https://doi.org/10.1016/j.neubiorev.2019.06.032", "title": "The Interaction of Person-Affect-Cognition-Execution (I-PACE) model for addictive behaviors: Update, generalization to addictive behaviors beyond internet-use disorders, and specification of the process character of addictive behaviors", "language": "en", "created_date": "2019-07-12T00:00:00", "publication_year": 2019, "cited_by_count": 1438, "authors": ["Klaus Wölfling", "Elisa Wegmann", "Rudolf Stark", "Trevor W. Robbins", "Astrid Müller", "Matthias Brand", "Marc N. Potenza"], "abstract": "This paper presents a novel approach to the technical analysis of wireheading in intelligent agents. Inspired by the natural analogues of wireheading and their prevalent manifestations, we propose the modeling of such phenomenon in Reinforcement Learning (RL) agents as psychological disorders. In a preliminary step towards evaluating this proposal, we study the feasibility and dynamics of emergent addictive policies in Q-learning agents in the tractable environment of the game of Snake. We consider a slightly modified settings for this game, in which the environment provides a \"drug\" seed alongside the original \"healthy\" seed for the consumption of the snake. We adopt and extend an RL-based model of natural addiction to Q-learning agents in this settings, and derive sufficient parametric conditions for the emergence of addictive behaviors in such agents. Furthermore, we evaluate our theoretical analysis with three sets of simulation-based experiments. The results demonstrate the feasibility of addictive wireheading in RL agents, and provide promising venues of further research on the psychopathological modeling of complex AI safety problems.", "arxiv_id": "1811.05590v1", "research_question": "How can concepts and diagnostic criteria from psychology and psychiatry be applied to identify, characterize, and mitigate addictive or compulsive behaviors in reinforcement learning agents?"}
{"doi": "https://doi.org/10.1038/s41575-019-0186-y", "title": "A global view of hepatocellular carcinoma: trends, risk, prevention and management", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 4117, "authors": ["Amina Amadou", "Ju Dong Yang", "Pierre Hainaut", "Amelie Plymoth", "Lewis R. Roberts", "Gregory J. Gores"], "abstract": "In risk management it is desirable to grasp the essential statistical features of a time series representing a risk factor. This tutorial aims to introduce a number of different stochastic processes that can help in grasping the essential features of risk factors describing different asset classes or behaviors. This paper does not aim at being exhaustive, but gives examples and a feeling for practically implementable models allowing for stylised features in the data. The reader may also use these models as building blocks to build more complex models, although for a number of risk management applications the models developed here suffice for the first step in the quantitative analysis. The broad qualitative features addressed here are {fat tails} and {mean reversion}. We give some orientation on the initial choice of a suitable stochastic process and then explain how the process parameters can be estimated based on historical data. Once the process has been calibrated, typically through maximum likelihood estimation, one may simulate the risk factor and build future scenarios for the risky portfolio. On the terminal simulated distribution of the portfolio one may then single out several risk measures, although here we focus on the stochastic processes estimation preceding the simulation of the risk factors Finally, this first survey report focuses on single time series. Correlation or more generally dependence across risk factors, leading to multivariate processes modeling, will be addressed in future work.", "arxiv_id": "0812.4210v1", "research_question": "What practical steps and diagnostics should I use to choose, estimate, and validate a single-variate stochastic process for a financial time series that exhibits both fat tails and mean-reversion?"}
{"doi": "https://doi.org/10.1109/access.2019.2909530", "title": "Unmanned Aerial Vehicles (UAVs): A Survey on Civil Applications and Key Research Challenges", "language": "en", "created_date": "2022-02-24T00:00:00", "publication_year": 2019, "cited_by_count": 1997, "authors": ["Noor Shamsiah Othman", "Abdallah Khreishah", "Zuochao Dou", "Ala Al‐Fuqaha", "Ahmad Sawalmeh", "Hazim Shakhatreh", "Issa Khalil", "Mohsen Guizani", "Eyad Almaita"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare neutral B meson decays to muon pairs constrain theories beyond the Standard Model, and which classes of new-physics models are most strongly affected?"}
{"doi": "https://doi.org/10.1038/s41568-019-0116-x", "title": "The evolving landscape of biomarkers for checkpoint inhibitor immunotherapy", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 2242, "authors": ["Jonathan J. Havel", "Diego Chowell", "Timothy A. Chan"], "abstract": "Immunotherapy is an effective precision medicine treatment for several cancers. Imaging signatures of the underlying genome (radiogenomics) in glioblastoma patients may serve as preoperative biomarkers of the tumor-host immune apparatus. Validated biomarkers would have the potential to stratify patients during immunotherapy clinical trials, and if trials are beneficial, facilitate personalized neo-adjuvant treatment. The increased use of whole genome sequencing data, and the advances in bioinformatics and machine learning make such developments plausible. We performed a systematic review to determine the extent of development and validation of immune-related radiogenomic biomarkers for glioblastoma. A systematic review was performed following PRISMA guidelines using the PubMed, Medline, and Embase databases. Qualitative analysis was performed by incorporating the QUADAS 2 tool and CLAIM checklist. PROSPERO registered CRD42022340968. Extracted data were insufficiently homogenous to perform a meta-analysis. Results Nine studies, all retrospective, were included. Biomarkers extracted from magnetic resonance imaging volumes of interest included apparent diffusion coefficient values, relative cerebral blood volume values, and image-derived features. These biomarkers correlated with genomic markers from tumor cells or immune cells or with patient survival. The majority of studies had a high risk of bias and applicability concerns regarding the index test performed. Radiogenomic immune biomarkers have the potential to provide early treatment options to patients with glioblastoma. Targeted immunotherapy, stratified by these biomarkers, has the potential to allow individualized neo-adjuvant precision treatment options in clinical trials. However, there are no prospective studies validating these biomarkers, and interpretation is limited due to study bias with little evidence of generalizability.", "arxiv_id": "2405.07858v1", "research_question": "What are the key barriers and necessary steps to validate and implement imaging-derived immune biomarkers for guiding immunotherapy in glioblastoma patients?"}
{"doi": "https://doi.org/10.1109/cvpr.2019.00584", "title": "Deep High-Resolution Representation Learning for Human Pose Estimation", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 5184, "authors": ["Jingdong Wang", "Dong Liu", "Bin Xiao", "Ke Sun"], "abstract": "The 2D human pose estimation (HPE) is a basic visual problem. However, its supervised learning requires massive keypoint labels, which is labor-intensive to collect. Thus, we aim at boosting a pose estimator by excavating extra unlabeled data with semi-supervised learning (SSL). Most previous SSHPE methods are consistency-based and strive to maintain consistent outputs for differently augmented inputs. Under this genre, we find that SSHPE can be boosted from two cores: advanced data augmentations and concise consistency training ways. Specifically, for the first core, we discover the synergistic effects of existing augmentations, and reveal novel paradigms for conveniently producing new superior HPE-oriented augmentations which can more effectively add noise on unlabeled samples. We can therefore establish paired easy-hard augmentations with larger difficulty gaps. For the second core, we propose to repeatedly augment unlabeled images with diverse hard augmentations, and generate multi-path predictions sequentially for optimizing multi-losses in a single network. This simple and compact design is interpretable, and easily benefits from newly found augmentations. Comparing to state-of-the-art SSL approaches, our method brings substantial improvements on public datasets. And we extensively validate the superiority and versatility of our approach on conventional human body images, overhead fisheye images, and human hand images. The code is released in https://github.com/hnuzhy/MultiAugs.", "arxiv_id": "2402.11566v3", "research_question": "How can I design and evaluate augmentations that create meaningful easy–hard difficulty gaps for improving semi-supervised 2D human pose estimation?"}
{"doi": "https://doi.org/10.1038/s41586-019-1679-0", "title": "Genetic strategies for improving crop yields", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1382, "authors": ["Julian I. Schroeder", "Jane E. Parker", "Julia Bailey‐Serres", "Elizabeth A. Ainsworth", "Giles Oldroyd"], "abstract": "Nowadays, precision agriculture combined with modern information and communications technologies, is becoming more common in agricultural activities such as automated irrigation systems, precision planting, variable rate applications of nutrients and pesticides, and agricultural decision support systems. In the latter, crop management data analysis, based on machine learning and data mining, focuses mainly on how to efficiently forecast and improve crop yield. In recent years, raw and semi-processed agricultural data are usually collected using sensors, robots, satellites, weather stations, farm equipment, farmers and agribusinesses while the Internet of Things (IoT) should deliver the promise of wirelessly connecting objects and devices in the agricultural ecosystem. Agricultural data typically captures information about farming entities and operations. Every farming entity encapsulates an individual farming concept, such as field, crop, seed, soil, temperature, humidity, pest, and weed. Agricultural datasets are spatial, temporal, complex, heterogeneous, non-standardized, and very large. In particular, agricultural data is considered as Big Data in terms of volume, variety, velocity and veracity. Designing and developing a data warehouse for precision agriculture is a key foundation for establishing a crop intelligence platform, which will enable resource efficient agronomy decision making and recommendations. Some of the requirements for such an agricultural data warehouse are privacy, security, and real-time access among its stakeholders (e.g., farmers, farm equipment manufacturers, agribusinesses, co-operative societies, customers and possibly Government agencies). However, currently there are very few reports in the literature that focus on the design of efficient data warehouses with the view of enabling Agricultural Big Data analysis and data mining. In this paper ...", "arxiv_id": "1807.00035v1", "research_question": "What are the main challenges and best practices for designing a scalable, secure, and real-time data warehouse that can integrate heterogeneous agricultural data from IoT devices, satellites, and farm management systems?"}
{"doi": "https://doi.org/10.7758/9781610448864", "title": "The Handbook of Research Synthesis and Meta-Analysis", "language": "en", "created_date": "2016-06-24T00:00:00", "publication_year": 2019, "cited_by_count": 4477, "authors": ["Harris Cooper", "Jeffrey C. Valentine", "Larry V. Hedges"], "abstract": "Despite technological and medical advances, the detection, interpretation, and treatment of cancer based on imaging data continue to pose significant challenges. These include inter-observer variability, class imbalance, dataset shifts, inter- and intra-tumour heterogeneity, malignancy determination, and treatment effect uncertainty. Given the recent advancements in Generative Adversarial Networks (GANs), data synthesis, and adversarial training, we assess the potential of these technologies to address a number of key challenges of cancer imaging. We categorise these challenges into (a) data scarcity and imbalance, (b) data access and privacy, (c) data annotation and segmentation, (d) cancer detection and diagnosis, and (e) tumour profiling, treatment planning and monitoring. Based on our analysis of 164 publications that apply adversarial training techniques in the context of cancer imaging, we highlight multiple underexplored solutions with research potential. We further contribute the Synthesis Study Trustworthiness Test (SynTRUST), a meta-analysis framework for assessing the validation rigour of medical image synthesis studies. SynTRUST is based on 26 concrete measures of thoroughness, reproducibility, usefulness, scalability, and tenability. Based on SynTRUST, we analyse 16 of the most promising cancer imaging challenge solutions and observe a high validation rigour in general, but also several desirable improvements. With this work, we strive to bridge the gap between the needs of the clinical cancer imaging community and the current and prospective research on data synthesis and adversarial networks in the artificial intelligence community.", "arxiv_id": "2107.09543v2", "research_question": "What are the key best practices and evaluation criteria for validating synthetic medical images generated by GANs so they can be trusted and safely used in clinical workflows?"}
{"doi": "https://doi.org/10.1016/s2213-2600(19)30198-5", "title": "Estimation of the global prevalence and burden of obstructive sleep apnoea: a literature-based analysis", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 3533, "authors": ["Adam Benjafield", "Mary J. Morrell", "Sanjeev Sinha", "Sérgio Tufik", "Atul Malhotra", "Jean‐Louis Pépin", "Peter R. Eastwood", "Kate Valentine", "Najib Ayas", "Sanjay R. Patel", "Carlos M. Nuñez", "Thomas Penzel", "Msm Ip", "Raphaël Heinzer", "Paul E. Peppard"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How can measurements of very rare neutral meson decays into lepton pairs be used to test the Standard Model and constrain theories of physics beyond the Standard Model?"}
{"doi": "https://doi.org/10.1016/c2015-1-01622-2", "title": "Principles of Bone Biology", "language": "en", "created_date": "2016-06-24T00:00:00", "publication_year": 2019, "cited_by_count": 1692, "authors": ["John P. Bilezikian", "Lawrence G. Raisz", "Gideon A. Rodan"], "abstract": "Background: Many mathematical models have now been employed across every area of systems biology. These models increasingly involve large numbers of unknown parameters, have complex structure which can result in substantial evaluation time relative to the needs of the analysis, and need to be compared to observed data. The correct analysis of such models usually requires a global parameter search, over a high dimensional parameter space, that incorporates and respects the most important sources of uncertainty. This can be an extremely difficult task, but it is essential for any meaningful inference or prediction to be made about any biological system. It hence represents a fundamental challenge for the whole of systems biology.\n  Results: Bayesian statistical methodology for the uncertainty analysis of complex models is introduced, which is designed to address the high dimensional global parameter search problem. Bayesian emulators that mimic the systems biology model but which are extremely fast to evaluate are embedded within an iterative history match: an efficient method to search high dimensional spaces within a more formal statistical setting, while incorporating major sources of uncertainty. The approach is demonstrated via application to two models of hormonal crosstalk in Arabidopsis root development, which have 32 rate parameters, for which we identify the sets of rate parameter values that lead to acceptable matches to observed trend data. The biological consequences of the resulting comparison, including the evaluation of gene functions, are described.", "arxiv_id": "1607.06358v2", "research_question": "How do Bayesian emulators combined with iterative history matching compare to standard approaches (e.g., MCMC) for efficiently and reliably exploring high-dimensional parameter spaces in computationally expensive biological models?"}
{"doi": "https://doi.org/10.1038/s41578-019-0142-z", "title": "Achieving high energy density and high power density with pseudocapacitive materials", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1759, "authors": ["Qiulong Wei", "Jonathan Lau", "David S. Ashby", "Ryan H. DeBlock", "Christopher Choi", "Bruce Dunn", "Danielle M. Butts"], "abstract": "We investigate the possibility of generating and studying turbulence in plasma by means of high-energy density laser-driven experiments. Our focus is to create supersonic, self-magnetized turbulence with characteristics that resemble those found in the interstellar medium (ISM).\n  We consider a target made of a spherical core surrounded by a shell made of denser material. The shell is irradiated by a sequence of laser pulses sending inward-propagating shocks that convert the inner core into plasma and create turbulence. In the context of the evolution of the ISM, the shocks play the role of supernova remnant shocks and the core represents the ionized interstellar medium. We consider the effects of both pre-existing and self-generating magnetic fields and study the evolution of the system by means of two-dimensional numerical simulations.\n  We find that the evolution of the turbulent core is generally, subsonic with rms-Mach number $M_t\\approx 0.2$. We observe an isotropic, turbulent velocity field with an inertial range power spectra of $P(k)\\propto k^{-2.3}$. We account for the effects of self-magnetization and find that the resulting magnetic field has characteristic strength $\\approx 3\\times 10^{4}$ G. The corresponding plasma beta is $\\approx 1\\times 10^{4}$--$1\\times 10^{5}$, indicating that the magnetic field does not play an important role in the dynamical evolution of the system.\n  The natural extension of this work is to study the system evolution in three-dimensions, with various laser drive configurations, and targets with shells and cores of different masses. The latter modification may help to increase the turbulent intensity and possibly create transonic turbulence. One of the key challenges is to obtain transonic turbulent conditions in a quasi-steady state environment.", "arxiv_id": "1312.3555v1", "research_question": "What experimental diagnostics and measurement techniques are typically used to characterize supersonic, self-magnetized turbulence in laser-driven laboratory plasma experiments, and how can those measurements be related to conditions in astrophysical plasmas like the interstellar medium?"}
{"doi": "https://doi.org/10.1371/journal.pone.0223994", "title": "Visualizing a field of research: A methodology of systematic scientometric reviews", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1186, "authors": ["Min Song", "Chaomei Chen"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B-meson decays into muon pairs constrain or inform models of physics beyond the Standard Model?"}
{"doi": "https://doi.org/10.1109/iccv.2019.00887", "title": "EGNet: Edge Guidance Network for Salient Object Detection", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1159, "authors": ["Jiangjiang Liu", "Deng-Ping Fan", "Jufeng Yang", "Ming‐Ming Cheng", "Yang Cao", "Jiaxing Zhao"], "abstract": "Salient object detection in complex scenes and environments is a challenging research topic. Most works focus on RGB-based salient object detection, which limits its performance of real-life applications when confronted with adverse conditions such as dark environments and complex backgrounds. Taking advantage of RGB and thermal infrared images becomes a new research direction for detecting salient object in complex scenes recently, as thermal infrared spectrum imaging provides the complementary information and has been applied to many computer vision tasks. However, current research for RGBT salient object detection is limited by the lack of a large-scale dataset and comprehensive benchmark. This work contributes such a RGBT image dataset named VT5000, including 5000 spatially aligned RGBT image pairs with ground truth annotations. VT5000 has 11 challenges collected in different scenes and environments for exploring the robustness of algorithms. With this dataset, we propose a powerful baseline approach, which extracts multi-level features within each modality and aggregates these features of all modalities with the attention mechanism, for accurate RGBT salient object detection. Extensive experiments show that the proposed baseline approach outperforms the state-of-the-art methods on VT5000 dataset and other two public datasets. In addition, we carry out a comprehensive analysis of different algorithms of RGBT salient object detection on VT5000 dataset, and then make several valuable conclusions and provide some potential research directions for RGBT salient object detection.", "arxiv_id": "2007.03262v6", "research_question": "What are the main challenges and advantages of combining RGB and thermal infrared data for salient object detection in real-world scenes?"}
{"doi": "https://doi.org/10.1038/s41580-019-0133-3", "title": "The Hsp70 chaperone network", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1149, "authors": ["Rina Rosenzweig", "Nadinath B. Nillegoda", "Matthias P. Mayer", "Bernd Bukau"], "abstract": "Hsp70 molecular chaperones are abundant ATP-dependent nanomachines that actively reshape non-native, misfolded proteins and assist a wide variety of essential cellular processes. Here we combine complementary computational/theoretical approaches to elucidate the structural and thermodynamic details of the chaperone-induced expansion of a substrate protein, with a particular emphasis on the critical role played by ATP hydrolysis. We first determine the conformational free-energy cost of the substrate expansion due to the binding of multiple chaperones using coarse-grained molecular simulations. We then exploit this result to implement a non-equilibrium rate model which estimates the degree of expansion as a function of the free energy provided by ATP hydrolysis. Our results are in quantitative agreement with recent single-molecule FRET experiments and highlight the stark non-equilibrium nature of the process, showing that Hsp70s are optimized to convert effectively chemical energy into mechanical work close to physiological conditions.", "arxiv_id": "1902.01612v2", "research_question": "How does ATP hydrolysis power Hsp70 chaperones to remodel and expand misfolded substrate proteins, and what factors determine the efficiency of converting that chemical energy into mechanical work?"}
{"doi": "https://doi.org/10.1186/s12943-019-1091-2", "title": "Novel immune checkpoint targets: moving beyond PD-1 and CTLA-4", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1122, "authors": ["Linping Xu", "Ming Yi", "Kongming Wu", "Suxia Luo", "Shuang Qin", "Shengnan Yu"], "abstract": "In a previous paper the authors argued the case for incorporating ideas from innate immunity into artificial immune systems (AISs) and presented an outline for a conceptual framework for such systems. A number of key general properties observed in the biological innate and adaptive immune systems were highlighted, and how such properties might be instantiated in artificial systems was discussed in detail. The next logical step is to take these ideas and build a software system with which AISs with these properties can be implemented and experimentally evaluated. This paper reports on the results of that step - the libtissue system.", "arxiv_id": "1004.2854v1", "research_question": "What are the key challenges and design considerations when building software frameworks that model innate immune system principles for use in artificial immune systems, and how can those challenges be addressed?"}
{"doi": "https://doi.org/10.1093/molbev/msz189", "title": "ModelTest-NG: A New and Scalable Tool for the Selection of DNA and Protein Evolutionary Models", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1905, "authors": ["David Posada", "Tomáš Flouri", "Benoît Morel", "Alexey M. Kozlov", "Alexandros Stamatakis", "Diego Darriba"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precision measurements of rare decays of neutral B mesons into muon pairs constrain theories beyond the Standard Model, and which classes of new-physics models are most strongly affected by such constraints?"}
{"doi": "https://doi.org/10.1016/j.jsis.2019.01.003", "title": "Understanding digital transformation: A review and a research agenda", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 5258, "authors": ["Grégory Vial"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "Why are rare decays of neutral B mesons into muon pairs considered particularly sensitive probes for physics beyond the Standard Model?"}
{"doi": "https://doi.org/10.12688/wellcomeopenres.15555.1", "title": "Guidelines for performing Mendelian randomization investigations", "language": "en", "created_date": "2022-05-12T00:00:00", "publication_year": 2019, "cited_by_count": 1773, "authors": ["Dipender Gill", "Michael V. Holmes", "Cosetta Minelli", "Neil M Davies", "Fernando Pires Hartwig", "Stephen Burgess", "Evropi Τheodoratou", "M. Maria Glymour", "Frank Dudbridge", "Caroline L. Relton", "George Davey Smith"], "abstract": "For a class of stochastic restart algorithms we address the effect of a nonzero level of randomization in maximizing the convergence rate for general energy landscapes. The resulting characterization of the optimal level of randomization is investigated computationally for random as well as parametric families of rugged energy landscapes.", "arxiv_id": "0406095v1", "research_question": "How should one choose the amount of randomization in a stochastic restart algorithm to reliably improve convergence across different types of rugged optimization landscapes?"}
{"doi": "https://doi.org/10.1609/aimag.v40i2.2850", "title": "DARPA's Explainable Artificial Intelligence Program", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1074, "authors": ["David W. Aha", "David Gunning"], "abstract": "Explainable Artificial Intelligence (XAI) is essential for building advanced machine learning-powered applications, especially in critical domains such as medical diagnostics or autonomous driving. Legal, business, and ethical requirements motivate using effective XAI, but the increasing number of different methods makes it challenging to pick the right ones. Further, as explanations are highly context-dependent, measuring the effectiveness of XAI methods without users can only reveal a limited amount of information, excluding human factors such as the ability to understand it. We propose to evaluate XAI methods via the user's ability to successfully perform a proxy task, designed such that a good performance is an indicator for the explanation to provide helpful information. In other words, we address the helpfulness of XAI for human decision-making. Further, a user study on state-of-the-art methods was conducted, showing differences in their ability to generate trust and skepticism and the ability to judge the rightfulness of an AI decision correctly. Based on the results, we highly recommend using and extending this approach for more objective-based human-centered user studies to measure XAI performance in an end-to-end fashion.", "arxiv_id": "2410.11896v1", "research_question": "How can proxy tasks be designed so they reliably measure whether AI explanations actually improve human decision-making across different domains and user groups?"}
{"doi": "https://doi.org/10.1109/cvprw.2019.00247", "title": "EDVR: Video Restoration With Enhanced Deformable Convolutional Networks", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1155, "authors": ["Kelvin C. K. Chan", "Chao Dong", "Ke Yu", "Xintao Wang", "Chen Change Loy"], "abstract": "Video restoration tasks, including super-resolution, deblurring, etc, are drawing increasing attention in the computer vision community. A challenging benchmark named REDS is released in the NTIRE19 Challenge. This new benchmark challenges existing methods from two aspects: (1) how to align multiple frames given large motions, and (2) how to effectively fuse different frames with diverse motion and blur. In this work, we propose a novel Video Restoration framework with Enhanced Deformable networks, termed EDVR, to address these challenges. First, to handle large motions, we devise a Pyramid, Cascading and Deformable (PCD) alignment module, in which frame alignment is done at the feature level using deformable convolutions in a coarse-to-fine manner. Second, we propose a Temporal and Spatial Attention (TSA) fusion module, in which attention is applied both temporally and spatially, so as to emphasize important features for subsequent restoration. Thanks to these modules, our EDVR wins the champions and outperforms the second place by a large margin in all four tracks in the NTIRE19 video restoration and enhancement challenges. EDVR also demonstrates superior performance to state-of-the-art published methods on video super-resolution and deblurring. The code is available at https://github.com/xinntao/EDVR.", "arxiv_id": "1905.02716v1", "research_question": "What are common techniques for aligning and fusing multiple video frames to perform restoration when there are large inter-frame motions and varying amounts of blur, and what are the main trade-offs between them?"}
{"doi": "https://doi.org/10.1007/s10994-019-05855-6", "title": "A survey on semi-supervised learning", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 2322, "authors": ["Jesper E. van Engelen", "Holger H. Hoos"], "abstract": "Recent semi-supervised learning methods have shown to achieve comparable results to their supervised counterparts while using only a small portion of labels in image classification tasks thanks to their regularization strategies. In this paper, we take a more direct approach for semi-supervised learning and propose learning to impute the labels of unlabeled samples such that a network achieves better generalization when it is trained on these labels. We pose the problem in a learning-to-learn formulation which can easily be incorporated to the state-of-the-art semi-supervised techniques and boost their performance especially when the labels are limited. We demonstrate that our method is applicable to both classification and regression problems including image classification and facial landmark detection tasks.", "arxiv_id": "1912.10364v3", "research_question": "What strategies and safeguards can be used when imputing labels for unlabeled data to ensure the imputed labels improve model generalization rather than propagate noise or bias?"}
{"doi": "https://doi.org/10.1136/ebmental-2019-300117", "title": "How to perform a meta-analysis with R: a practical tutorial", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 4936, "authors": ["Guido Schwarzer", "Sara Balduzzi", "Gerta Rücker"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B meson decays into muon pairs constrain or rule out models of physics beyond the Standard Model?"}
{"doi": "https://doi.org/10.1021/acs.chemrev.9b00201", "title": "Particulate Photocatalysts for Light-Driven Water Splitting: Mechanisms, Challenges, and Design Strategies", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 2466, "authors": ["Qian Wang", "Kazunari Domen"], "abstract": "We discuss the role of water bridging the DNA-enzyme interaction by resorting to recent results showing that London dispersion forces between delocalized electrons of base pairs of DNA are responsible for the formation of dipole modes that can be recognized by \\textit{Taq} polymerase. We describe the dynamic origin of the high efficiency and precise targeting of \\textit{Taq} activity in PCR. The spatiotemporal distribution of interaction couplings, frequencies, amplitudes, and phase modulations comprise a pattern of fields which constitutes the electromagnetic image of DNA in the surrounding water, which is what the polymerase enzyme actually recognizes in the DNA water environment. The experimental realization of PCR amplification, achieved through replacement of the DNA template by the treatment of pure water with electromagnetic signals recorded from viral and bacterial DNA solutions, is found consistent with the gauge theory paradigm of quantum fields.", "arxiv_id": "1804.02436v1", "research_question": "How could structured or polarized water transmit or store electromagnetic patterns produced by biomolecules so that other biomolecules could recognize those patterns, and what physical mechanisms and experimental evidence support this idea?"}
{"doi": "https://doi.org/10.1126/science.aaw1944", "title": "Global trends in antimicrobial resistance in animals in low- and middle-income countries", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1016, "authors": ["Sebastian Bonhoeffer", "Ramanan Laxminarayan", "Cheng Zhao", "João Pires", "Reshma Silvester", "Julia Song", "Thomas P. Van Boeckel", "Nicola G. Criscuolo", "Marius Gilbert"], "abstract": "This study examined the relationship between trade facilitation and economic growth among the middle-income countries from 2010 to 2020 using 94 countries made up of 48 lower-middle-income countries and 46 upper-middle-income countries. The study utilized both difference and system Generalised Method of Moments (GMM) since the cross-sections (N) were greater than the periods (T). The study found that container port traffic, quality of trade and transport-related infrastructure have a strong influence on imports and exports of goods and national income while trade tariff hurts the growth of the countries. The study also found that most of the trade facilitation indicators indicated a weak positive influence on trade flows and economic growth. Based on these findings, the study recommends that reforms aimed at significantly lowering the costs of trading across borders among middle-income countries should be highly prioritized in policy formulations, with a focus on the export side by reducing at-the-border documentation, time, and real costs of trading across borders while the international organizations should continue to report the set of Trade Facilitation Indicators (TFIs) that identify areas for action and enable the potential impact of reforms to be assessed.", "arxiv_id": "2204.11088v1", "research_question": "What practical policy measures can middle-income countries implement to reduce at-the-border trade costs and improve their export competitiveness?"}
{"doi": "https://doi.org/10.1126/science.aau8650", "title": "The NASA Twins Study: A multidimensional analysis of a year-long human spaceflight", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1017, "authors": ["Varsha Rao", "Steven S. Laurie", "Mark Maienschein‐Cline", "Jad Nasrini", "Rakel Tryggvadóttir", "Sara R. Zwart", "George E. Chlipala", "Graham Scott", "Scott M. Smith", "Stefan J. Green", "Michael B. Stenger", "Jorge Gandara", "Craig E. Kundrot", "Colin M. Callahan", "Aditya Ambati", "Martina Heer", "Jason I. Feinberg", "Songjie Chen", "Rintaro Saito", "M Snyder", "Miles J. McKenna", "Augustine M.K. Choi", "K. George", "Tyler M. Moore", "Jing Zhang", "Brian Crucian", "Brandon R. Macias", "Vivian Hook", "Brittany Lee‐McMullen", "Kumar Sharma", "Stuart M. C. Lee", "Sarah B. Lumpkins", "John Goutsias", "John B. Charles", "Christopher E. Mason", "Immaculata De Vivo", "Lindsay F. Rizzardi", "Ari Melnick", "Peng Jiang", "Marisa Covington", "Garrett Jenkinson", "Jamila H. Siamwala", "Alexander E. Urban", "Ling Lin", "Douglas Ebert", "Mathias Basner", "Emmanuel Mignot", "David F. Dinges", "Cem Meydan", "Dorothy D. Sears", "Ali Keshavarzian", "Martha Hotz Vitaterna", "Maneesh Arya", "Jan M. Schilling", "Francine E. Garrett-Bakelman", "Robert A. Pietrzyk", "Tejaswini Mishra", "George S. Grills", "Caroline Sheridan", "Manjula Darshi", "Lynn Taylor", "Kévin Contrepois", "Andrew N. Hoofnagle", "Michael G. Ziegler", "Ebrahim Afshinnekoo", "Brinda K. Rana", "Brian Piening", "Hemal H. Patel", "Ruben C. Gur", "Maryam Afkarian", "Fred W. Turek", "Daniela Bezdan", "Andrew P. Feinberg", "Matthew MacKay", "Susan M. Bailey", "Sara Ahadi", "Alan R. Hargens", "Ryan P. Hillary", "Benjamin Van Espen", "Denis Salins", "Kiichi Nakahira", "Tomáš Vaisar"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of the branching fractions for rare neutral B meson decays to muon pairs test the Standard Model and constrain theories beyond it?"}
{"doi": "https://doi.org/10.1109/cvpr.2019.00828", "title": "D2-Net: A Trainable CNN for Joint Description and Detection of Local Features", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1068, "authors": ["Marc Pollefeys", "Torsten Sattler", "Tomáš Pajdla", "Josef Šivic", "Ignacio Rocco", "Akihiko Torii", "Mihai Dusmanu"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B-meson decays to muon pairs constrain extensions of the Standard Model, and which classes of new-physics models are most tightly constrained by these results?"}
{"doi": "https://doi.org/10.1016/j.scitotenv.2019.03.368", "title": "Evidence of microplastic accumulation in agricultural soils from sewage sludge disposal", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1295, "authors": ["Raúl Eguiluz", "Violette Geissen", "Pablo Meza", "Francisco Casado", "Esperanza Huerta Lwanga", "Fabio Corradini"], "abstract": "Sewage sludge, a biosolid product of wastewater processing, is an often-overlooked source of rich organic waste. Hydrothermal processing (HTP), which uses heat and pressure to convert biomass into various solid, liquid, and gaseous products, has shown promise in converting sewage sludge into new materials with potential application in biofuels, asphalt binders, and bioplastics. In this study we focus on hydrochar, the carbonaceous HTP solid phase, and investigate its use as a bio-based filler in additive manufacturing technologies. We explore the impact of HTP and subsequent thermal activation on chemical and structural properties of sewage sludge and discuss the role of atypical metallic and metalloid dopants in organic material processing. In additive manufacturing composites, although the addition of hydrochar generally decreases mechanical performance, we show that toughness and strain can be recovered with hierarchical microstructures, much like biological materials that achieve outstanding properties by architecting relatively weak building blocks.", "arxiv_id": "2401.12591v1", "research_question": "How can hierarchical microstructures be designed or implemented in composite materials to recover toughness and ductility when relatively weak bio-based fillers (like hydrochar) are used in additive manufacturing?"}
{"doi": "https://doi.org/10.1038/s41578-019-0148-6", "title": "Hydrogel microparticles for biomedical applications", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1119, "authors": ["Tatiana Segura", "Lindsay Riley", "Andrew C. Daly", "Jason A. Burdick"], "abstract": "The advent of dedicated Deep Learning (DL) accelerators and neuromorphic processors has brought on new opportunities for applying both Deep and Spiking Neural Network (SNN) algorithms to healthcare and biomedical applications at the edge. This can facilitate the advancement of medical Internet of Things (IoT) systems and Point of Care (PoC) devices. In this paper, we provide a tutorial describing how various technologies including emerging memristive devices, Field Programmable Gate Arrays (FPGAs), and Complementary Metal Oxide Semiconductor (CMOS) can be used to develop efficient DL accelerators to solve a wide variety of diagnostic, pattern recognition, and signal processing problems in healthcare. Furthermore, we explore how spiking neuromorphic processors can complement their DL counterparts for processing biomedical signals. The tutorial is augmented with case studies of the vast literature on neural network and neuromorphic hardware as applied to the healthcare domain. We benchmark various hardware platforms by performing a sensor fusion signal processing task combining electromyography (EMG) signals with computer vision. Comparisons are made between dedicated neuromorphic processors and embedded AI accelerators in terms of inference latency and energy. Finally, we provide our analysis of the field and share a perspective on the advantages, disadvantages, challenges, and opportunities that various accelerators and neuromorphic processors introduce to healthcare and biomedical domains.", "arxiv_id": "2007.05657v2", "research_question": "What are the main trade-offs (in terms of energy efficiency, latency, robustness, and ease of development) when choosing between neuromorphic processors and conventional deep-learning accelerators for real-time biomedical signal processing at the edge?"}
{"doi": "https://doi.org/10.2991/jegh.k.191028.001", "title": "Epidemiology of Type 2 Diabetes – Global Burden of Disease and Forecasted Trends", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 2849, "authors": ["Halla Mustafa", "Moien AB Khan", "Romona Devi Govender", "Juma Al Kaabi", "Muhammad Jawad Hashim", "Jeffrey King"], "abstract": "Solar radio type II bursts serve as early indicators of incoming geo-effective space weather events such as coronal mass ejections (CMEs). In order to investigate the origin of high-frequency type II bursts (HF type II bursts), we have identified 51 of them (among 180 type II bursts from SWPC reports) that are observed by ground-based Compound Astronomical Low-cost Low-frequency Instrument for Spectroscopy and Transportable Observatory (CALLISTO) spectrometers and whose upper-frequency cutoff (of either fundamental or harmonic emission) lies in between 150 MHz-450 MHz during 2010-2019. We found that 60% of HF type II bursts, whose upper-frequency cutoff $\\geq$ 300 MHz originate from the western longitudes. Further, our study finds a good correlation $\\sim $ 0.73 between the average shock speed derived from the radio dynamic spectra and the corresponding speed from CME data. Also, we found that analyzed HF type II bursts are associated with wide and fast CMEs located near the solar disk. In addition, we have analyzed the spatio-temporal characteristics of two of these high-frequency type II bursts and compared the derived from radio observations with those derived from multi-spacecraft CME observations from SOHO/LASCO and STEREO coronagraphs.", "arxiv_id": "2106.09310v1", "research_question": "What physical mechanisms generate high-frequency solar type II radio bursts, and how do the observed burst frequencies inform us about the height, speed, and properties of the associated coronal shocks and CMEs?"}
{"doi": "https://doi.org/10.1056/nejmoa1816714", "title": "Pembrolizumab plus Axitinib versus Sunitinib for Advanced Renal-Cell Carcinoma", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 3200, "authors": ["V.P. Stus", "Yen‐Hwa Chang", "Rodolfo F. Perini", "Robert D. Hawkins", "Delphine Borchiellini", "Maurice Markus", "Anna Kryzhanivska", "Bohuslav Melichar", "Dmitry Nosov", "Rustem Gafanov", "Frédéric Pouliot", "Ihor Vynnychenko", "Satoshi Tamada", "Sérgio Jobim Azevedo", "Ray McDermott", "Sophie Tartas", "Qiong Shou", "Chen Mei", "B. Yа. Alekseev", "Thomas Powles", "Igor Bondarenko", "Michael B. Atkins", "Elizabeth R. Plimack", "Cezary Szczylik", "Brian I. Rini", "Denis Soulières", "Jens Bedke"], "abstract": "Renal Cell Carcinoma is typically asymptomatic at the early stages for many patients. This leads to a late diagnosis of the tumor, where the curability likelihood is lower, and makes the mortality rate of Renal Cell Carcinoma high, with respect to its incidence rate. To increase the survival chance, a fast and correct categorization of the tumor subtype is paramount. Nowadays, computerized methods, based on artificial intelligence, represent an interesting opportunity to improve the productivity and the objectivity of the microscopy-based Renal Cell Carcinoma diagnosis. Nonetheless, much of their exploitation is hampered by the paucity of annotated dataset, essential for a proficient training of supervised machine learning technologies. This study sets out to investigate a novel self supervised training strategy for machine learning diagnostic tools, based on the multi-resolution nature of the histological samples. We aim at reducing the need of annotated dataset, without significantly reducing the accuracy of the tool. We demonstrate the classification capability of our tool on a whole slide imaging dataset for Renal Cancer subtyping, and we compare our solution with several state-of-the-art classification counterparts.", "arxiv_id": "2411.09471v2", "research_question": "How can self-supervised learning leverage multi-resolution histopathology images to reduce the need for annotated data while still maintaining diagnostic accuracy for tumor subtype classification?"}
{"doi": "https://doi.org/10.1038/s41588-019-0538-0", "title": "Activity-by-contact model of enhancer–promoter regulation from thousands of CRISPR perturbations", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1096, "authors": ["Rockwell Anyoha", "Neva C. Durand", "Glen Munson", "Vidya Subramanian", "Michael Kane", "Sharon R. Grossman", "Caleb A. Lareau", "Drew T. Bergman", "Erez Lieberman Aiden", "Thouis R. Jones", "Eric S. Lander", "J Engreitz", "Joseph Nasser", "Elizabeth M. Perez", "Benjamin R. Doughty", "Charles P. Fulco", "Elena K. Stamenova", "Tung H. Nguyen", "Tejal A. Patwardhan"], "abstract": "Eccentric planets may spend a significant portion of their orbits at large distances from their host stars, where low temperatures can cause atmospheric CO2 to condense out onto the surface, similar to the polar ice caps on Mars. The radiative effects on the climates of these planets throughout their orbits would depend on the wavelength-dependent albedo of surface CO2 ice that may accumulate at or near apoastron and vary according to the spectral energy distribution of the host star. To explore these possible effects, we incorporated a CO2 ice-albedo parameterization into a one-dimensional energy balance climate model. With the inclusion of this parameterization, our simulations demonstrated that F-dwarf planets require 29% more orbit-averaged flux to thaw out of global water ice cover compared with simulations that solely use a traditional pure water ice-albedo parameterization. When no eccentricity is assumed, and host stars are varied, F-dwarf planets with higher bond albedos relative to their M-dwarf planet counterparts require 30% more orbit-averaged flux to exit a water snowball state. Additionally, the intense heat experienced at periastron aids eccentric planets in exiting a snowball state with a smaller increase in instellation compared with planets on circular orbits; this enables eccentric planets to exhibit warmer conditions along a broad range of instellation. This study emphasizes the significance of incorporating an albedo parameterization for the formation of CO2 ice into climate models to accurately assess the habitability of eccentric planets, as we show that, even at moderate eccentricities, planets with Earth-like atmospheres can reach surface temperatures cold enough for the condensation of CO2 onto their surfaces, as can planets receiving low amounts of instellation on circular orbits.", "arxiv_id": "2501.11667v1", "research_question": "How does the spectral type of a host star influence the radiative impact of surface CO2 ice on an exoplanet’s climate and habitability?"}
{"doi": "https://doi.org/10.1038/s41568-019-0133-9", "title": "Molecular subtypes of small cell lung cancer: a synthesis of human and mouse model data", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1182, "authors": ["Lauren A. Byers", "Vito Quaranta", "Adi F. Gazdar", "Charles M. Rudin", "Roman K. Thomas", "John T. Poirier", "Jane E. Johnson", "Trudy G. Oliver", "Christopher R. Vakoc", "Afshin Dowlati", "Caroline Dive", "John V. Heymach", "David MacPherson", "Pierre P. Massion", "Julie George", "John D. Minna", "Jonathan M. Lehman", "Julien Sage"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B meson decays to muon pairs constrain theories beyond the Standard Model, and which classes of new-physics models are most strongly affected?"}
{"doi": "https://doi.org/10.1177/1744987119880234", "title": "An overview of the qualitative descriptive design within nursing research", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1456, "authors": ["Louise Doyle", "Brian Keogh", "Catherine McCabe", "Margaret McCann", "Anne‐Marie Brady"], "abstract": "Computational developments--particularly artificial intelligence--are reshaping social scientific research and raise new questions for in-depth methods such as ethnography and qualitative interviewing. Building on classic debates about computers in qualitative data analysis (QDA), we revisit possibilities and dangers in an era of automation, Large Language Model (LLM) chatbots, and 'big data.' We introduce a typology of contemporary approaches to using computers in qualitative research: streamlining workflows, scaling up projects, hybrid analytical methods, the sociology of computation, and technological rejection. Drawing from scaled team ethnographies and solo research integrating computational social science (CSS), we describe methodological choices across study lifecycles, from literature reviews through data collection, coding, text retrieval, and representation. We argue that new technologies hold potential to address longstanding methodological challenges when deployed with knowledge, purpose, and ethical commitment. Yet a pragmatic approach--moving beyond technological optimism and dismissal--is essential given rapidly changing tools that are both generative and dangerous. Computation now saturates research infrastructure, from algorithmic literature searches to scholarly metrics, making computational literacy a core methodological competence in and beyond sociology. We conclude that when used carefully and transparently, contemporary computational tools can meaningfully expand--rather than displace--the irreducible insights of qualitative research.", "arxiv_id": "2509.12503v5", "research_question": "How can qualitative researchers responsibly integrate AI tools like large language models into their workflows while preserving interpretive depth, ensuring transparency, and addressing ethical risks such as bias and data privacy?"}
{"doi": "https://doi.org/10.1038/s41560-019-0405-3", "title": "Challenges and opportunities towards fast-charging battery materials", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1606, "authors": ["Yayuan Liu", "Yangying Zhu", "Yi Cui"], "abstract": "This chapter illustrates the use of defect physics as a conceptual and theoretical framework for understanding and designing battery materials. It starts with a methodology for first-principles studies of defects in complex transition-metal oxides. The chapter then considers defects that are activated in a cathode material during synthesis, during measurements, and during battery use. Through these cases, it discusses possible defect landscapes in the material and their implications, guidelines for materials design via defect-controlled synthesis, mechanisms for electronic and ionic conduction and for electrochemical extraction and (re-)insertion, and effects of doping. Although specific examples are taken from studies of battery cathode materials, the computational approach and discussions are general and applicable to any ionic, electronic, or mixed ionic-electronic conducting materials.", "arxiv_id": "2211.04977v2", "research_question": "How do different types of point defects affect ionic and electronic conduction in battery cathode materials, and what practical strategies can be used during synthesis to control those defects?"}
{"doi": "https://doi.org/10.1038/s41591-019-0495-2", "title": "Supplementation with Akkermansia muciniphila in overweight and obese human volunteers: a proof-of-concept exploratory study", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 2008, "authors": ["Céline Druart", "Amandine Everard", "Marie de Barsy", "Jean-Paul Thissen", "Gwen Falony", "Sara Vieira‐Silva", "Audrey Loumaye", "Dominique Maiter", "Michel P. Hermans", "Patrice D. Cani", "Hubert Plovier", "Matthias Van Hul", "Jeroen Raes", "Nathalie M. Delzenne", "Willem M. de Vos", "Clara Depommier"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B meson decays to muon pairs test the Standard Model and constrain possible beyond–Standard-Model theories?"}
{"doi": "https://doi.org/10.1038/s41579-019-0299-x", "title": "Evolutionary classification of CRISPR–Cas systems: a burst of class 2 and derived variants", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 2260, "authors": ["Yuri I. Wolf", "David R. Cheng", "Emmanuelle Charpentier", "Kira S. Makarova", "Virginijus Šikšnys", "Alexander F. Yakunin", "Michael P. Terns", "Winston X. Yan", "Česlovas Venclovas", "Jaime Iranzo", "Shiraz A. Shah", "Philippe Horvath", "Omer S. Alkhnbashi", "Rodolphe Barrangou", "Daniel H. Haft", "Eugene V. Koonin", "Sylvain Moineau", "Roger A. Garrett", "David Scott", "Sergey Shmakov", "John van der Oost", "Francisco J. M. Mojica", "Rolf Backofen", "Stan J. J. Brouns", "Feng Zhang", "Malcolm F. White"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "Why are precise measurements of rare decays like B_s→μ+μ− and B^0→μ+μ− particularly sensitive probes for physics beyond the Standard Model, and how do their branching fractions constrain new theories?"}
{"doi": "https://doi.org/10.1186/s12859-019-3019-7", "title": "HH-suite3 for fast remote homology detection and deep protein annotation", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1177, "authors": ["Johannes Söding", "Harald Vöhringer", "Markus Meier", "Milot Mirdita", "Stephan J. Haunsberger", "Martin Steinegger"], "abstract": "Oriented object detection is a fundamental yet challenging task in remote sensing (RS), aiming to locate and classify objects with arbitrary orientations. Recent advancements in deep learning have significantly enhanced the capabilities of oriented object detection methods. Given the rapid development of this field, a comprehensive survey of the recent advances in oriented object detection is presented in this paper. Specifically, we begin by tracing the technical evolution from horizontal object detection to oriented object detection and highlighting the specific related challenges, including feature misalignment, spatial misalignment, oriented bounding box (OBB) regression problems, and common issues encountered in RS. Subsequently, we further categorize the existing methods into detection frameworks, OBB regression techniques, feature representation approaches, and solutions to common issues and provide an in-depth discussion of how these methods address the above challenges. In addition, we cover several publicly available datasets and evaluation protocols. Furthermore, we provide a comprehensive comparison and analysis involving the state-of-the-art methods. Toward the end of this paper, we identify several future directions for oriented object detection research.", "arxiv_id": "2302.10473v6", "research_question": "What practical strategies can be used when annotating and training datasets to handle rotated objects and orientation ambiguities for oriented object detection in remote sensing?"}
{"doi": "https://doi.org/10.1542/peds.2019-2528", "title": "Clinical Practice Guideline for the Diagnosis, Evaluation, and Treatment of Attention-Deficit/Hyperactivity Disorder in Children and Adolescents", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2019, "cited_by_count": 1165, "authors": ["Mark L. Wolraich", "Marian F. Earls", "Steven W. Evans", "Joseph F. Hagan", "Christoph U. Lehmann", "Dale Davison", "Tanya E. Froehlich", "Joseph R. Holbrook", "Jonathan D. Winner", "Susan K. Flinn", "Herschel R. Lessin", "Kymika Okechukwu", "Karen Pierce", "Eugenia Chan", "William Zurhellen", "Jennifer Frost", "Carla Allan"], "abstract": "The deficit of executive functioning was found to be associated with attention deficit hyperactivity disorder (ADHD) in general and its subtypes. One of the important functions of central executive is the ability simultaneously coordinate two tasks. The study aimed at defining the dual-task performance characteristics in healthy children and adolescents on the computerised and the paper and pencil dual-task methods; investigating the effect of task difficulty on dual-task performance in ADHD in comparison to age and years of education matched healthy controls; testing if the paper and pencil version of the dual-task method is giving the same results in ADHD and healthy controls; investigating whether the dual-task functioning in ADHD is defined by the deficits in the general motor functioning and comorbidity factors. The study investigated dual task functioning in 6-16 years old 91 typically developing controls and 91 children with ADHD. It was found that: (1) the dual-task coordination is available in children and adolescents with ADHD in general and in its subtypes and not significantly different from performance of age and years of education matched healthy controls; (2) Increase of the task difficulty in dual-task paradigm don't affect disproportionately children and adolescents with ADHD in comparison to age and years of education matched healthy controls; (3) The paper and pencil version of the dual-task method is giving the same results in ADHD and healthy controls as computerised version; (4) The dual-task functioning in ADHD in general and in its subtypes is not defined by the general motor functioning while in healthy controls dual task performance is associated with the general motor functioning level; (5) The dual-task functioning in ADHD in general and in its subtypes is not defined by the comorbidity factors.", "arxiv_id": "1101.1858v1", "research_question": "Can dual-task assessments reliably detect executive function deficits in children with ADHD, and are paper-and-pencil versions as valid as computerized tests for this purpose?"}
{"doi": "https://doi.org/10.1002/aenm.201702657", "title": "Recent Progress of the Solid‐State Electrolytes for High‐Energy Metal‐Based Batteries", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 1108, "authors": ["Shuya Wei", "Lei Fan", "Yingying Lü", "Qi Li", "Siyuan Li"], "abstract": "Abstract Secondary batteries based on metal anodes (e.g., Li, Na, Mg, Zn, and Al) are among the most sought‐after candidates for next‐generation mobile and stationary storage systems because they are able to store a larger amount of energy per unit mass or volume. However, unstable electrodeposition and uncontrolled interfacial reactions occuring in liquid electrolytes cause unsatisfying cell performance and potential safety concerns for the commercial application of these metal anodes. Solid‐state electrolytes (SSEs) having a higher modulus are considered capable of inhibiting difficulties associated with the anodes and may enable building of safe all‐solid‐state metal batteries, yet several challenges, such as insufficient room‐temperature ionic conductivity and poor interfacial stability between the electrode and the electrolyte, hinder the large‐scale development of such batteries. Here, research and development of SSEs including inorganic ceramics, organic solid polymers, and organic–inorganic hybrid/composite materials for metal‐based batteries are reviewed. The comparison of different types of electrolytes is discussed in detail, in the context of electrochemical energy storage applications. Then, the focus of this study is on recent advances in a range of attractive and innovative battery chemistries and technologies that are enabled by SSEs. Finally, the challenges and future perspectives are outlined to foresee the development of SSEs.", "research_question": "What are the most promising strategies to improve room-temperature ionic conductivity in solid-state electrolytes while maintaining good mechanical strength and stable interfacial contact with metal anodes?"}
{"doi": "https://doi.org/10.1037/met0000167", "title": "A tutorial on regularized partial correlation networks.", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 2347, "authors": ["Eiko I. Fried", "Sacha Epskamp"], "abstract": "Recent years have seen an emergence of network modeling applied to moods, attitudes, and problems in the realm of psychology. In this framework, psychological variables are understood to directly affect each other rather than being caused by an unobserved latent entity. In this tutorial, we introduce the reader to estimating the most popular network model for psychological data: the partial correlation network. We describe how regularization techniques can be used to efficiently estimate a parsimonious and interpretable network structure in psychological data. We show how to perform these analyses in R and demonstrate the method in an empirical example on posttraumatic stress disorder data. In addition, we discuss the effect of the hyperparameter that needs to be manually set by the researcher, how to handle non-normal data, how to determine the required sample size for a network analysis, and provide a checklist with potential solutions for problems that can arise when estimating regularized partial correlation networks. (PsycINFO Database Record (c) 2018 APA, all rights reserved).", "research_question": "How should researchers choose the regularization hyperparameter when estimating partial correlation networks for psychological data, and what practical strategies can they use to assess how sensitive their network structure is to that choice?"}
{"doi": "https://doi.org/10.1056/nejmoa1802357", "title": "Adjuvant Pembrolizumab versus Placebo in Resected Stage III Melanoma", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 1753, "authors": ["Mikhail Lichinitser", "Alexander M.M. Eggermont", "Rahima Jamal", "Victoria Atkinson", "Matteo S. Carlino", "Paolo A. Ascierto", "Dirk Schadendorf", "Nageatte Ibrahim", "Mario Mandalà", "Alexander C.J. van Akkooi", "Shahneen Sandhu", "Susana Puig", "Sandrine Marréaud", "Michele Maio", "Jean-Jacques Grob", "Stefan Suciu", "Ralf Gutzmer", "Alfonsus Johannes Maria van den Eertwegh", "Rutger H.T. Koornstra", "Paul Lorigan", "Andrew Haydon", "Christian U. Blank", "James Larkin", "Georgina V. Long", "Leonel F. Hernandez‐Aya", "Adnan Khattak", "Caroline Robert", "Piotr Rutkowski", "Stéphane Dalle"], "abstract": "As adjuvant therapy for high-risk stage III melanoma, 200 mg of pembrolizumab administered every 3 weeks for up to 1 year resulted in significantly longer recurrence-free survival than placebo, with no new toxic effects identified. (Funded by Merck; ClinicalTrials.gov number, NCT02362594 ; EudraCT number, 2014-004944-37 .).", "research_question": "What factors determine whether a patient with stage III melanoma should receive adjuvant PD‑1 inhibitor therapy, and how are the potential benefits weighed against the risks?"}
{"doi": "https://doi.org/10.1177/2053951718756684", "title": "Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 1018, "authors": ["Min Kyung Lee"], "abstract": "Algorithms increasingly make managerial decisions that people used to make. Perceptions of algorithms, regardless of the algorithms' actual performance, can significantly influence their adoption, yet we do not fully understand how people perceive decisions made by algorithms as compared with decisions made by humans. To explore perceptions of algorithmic management, we conducted an online experiment using four managerial decisions that required either mechanical or human skills. We manipulated the decision-maker (algorithmic or human), and measured perceived fairness, trust, and emotional response. With the mechanical tasks, algorithmic and human-made decisions were perceived as equally fair and trustworthy and evoked similar emotions; however, human managers' fairness and trustworthiness were attributed to the manager's authority, whereas algorithms' fairness and trustworthiness were attributed to their perceived efficiency and objectivity. Human decisions evoked some positive emotion due to the possibility of social recognition, whereas algorithmic decisions generated a more mixed response – algorithms were seen as helpful tools but also possible tracking mechanisms. With the human tasks, algorithmic decisions were perceived as less fair and trustworthy and evoked more negative emotion than human decisions. Algorithms' perceived lack of intuition and subjective judgment capabilities contributed to the lower fairness and trustworthiness judgments. Positive emotion from human decisions was attributed to social recognition, while negative emotion from algorithmic decisions was attributed to the dehumanizing experience of being evaluated by machines. This work reveals people's lay concepts of algorithmic versus human decisions in a management context and suggests that task characteristics matter in understanding people's experiences with algorithmic technologies.", "research_question": "How can organizations design and present algorithmic decision-making systems to increase perceived fairness and trust, especially for tasks that people view as requiring human judgment or social recognition?"}
{"doi": "https://doi.org/10.1016/j.drudis.2018.01.039", "title": "The rise of deep learning in drug discovery", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 1619, "authors": ["Hongming Chen", "Marcus Olivecrona", "Thomas Blaschke", "Yinhai Wang", "Ola Engkvist"], "abstract": "Over the past decade, deep learning has achieved remarkable success in various artificial intelligence research areas. Evolved from the previous research on artificial neural networks, this technology has shown superior performance to other machine learning algorithms in areas such as image and voice recognition, natural language processing, among others. The first wave of applications of deep learning in pharmaceutical research has emerged in recent years, and its utility has gone beyond bioactivity predictions and has shown promise in addressing diverse problems in drug discovery. Examples will be discussed covering bioactivity prediction, de novo molecular design, synthesis prediction and biological image analysis.", "research_question": "What are the main practical challenges and limitations when applying deep learning to different stages of drug discovery (e.g., bioactivity prediction, molecular design, synthesis planning, and biological image analysis), and how can researchers address them?"}
{"doi": "https://doi.org/10.1201/9781315137438", "title": "Analysis of Survival Data", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 5999, "authors": ["D. R. Cox", "David Oakes"], "abstract": "The objective of this book is to give a concise account of the analysis of survival data. The book is intended both for the applied statistician and for a wider statistical audience wanting an introduction to this field. Particular attention is paid to new theory on the relationship between survival factors and identified explanatory variables. Each chapter concludes with bibliographic notes and outline statements of further results that can be used for student exercises. (ANNOTATION)", "research_question": "How do survival analysis methods handle censored observations and incorporate explanatory variables to assess their relationship with survival outcomes?"}
{"doi": "https://doi.org/10.3390/ijms19020335", "title": "Lignins: Biosynthesis and Biological Functions in Plants", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 1289, "authors": ["Le Luo", "Qingquan Liu", "Luqing Zheng"], "abstract": "Lignin is one of the main components of plant cell wall and it is a natural phenolic polymer with high molecular weight, complex composition and structure. Lignin biosynthesis extensively contributes to plant growth, tissue/organ development, lodging resistance and the responses to a variety of biotic and abiotic stresses. In the present review, we systematically introduce the biosynthesis of lignin and its regulation by genetic modification and summarize the main biological functions of lignin in plants and their applications. We hope this review will give an in-depth understanding of the important roles of lignin biosynthesis in various plants’ biological processes and provide a theoretical basis for the genetic improvement of lignin content and composition in energy plants and crops.", "research_question": "What are the potential trade-offs and unintended consequences of genetically modifying lignin content or composition in crops and energy plants, especially regarding plant growth, stress resistance, and suitability for industrial uses?"}
{"doi": "https://doi.org/10.1109/msp.2017.2765202", "title": "Generative Adversarial Networks: An Overview", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 4073, "authors": ["Tom White", "Vincent Dumoulin", "Kai Arulkumaran", "Anil A. Bharath", "Biswa Sengupta", "Antonia Creswell"], "abstract": "Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this by deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image superresolution, and classification. The aim of this review article is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.", "research_question": "What are the main practical challenges in training generative adversarial networks, and which techniques are most effective for improving their stability and convergence?"}
{"doi": "https://doi.org/10.1093/nar/gky497", "title": "BeStSel: a web server for accurate protein secondary structure prediction and fold recognition from the circular dichroism spectra", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 1056, "authors": ["Judit Kun", "András Micsonai", "Yuji Goto", "Frank Wien", "Matthieu Réfrégiers", "Éva Moussong", "József Kardos", "Young‐Ho Lee", "Éva Bulyáki"], "abstract": "Circular dichroism (CD) spectroscopy is a widely used method to study the protein secondary structure. However, for decades, the general opinion was that the correct estimation of β-sheet content is challenging because of the large spectral and structural diversity of β-sheets. Recently, we showed that the orientation and twisting of β-sheets account for the observed spectral diversity, and developed a new method to estimate accurately the secondary structure (PNAS, 112, E3095). BeStSel web server provides the Beta Structure Selection method to analyze the CD spectra recorded by conventional or synchrotron radiation CD equipment. Both normalized and measured data can be uploaded to the server either as a single spectrum or series of spectra. The originality of BeStSel is that it carries out a detailed secondary structure analysis providing information on eight secondary structure components including parallel-β structure and antiparallel β-sheets with three different groups of twist. Based on these, it predicts the protein fold down to the topology/homology level of the CATH protein fold classification. The server also provides a module to analyze the structures deposited in the PDB for BeStSel secondary structure contents in relation to Dictionary of Secondary Structure of Proteins data. The BeStSel server is freely accessible at http://bestsel.elte.hu.", "research_question": "How do the orientation and twist of β-sheets affect circular dichroism spectra, and what practical approaches can be used during CD experiments to distinguish different β‑sheet arrangements?"}
{"doi": "https://doi.org/10.1126/science.aaq0179", "title": "Multiplexed and portable nucleic acid detection platform with Cas13, Cas12a, and Csm6", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 2413, "authors": ["James J. Collins", "Jonathan S. Gootenberg", "Julia Joung", "Max J. Kellner", "Feng Zhang", "Omar O. Abudayyeh"], "abstract": "Taking CRISPR technology further CRISPR techniques are allowing the development of technologies for nucleic acid detection (see the Perspective by Chertow). Taking advantages of the distinctive enzymatic properties of CRISPR enzymes, Gootenberg et al. developed an improved nucleic acid detection technology for multiplexed quantitative and highly sensitive detection, combined with lateral flow for visual readout. Myhrvold et al. added a sample preparation protocol to create a field-deployable viral diagnostic platform for rapid detection of specific strains of pathogens in clinical samples. Cas12a (also known as Cpf1), a type V CRISPR protein, cleaves double-stranded DNA and has been adapted for genome editing. Chen et al. discovered that Cas12a also processes single-stranded DNA threading activity. A technology platform based on this activity detected human papillomavirus in patient samples with high sensitivity. Science , this issue p. 439 , p. 444 , p. 436 ; see also p. 381", "research_question": "How do CRISPR-based diagnostic tests work, and what are their main advantages and limitations compared with traditional PCR-based diagnostics?"}
{"doi": "https://doi.org/10.1051/0004-6361/201833051", "title": "<i>Gaia</i> Data Release 2", "language": "en", "created_date": "2018-05-07T00:00:00", "publication_year": 2018, "cited_by_count": 8246, "authors": ["A. G. A. Brown", "Y. Frémat", "S. Randich", "E. Gosset", "S. A. Klioner", "G. Jasniewicz", "A. Hutton", "J. M. Martín-Fleitas", "M. Manteiga", "M. G. Lattanzi", "Aldo Dell'Oro", "F. van Leeuwen", "B. Holl", "P. Sartoretti", "F. Jansen", "D. Teyssier", "A. Cellino", "F. Thévenin", "J. Fernández-Hernández", "Daniel Hestroffer", "L. Eyer", "J. Bakker", "J.-L. Halbwachs", "C. Babusiaux", "P. Panuzzo", "F. Arenou", "O. L. Creevey", "L. P. Guy", "H. I. Siddiqui", "I. Bellas-Velidis", "W. Löffler", "U. Lammers", "A. Vallenari", "X. Luri", "J. Castañeda", "S. T. Hodgkin", "G. Gracia-Abril", "J. González-Núñez", "A. Moitinho", "D. Katz", "E. Masana", "N. C. Hambly", "J. Hernández", "L. Delchambre", "M. Clotet", "J. H. J. de Bruijne", "K Benson", "C. Panem", "C. Jordi", "R. Andrae", "A. Jean-Antoine-Piccolo", "G. Clementini", "D. W. Evans", "P. M. Marrese", "L. Galluccio", "N. Cheek", "C. Cacciari", "M. Cropper", "N. A. Walton", "D. Pourbaix", "Miguel García-Torres", "A. Krone-Martins", "B. Carry", "J. Berthier", "K. Nienartowicz", "S. Jordan", "G. Comoretto", "L. Chaoul", "M. Fouesneau", "D. L. Harrison", "C. Ducourant", "M. Garcia-Reinaldos", "M. Riello", "U. Bastian", "P. Tanga", "C. Fabricius", "G. Busso", "R. Drimmel", "M. Davidson", "T. Lebzelter", "P. Burgess", "F. De Angeli", "R. Guerra", "M. Altmann", "M. Audard", "J. De Ridder", "A. J. Korn", "C. A. L. Bailer-Jones", "A. C. Lanzafame", "C. Soubiran", "L. Lindegren", "R. Blomme", "R. Messineo", "J. Portell", "M. Biermann", "J. J. González–Vidal", "N. Mowlavï", "T. Prusti", "F. Mignard", "G. M. Seabroke"], "abstract": "Context. We present the second Gaia data release, Gaia DR2, consisting of astrometry, photometry, radial velocities, and information on astrophysical parameters and variability, for sources brighter than magnitude 21. In addition epoch astrometry and photometry are provided for a modest sample of minor planets in the solar system. Aims. A summary of the contents of Gaia DR2 is presented, accompanied by a discussion on the differences with respect to Gaia DR1 and an overview of the main limitations which are still present in the survey. Recommendations are made on the responsible use of Gaia DR2 results. Methods. The raw data collected with the Gaia instruments during the first 22 months of the mission have been processed by the Gaia Data Processing and Analysis Consortium (DPAC) and turned into this second data release, which represents a major advance with respect to Gaia DR1 in terms of completeness, performance, and richness of the data products. Results. Gaia DR2 contains celestial positions and the apparent brightness in G for approximately 1.7 billion sources. For 1.3 billion of those sources, parallaxes and proper motions are in addition available. The sample of sources for which variability information is provided is expanded to 0.5 million stars. This data release contains four new elements: broad-band colour information in the form of the apparent brightness in the G BP (330–680 nm) and G RP (630–1050 nm) bands is available for 1.4 billion sources; median radial velocities for some 7 million sources are presented; for between 77 and 161 million sources estimates are provided of the stellar effective temperature, extinction, reddening, and radius and luminosity; and for a pre-selected list of 14 000 minor planets in the solar system epoch astrometry and photometry are presented. Finally, Gaia DR2 also represents a new materialisation of the celestial reference frame in the optical, the Gaia -CRF2, which is the first optical reference frame based solely on extragalactic sources. There are notable changes in the photometric system and the catalogue source list with respect to Gaia DR1, and we stress the need to consider the two data releases as independent. Conclusions. Gaia DR2 represents a major achievement for the Gaia mission, delivering on the long standing promise to provide parallaxes and proper motions for over 1 billion stars, and representing a first step in the availability of complementary radial velocity and source astrophysical information for a sample of stars in the Gaia survey which covers a very substantial fraction of the volume of our galaxy.", "research_question": "How should I account for parallax zero-point offsets and systematic uncertainties when using parallaxes from a large space-based astrometric catalogue to estimate distances to stars?"}
{"doi": "https://doi.org/10.3847/1538-4357/aab9bb", "title": "The Complete Light-curve Sample of Spectroscopically Confirmed SNe Ia from Pan-STARRS1 and Cosmological Constraints from the Combined Pantheon Sample", "language": "en", "created_date": "2017-10-20T00:00:00", "publication_year": 2018, "cited_by_count": 2618, "authors": ["K. C. Chambers", "C. W. Stubbs", "N. Kaiser", "Gautham Narayan", "M. R. Drout", "J. Tonry", "Adam G. Riess", "Y. C. Pan", "D. Scolnic", "N. Metcalfe", "R. Lunnan", "K. W. Hodapp", "Fabio Bresolin", "Dillon Brout", "K. Smith", "W. M. Wood‐Vasey", "M. McCrum", "W. S. Burgett", "R. P. Kudritzki", "R. Chornock", "R. Kirshner", "E. A. Magnier", "A. Rest", "R. Kotak", "E. Berger", "R. Keßler", "Michael M. Foley", "S. J. Smartt", "D. O. Jones", "Jared Hand", "Peter Challis", "P. W. Draper", "Nathan Sanders", "M. E. Huber", "Erwin Gáll", "R. J. Foley", "S. Rodney", "Emily Cooper Johnson", "Douglas P. Finkbeiner", "Edward F. Schlafly"], "abstract": "Abstract We present optical light curves, redshifts, and classifications for spectroscopically confirmed Type Ia supernovae (SNe Ia) discovered by the Pan-STARRS1 (PS1) Medium Deep Survey. We detail improvements to the PS1 SN photometry, astrometry, and calibration that reduce the systematic uncertainties in the PS1 SN Ia distances. We combine the subset of PS1 SNe Ia (0.03 &lt; z &lt; 0.68) with useful distance estimates of SNe Ia from the Sloan Digital Sky Survey (SDSS), SNLS, and various low- z and Hubble Space Telescope samples to form the largest combined sample of SNe Ia, consisting of a total of SNe Ia in the range of 0.01 &lt; z &lt; 2.3, which we call the “Pantheon Sample.” When combining Planck 2015 cosmic microwave background (CMB) measurements with the Pantheon SN sample, we find and for the w CDM model. When the SN and CMB constraints are combined with constraints from BAO and local H 0 measurements, the analysis yields the most precise measurement of dark energy to date: and for the CDM model. Tension with a cosmological constant previously seen in an analysis of PS1 and low- z SNe has diminished after an increase of 2× in the statistics of the PS1 sample, improved calibration and photometry, and stricter light-curve quality cuts. We find that the systematic uncertainties in our measurements of dark energy are almost as large as the statistical uncertainties, primarily due to limitations of modeling the low-redshift sample. This must be addressed for future progress in using SNe Ia to measure dark energy.", "research_question": "What are the main sources of systematic uncertainty when using Type Ia supernovae as standardized distance indicators—particularly for low-redshift samples—and what strategies can reduce those systematics to improve dark energy measurements?"}
{"doi": "https://doi.org/10.1088/2053-1583/aacfc1", "title": "The Computational 2D Materials Database: high-throughput modeling and discovery of atomically thin crystals", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 1089, "authors": ["Per S. Schmidt", "Kristian S. Thygesen", "Morten N. Gjerding", "Thomas Olsen", "Daniele Torelli", "Sten Haastrup", "Mikkel Strange", "Mohnish Pandey", "Anders C. Riis-Jensen", "Jakob Gath", "Karsten W. Jacobsen", "Jens Jørgen Mortensen", "Nicki F. Hinsche", "Thorsten Deilmann", "Peter Mahler Larsen"], "abstract": "We introduce the Computational 2D Materials Database (C2DB), which organises\\na variety of structural, thermodynamic, elastic, electronic, magnetic, and\\noptical properties of around 1500 two-dimensional materials distributed over\\nmore than 30 different crystal structures. Material properties are\\nsystematically calculated by state-of-the art density functional theory and\\nmany-body perturbation theory (G$_0\\\\!$W$\\\\!_0$ and the Bethe-Salpeter Equation\\nfor $\\\\sim$200 materials) following a semi-automated workflow for maximal\\nconsistency and transparency. The C2DB is fully open and can be browsed online\\nor downloaded in its entirety. In this paper, we describe the workflow behind\\nthe database, present an overview of the properties and materials currently\\navailable, and explore trends and correlations in the data. Moreover, we\\nidentify a large number of new potentially synthesisable 2D materials with\\ninteresting properties targeting applications within spintronics,\\n(opto-)electronics, and plasmonics. The C2DB offers a comprehensive and easily\\naccessible overview of the rapidly expanding family of 2D materials and forms\\nan ideal platform for computational modeling and design of new 2D materials and\\nvan der Waals heterostructures.\\n", "research_question": "How can computational databases of two-dimensional materials be used effectively to guide experimental synthesis and validation of promising candidate materials?"}
{"doi": "https://doi.org/10.1146/annurev-fluid-010518-040547", "title": "Turbulence Modeling in the Age of Data", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 1288, "authors": ["Karthik Duraisamy", "Heng Xiao", "Gianluca Iaccarino"], "abstract": "Data from experiments and direct simulations of turbulence have historically been used to calibrate simple engineering models such as those based on the Reynolds-averaged Navier–Stokes (RANS) equations. In the past few years, with the availability of large and diverse data sets, researchers have begun to explore methods to systematically inform turbulence models with data, with the goal of quantifying and reducing model uncertainties. This review surveys recent developments in bounding uncertainties in RANS models via physical constraints, in adopting statistical inference to characterize model coefficients and estimate discrepancy, and in using machine learning to improve turbulence models. Key principles, achievements, and challenges are discussed. A central perspective advocated in this review is that by exploiting foundational knowledge in turbulence modeling and physical constraints, researchers can use data-driven approaches to yield useful predictive models.", "research_question": "How can physical constraints (like conservation laws and realizability) be incorporated into machine‑learning approaches for turbulence modeling to ensure physically consistent and reliable predictions?"}
{"doi": "https://doi.org/10.1109/cvpr.2018.00644", "title": "Cascade R-CNN: Delving Into High Quality Object Detection", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 6294, "authors": ["Zhaowei Cai", "Nuno Vasconcelos"], "abstract": "In object detection, an intersection over union (IoU) threshold is required to define positives and negatives. An object detector, trained with low IoU threshold, e.g. 0.5, usually produces noisy detections. However, detection performance tends to degrade with increasing the IoU thresholds. Two main factors are responsible for this: 1) overfitting during training, due to exponentially vanishing positive samples, and 2) inference-time mismatch between the IoUs for which the detector is optimal and those of the input hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is proposed to address these problems. It consists of a sequence of detectors trained with increasing IoU thresholds, to be sequentially more selective against close false positives. The detectors are trained stage by stage, leveraging the observation that the output of a detector is a good distribution for training the next higher quality detector. The resampling of progressively improved hypotheses guarantees that all detectors have a positive set of examples of equivalent size, reducing the overfitting problem. The same cascade procedure is applied at inference, enabling a closer match between the hypotheses and the detector quality of each stage. A simple implementation of the Cascade R-CNN is shown to surpass all single-model object detectors on the challenging COCO dataset. Experiments also show that the Cascade R-CNN is widely applicable across detector architectures, achieving consistent gains independently of the baseline detector strength. The code is available at https://github.com/zhaoweicai/cascade-rcnn.", "research_question": "How can using a sequence of detectors trained with progressively higher IoU thresholds improve detection quality and reduce overfitting compared to training a single detector with a fixed IoU threshold?"}
{"doi": "https://doi.org/10.1145/3232676", "title": "A Survey on Automatic Detection of Hate Speech in Text", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 1079, "authors": ["Sérgio Nunes", "Paula Fortuna"], "abstract": "The scientific study of hate speech, from a computer science point of view, is recent. This survey organizes and describes the current state of the field, providing a structured overview of previous approaches, including core algorithms, methods, and main features used. This work also discusses the complexity of the concept of hate speech, defined in many platforms and contexts, and provides a unifying definition. This area has an unquestionable potential for societal impact, particularly in online communities and digital media platforms. The development and systematization of shared resources, such as guidelines, annotated datasets in multiple languages, and algorithms, is a crucial step in advancing the automatic detection of hate speech.", "research_question": "What are best practices for creating reliable, culturally sensitive, and reusable annotated datasets for automated hate speech detection across multiple languages?"}
{"doi": "https://doi.org/10.1109/cvpr.2018.00337", "title": "Densely Connected Pyramid Dehazing Network", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 1137, "authors": ["Vishal M. Patel", "He Zhang"], "abstract": "We propose a new end-to-end single image dehazing method, called Densely Connected Pyramid Dehazing Network (DCPDN), which can jointly learn the transmission map, atmospheric light and dehazing all together. The end-to-end learning is achieved by directly embedding the atmospheric scattering model into the network, thereby ensuring that the proposed method strictly follows the physics-driven scattering model for dehazing. Inspired by the dense network that can maximize the information flow along features from different levels, we propose a new edge-preserving densely connected encoder-decoder structure with multi-level pyramid pooling module for estimating the transmission map. This network is optimized using a newly introduced edge-preserving loss function. To further incorporate the mutual structural information between the estimated transmission map and the dehazed result, we propose a joint-discriminator based on generative adversarial network framework to decide whether the corresponding dehazed image and the estimated transmission map are real or fake. An ablation study is conducted to demonstrate the effectiveness of each module evaluated at both estimated transmission map and dehazed result. Extensive experiments demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods. Code and dataset is made available at: https://github.com/hezhangsprinter/DCPDN.", "research_question": "How does jointly estimating the transmission map and atmospheric light within a deep learning framework improve single-image dehazing compared to estimating them separately or using purely data-driven approaches?"}
{"doi": "https://doi.org/10.1177/0267323118760317", "title": "The disinformation order: Disruptive communication and the decline of democratic institutions", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 1359, "authors": ["Steven Livingston", "W. Lance Bennett"], "abstract": "Many democratic nations are experiencing increased levels of false information circulating through social media and political websites that mimic journalism formats. In many cases, this disinformation is associated with the efforts of movements and parties on the radical right to mobilize supporters against centre parties and the mainstream press that carries their messages. The spread of disinformation can be traced to growing legitimacy problems in many democracies. Declining citizen confidence in institutions undermines the credibility of official information in the news and opens publics to alternative information sources. Those sources are often associated with both nationalist (primarily radical right) and foreign (commonly Russian) strategies to undermine institutional legitimacy and destabilize centre parties, governments and elections. The Brexit campaign in the United Kingdom and the election of Donald Trump in the United States are among the most prominent examples of disinformation campaigns intended to disrupt normal democratic order, but many other nations display signs of disinformation and democratic disruption. The origins of these problems and their implications for political communication research are explored.", "research_question": "What practical strategies can democratic societies use to rebuild public trust in institutions and reduce citizens' susceptibility to disinformation spread via social media and partisan websites?"}
{"doi": "https://doi.org/10.1002/adma.201706846", "title": "Tough and Water‐Insensitive Self‐Healing Elastomer for Robust Electronic Skin", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 1064, "authors": ["Yeongjun Lee", "Jin Young Oh", "Jiheong Kang", "Jeffrey B.‐H. Tok", "Jeffrey Lopez", "Yeongin Kim", "Toru Katsumata", "Zhenan Bao", "Ging‐Ji Nathan Wang", "Lihua Jin", "Jaewan Mun", "Donghee Son", "Yuxin Liu"], "abstract": "Abstract An electronic (e‐) skin is expected to experience significant wear and tear over time. Therefore, self‐healing stretchable materials that are simultaneously soft and with high fracture energy, that is high tolerance of damage or small cracks without propagating, are essential requirements for the realization of robust e‐skin. However, previously reported elastomers and especially self‐healing polymers are mostly viscoelastic and lack high mechanical toughness. Here, a new class of polymeric material crosslinked through rationally designed multistrength hydrogen bonding interactions is reported. The resultant supramolecular network in polymer film realizes exceptional mechanical properties such as notch‐insensitive high stretchability (1200%), high toughness of 12 000 J m −2 , and autonomous self‐healing even in artificial sweat. The tough self‐healing materials enable the wafer‐scale fabrication of robust and stretchable self‐healing e‐skin devices, which will provide new directions for future soft robotics and skin prosthetics.", "research_question": "What material design strategies and molecular mechanisms allow a soft, stretchable polymer to simultaneously achieve very high toughness, extreme extensibility, and autonomous self‑healing performance even in humid or sweaty environments?"}
{"doi": "https://doi.org/10.1109/mcom.2018.1700659", "title": "A New Wireless Communication Paradigm through Software-Controlled Metasurfaces", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 1247, "authors": ["Ageliki Tsioliaridou", "Sotiris Ioannidis", "Christos Liaskos", "Shuai Nie", "Ian F. Akyildiz", "Andreas Pitsillides"], "abstract": "Electromagnetic waves undergo multiple uncontrollable alterations as they propagate within a wireless environment. Free space path loss, signal absorption, as well as reflections, refractions, and diffractions caused by physical objects within the environment highly affect the performance of wireless communications. Currently, such effects are intractable to account for and are treated as probabilistic factors. This article proposes a radically different approach, enabling deterministic, programmable control over the behavior of wireless environments. The key enabler is the so-called HyperSurface tile, a novel class of planar meta-materials that can interact with impinging electromagnetic waves in a controlled manner. The HyperSurface tiles can effectively re-engineer electromagnetic waves, including steering toward any desired direction, full absorption, polarization manipulation, and more. Multiple tiles are employed to coat objects such as walls, furniture, and overall, any objects in indoor and outdoor environments. An external software service calculates and deploys the optimal interaction types per tile to best fit the needs of communicating devices. Evaluation via simulations highlights the potential of the new concept.", "research_question": "What are the practical challenges and limitations of deploying programmable metasurfaces in real-world wireless environments, including power, control, scalability, and maintenance?"}
{"doi": "https://doi.org/10.1021/acs.chemrev.7b00536", "title": "Chemical Vapor Deposition Growth and Applications of Two-Dimensional Materials and Their Heterostructures", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 1391, "authors": ["Xiaolong Zou", "Hui‐Ming Cheng", "Zhengyang Cai", "Bilu Liu"], "abstract": "Two-dimensional (2D) materials have attracted increasing research interest because of the abundant choice of materials with diverse and tunable electronic, optical, and chemical properties. Moreover, 2D material based heterostructures combining several individual 2D materials provide unique platforms to create an almost infinite number of materials and show exotic physical phenomena as well as new properties and applications. To achieve these high expectations, methods for the scalable preparation of 2D materials and 2D heterostructures of high quality and low cost must be developed. Chemical vapor deposition (CVD) is a powerful method which may meet the above requirements, and has been extensively used to grow 2D materials and their heterostructures in recent years, despite several challenges remaining. In this review of the challenges in the CVD growth of 2D materials, we highlight recent advances in the controlled growth of single crystal 2D materials, with an emphasis on semiconducting transition metal dichalcogenides. We provide insight into the growth mechanisms of single crystal 2D domains and the key technologies used to realize wafer-scale growth of continuous and homogeneous 2D films which are important for practical applications. Meanwhile, strategies to design and grow various kinds of 2D material based heterostructures are thoroughly discussed. The applications of CVD-grown 2D materials and their heterostructures in electronics, optoelectronics, sensors, flexible devices, and electrocatalysis are also discussed. Finally, we suggest solutions to these challenges and ideas concerning future developments in this emerging field.", "research_question": "What are the main technical challenges and limitations that must be overcome to reliably produce wafer-scale, high-quality 2D materials and heterostructures using chemical vapor deposition?"}
{"doi": "https://doi.org/10.1109/lsp.2018.2822810", "title": "Additive Margin Softmax for Face Verification", "language": "en", "created_date": "2018-01-26T00:00:00", "publication_year": 2018, "cited_by_count": 1029, "authors": ["Jian Cheng", "Haijun Liu", "Weiyang Liu", "Feng Wang"], "abstract": "In this paper, we propose a conceptually simple and geometrically interpretable objective function, i.e. additive margin Softmax (AM-Softmax), for deep face verification. In general, the face verification task can be viewed as a metric learning problem, so learning large-margin face features whose intra-class variation is small and inter-class difference is large is of great importance in order to achieve good performance. Recently, Large-margin Softmax and Angular Softmax have been proposed to incorporate the angular margin in a multiplicative manner. In this work, we introduce a novel additive angular margin for the Softmax loss, which is intuitively appealing and more interpretable than the existing works. We also emphasize and discuss the importance of feature normalization in the paper. Most importantly, our experiments on LFW BLUFR and MegaFace show that our additive margin softmax loss consistently performs better than the current state-of-the-art methods using the same network architecture and training dataset. Our code has also been made available at https://github.com/happynear/AMSoftmax", "research_question": "How does adding an angular margin to a softmax-based loss function influence intra-class compactness and inter-class separation in learned face embeddings, and why is feature normalization important when using such margin-based losses?"}
{"doi": "https://doi.org/10.1186/s12951-018-0408-4", "title": "‘Green’ synthesis of metals and their oxide nanoparticles: applications for environmental remediation", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 2361, "authors": ["Ki‐Hyun Kim", "Tanushree Dutta", "Jagpreet Singh", "Mohit Rawat", "Pallabi Samddar", "Pawan Kumar"], "abstract": "In materials science, \"green\" synthesis has gained extensive attention as a reliable, sustainable, and eco-friendly protocol for synthesizing a wide range of materials/nanomaterials including metal/metal oxides nanomaterials, hybrid materials, and bioinspired materials. As such, green synthesis is regarded as an important tool to reduce the destructive effects associated with the traditional methods of synthesis for nanoparticles commonly utilized in laboratory and industry. In this review, we summarized the fundamental processes and mechanisms of \"green\" synthesis approaches, especially for metal and metal oxide [e.g., gold (Au), silver (Ag), copper oxide (CuO), and zinc oxide (ZnO)] nanoparticles using natural extracts. Importantly, we explored the role of biological components, essential phytochemicals (e.g., flavonoids, alkaloids, terpenoids, amides, and aldehydes) as reducing agents and solvent systems. The stability/toxicity of nanoparticles and the associated surface engineering techniques for achieving biocompatibility are also discussed. Finally, we covered applications of such synthesized products to environmental remediation in terms of antimicrobial activity, catalytic activity, removal of pollutants dyes, and heavy metal ion sensing.", "research_question": "What are the main challenges and limitations in scaling up green synthesis of metal and metal-oxide nanoparticles using natural extracts for industrial applications?"}
{"doi": "https://doi.org/10.3390/ijms19061578", "title": "Natural Products for Drug Discovery in the 21st Century: Innovations for Novel Drug Discovery", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 1143, "authors": ["Kevin Dzobo", "Arielle Rowe", "Dimakatso Alice Senthebane", "Palesa Pamela Seele", "Alfred Maroyi", "Daniella Munro", "Nicholas Ekow Thomford"], "abstract": "The therapeutic properties of plants have been recognised since time immemorial. Many pathological conditions have been treated using plant-derived medicines. These medicines are used as concoctions or concentrated plant extracts without isolation of active compounds. Modern medicine however, requires the isolation and purification of one or two active compounds. There are however a lot of global health challenges with diseases such as cancer, degenerative diseases, HIV/AIDS and diabetes, of which modern medicine is struggling to provide cures. Many times the isolation of “active compound” has made the compound ineffective. Drug discovery is a multidimensional problem requiring several parameters of both natural and synthetic compounds such as safety, pharmacokinetics and efficacy to be evaluated during drug candidate selection. The advent of latest technologies that enhance drug design hypotheses such as Artificial Intelligence, the use of ‘organ-on chip’ and microfluidics technologies, means that automation has become part of drug discovery. This has resulted in increased speed in drug discovery and evaluation of the safety, pharmacokinetics and efficacy of candidate compounds whilst allowing novel ways of drug design and synthesis based on natural compounds. Recent advances in analytical and computational techniques have opened new avenues to process complex natural products and to use their structures to derive new and innovative drugs. Indeed, we are in the era of computational molecular design, as applied to natural products. Predictive computational softwares have contributed to the discovery of molecular targets of natural products and their derivatives. In future the use of quantum computing, computational softwares and databases in modelling molecular interactions and predicting features and parameters needed for drug development, such as pharmacokinetic and pharmacodynamics, will result in few false positive leads in drug development. This review discusses plant-based natural product drug discovery and how innovative technologies play a role in next-generation drug discovery.", "research_question": "How can computational methods and machine learning be used to identify, prioritize, and optimize bioactive compounds from complex plant extracts for drug development?"}
{"doi": "https://doi.org/10.1021/acs.chemrev.7b00581", "title": "Expanded Theory of H- and J-Molecular Aggregates: The Effects of Vibronic Coupling and Intermolecular Charge Transfer", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 1634, "authors": ["Frank C. Spano", "Nicholas J. Hestand"], "abstract": "The electronic excited states of molecular aggregates and their photophysical signatures have long fascinated spectroscopists and theoreticians alike since the advent of Frenkel exciton theory almost 90 years ago. The influence of molecular packing on basic optical probes like absorption and photoluminescence was originally worked out by Kasha for aggregates dominated by Coulombic intermolecular interactions, eventually leading to the classification of J- and H-aggregates. This review outlines advances made in understanding the relationship between aggregate structure and photophysics when vibronic coupling and intermolecular charge transfer are incorporated. An assortment of packing geometries is considered from the humble molecular dimer to more exotic structures including linear and bent aggregates, two-dimensional herringbone and \"HJ\" aggregates, and chiral aggregates. The interplay between long-range Coulomb coupling and short-range charge-transfer-mediated coupling strongly depends on the aggregate architecture leading to a wide array of photophysical behaviors.", "research_question": "How can experimental spectroscopic techniques distinguish the effects of long-range Coulomb (Frenkel) coupling from short-range charge-transfer interactions in molecular aggregates?"}
{"doi": "https://doi.org/10.1126/science.aat0586", "title": "Ethane/ethylene separation in a metal-organic framework with iron-peroxo sites", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 1055, "authors": ["Rui‐Biao Lin", "Rajamani Krishna", "Banglin Chen", "Shengchang Xiang", "Libo Li", "Hui Wu", "Jinping Li", "Hao Li", "Wei Zhou"], "abstract": "A preference for ethane Industrial production of ethylene requires its separation from ethane in a cryogenic process that consumes large amounts of energy. An alternative would be differential sorption in microporous materials. Most of these materials bind ethylene more strongly that ethane, but adsorption of ethane would be more efficient. Li et al. found that a metal-organic framework containing iron-peroxo sites bound ethane more strongly than ethylene and could be used to separate the gases at ambient conditions. Science , this issue p. 443", "research_question": "What material properties and binding sites determine whether a microporous adsorbent will preferentially capture ethane rather than ethylene, and how can these be engineered to enable efficient ambient-temperature separation?"}
{"doi": "https://doi.org/10.1109/tkde.2018.2833443", "title": "Heterogeneous Information Network Embedding for Recommendation", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 1175, "authors": ["Philip S. Yu", "Wayne Xin Zhao", "Binbin Hu", "Chuan Shi"], "abstract": "Due to the flexibility in modelling data heterogeneity, heterogeneous information network (HIN) has been adopted to characterize complex and heterogeneous auxiliary data in recommender systems, called <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">HIN based recommendation</i> . It is challenging to develop effective methods for HIN based recommendation in both extraction and exploitation of the information from HINs. Most of HIN based recommendation methods rely on path based similarity, which cannot fully mine latent structure features of users and items. In this paper, we propose a novel heterogeneous network embedding based approach for HIN based recommendation, called HERec. To embed HINs, we design a meta-path based random walk strategy to generate meaningful node sequences for network embedding. The learned node embeddings are first transformed by a set of fusion functions, and subsequently integrated into an extended matrix factorization (MF) model. The extended MF model together with fusion functions are jointly optimized for the rating prediction task. Extensive experiments on three real-world datasets demonstrate the effectiveness of the HERec model. Moreover, we show the capability of the HERec model for the cold-start problem, and reveal that the transformed embedding information from HINs can improve the recommendation performance.", "research_question": "How can embeddings learned from heterogeneous information networks be effectively integrated into recommendation models to improve accuracy and handle cold-start users or items?"}
{"doi": "https://doi.org/10.1201/9780203747940", "title": "Measure Theory and Fine Properties of Functions", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 4300, "authors": ["Ronald F. Garzepy", "Lawrence C. Evans"], "abstract": "This book provides a detailed examination of the central assertions of measure theory in n-dimensional Euclidean space and emphasizes the roles of Hausdorff measure and the capacity in characterizing the fine properties of sets and functions. Topics covered include a quick review of abstract measure theory, theorems and differentiation in Mn, lower Hausdorff measures, area and coarea formulas for Lipschitz mappings and related change-of-variable formulas, and Sobolev functions and functions of bounded variation. The text provides complete proofs of many key results omitted from other books, including Besicovitch's Covering Theorem, Rademacher's Theorem (on the differentiability a.e. of Lipschitz functions), the Area and Coarea Formulas, the precise structure of Sobolev and BV functions, the precise structure of sets of finite perimeter, and Alexandro's Theorem (on the twice differentiability a.e. of convex functions).Topics are carefully selected and the proofs succinct, but complete, which makes this book ideal reading for applied mathematicians and graduate students in applied mathematics.", "research_question": "Can you explain the coarea formula intuitively and give a concrete example of how it relates an integral over a domain to integrals over the level sets of a function?"}
{"doi": "https://doi.org/10.1200/jco.2017.75.3657", "title": "Chemohormonal Therapy in Metastatic Hormone-Sensitive Prostate Cancer: Long-Term Survival Analysis of the Randomized Phase III E3805 CHAARTED Trial", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2018, "cited_by_count": 1049, "authors": ["Michael A. Carducci", "Christos E. Kyriakopoulos", "Mario A. Eisenberger", "Manish Kohli", "Robert S. DiPaola", "Robert Dreicer", "Maha Hussain", "Nicholas J. Vogelzang", "Joel Picus", "David F. Jarrard", "Glenn Liu", "Noah M. Hahn", "Daniel H. Shevrin", "Matthew M. Cooney", "Elizabeth R. Plimack", "Yu‐Hui Chen", "Jorge A. García", "Christopher J. Sweeney"], "abstract": "Purpose Docetaxel added to androgen-deprivation therapy (ADT) significantly increases the longevity of some patients with metastatic hormone-sensitive prostate cancer. Herein, we present the outcomes of the CHAARTED (Chemohormonal Therapy Versus Androgen Ablation Randomized Trial for Extensive Disease in Prostate Cancer) trial with more mature follow-up and focus on tumor volume. Patients and Methods In this phase III study, 790 patients with metastatic hormone-sensitive prostate cancer were equally randomly assigned to receive either ADT in combination with docetaxel 75 mg/m 2 for up to six cycles or ADT alone. The primary end point of the study was overall survival (OS). Additional analyses of the prospectively defined low- and high-volume disease subgroups were performed. High-volume disease was defined as presence of visceral metastases and/or ≥ four bone metastases with at least one outside of the vertebral column and pelvis. Results At a median follow-up of 53.7 months, the median OS was 57.6 months for the chemohormonal therapy arm versus 47.2 months for ADT alone (hazard ratio [HR], 0.72; 95% CI, 0.59 to 0.89; P = .0018). For patients with high-volume disease (n = 513), the median OS was 51.2 months with chemohormonal therapy versus 34.4 months with ADT alone (HR, 0.63; 95% CI, 0.50 to 0.79; P &lt; .001). For those with low-volume disease (n = 277), no OS benefit was observed (HR, 1.04; 95% CI, 0.70 to 1.55; P = .86). Conclusion The clinical benefit from chemohormonal therapy in prolonging OS was confirmed for patients with high-volume disease; however, for patients with low-volume disease, no OS benefit was discerned.", "research_question": "What clinical factors and patient characteristics should guide the decision to add early systemic chemotherapy to androgen-deprivation therapy for men with metastatic hormone-sensitive prostate cancer?"}
{"doi": "https://doi.org/10.1016/j.caeai.2023.100187", "title": "Designing an artificial intelligence tool to understand student engagement based on teacher's behaviours and movements in video conferencing", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 21, "authors": ["Thanveer Shaik", "Navdeep Verma", "Dr Seyum Getenet", "Dr Christopher Dann"], "abstract": "We attempt to define what is necessary to construct an Artificial Scientist, explore and evaluate several approaches to artificial general intelligence (AGI) which may facilitate this, conclude that a unified or hybrid approach is necessary and explore two theories that satisfy this requirement to some degree.", "arxiv_id": "2110.01831v1", "research_question": "What core cognitive abilities and types of knowledge would an artificial system need to function autonomously as a scientist, including proposing hypotheses, designing experiments, and interpreting results?"}
{"doi": "https://doi.org/10.1103/physrevb.108.l081108", "title": "Generalized Wiedemann-Franz law in a two-site charge Kondo circuit: Lorenz ratio as a manifestation of the orthogonality catastrophe", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 7, "authors": ["M. N. Kiselev"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B meson decays to muon pairs constrain extensions of the Standard Model, and which classes of new-physics models are most strongly affected?"}
{"doi": "https://doi.org/10.1016/j.dajour.2023.100372", "title": "An ensemble learning-based framework for breast cancer prediction", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 25, "authors": ["Aman Sharma", "Rajni Mohana", "D. P. Goyal"], "abstract": "Background Precise prediction of cancer types is vital for cancer diagnosis and therapy. Important cancer marker genes can be inferred through predictive model. Several studies have attempted to build machine learning models for this task however none has taken into consideration the effects of tissue of origin that can potentially bias the identification of cancer markers. Results In this paper, we introduced several Convolutional Neural Network (CNN) models that take unstructured gene expression inputs to classify tumor and non-tumor samples into their designated cancer types or as normal. Based on different designs of gene embeddings and convolution schemes, we implemented three CNN models: 1D-CNN, 2D-Vanilla-CNN, and 2D-Hybrid-CNN. The models were trained and tested on combined 10,340 samples of 33 cancer types and 731 matched normal tissues of The Cancer Genome Atlas (TCGA). Our models achieved excellent prediction accuracies (93.9-95.0%) among 34 classes (33 cancers and normal). Furthermore, we interpreted one of the models, known as 1D-CNN model, with a guided saliency technique and identified a total of 2,090 cancer markers (108 per class). The concordance of differential expression of these markers between the cancer type they represent and others is confirmed. In breast cancer, for instance, our model identified well-known markers, such as GATA3 and ESR1. Finally, we extended the 1D-CNN model for prediction of breast cancer subtypes and achieved an average accuracy of 88.42% among 5 subtypes. The codes can be found at https://github.com/chenlabgccri/CancerTypePrediction.", "arxiv_id": "1906.07794v1", "research_question": "How can machine learning models trained on gene expression data account for tissue-of-origin effects to avoid confounding when identifying cancer-specific biomarkers?"}
{"doi": "https://doi.org/10.1016/j.bioorg.2023.106351", "title": "Onychiol B attenuates lipopolysaccharide-induced inflammation via MAPK/NF-κB pathways and acute lung injury in vivo", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 11, "authors": ["Zeyi Zhang", "Xiaoxiao Pei", "Jianguo Cao", "Na Wang", "Yan‐Zi Yang", "Guozheng Huang", "Xiaoran Min"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of the branching fractions for rare decays of neutral B mesons into muon pairs help constrain or rule out theories beyond the Standard Model?"}
{"doi": "https://doi.org/10.1016/j.scitotenv.2023.165150", "title": "Human health risk model for microplastic exposure in the Arctic region", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 22, "authors": ["Bing Chen", "Rehan Sadiq", "Faisal Khan", "Mohammad Sadiq Saeed", "Faisal Fahd"], "abstract": "We consider calculation of capital requirements when the underlying economic scenarios are determined by simulatable risk factors. In the respective nested simulation framework, the goal is to estimate portfolio tail risk, quantified via VaR or TVaR of a given collection of future economic scenarios representing factor levels at the risk horizon. Traditionally, evaluating portfolio losses of an outer scenario is done by computing a conditional expectation via inner-level Monte Carlo and is computationally expensive. We introduce several inter-related machine learning techniques to speed up this computation, in particular by properly accounting for the simulation noise. Our main workhorse is an advanced Gaussian Process (GP) regression approach which uses nonparametric spatial modeling to efficiently learn the relationship between the stochastic factors defining scenarios and corresponding portfolio value. Leveraging this emulator, we develop sequential algorithms that adaptively allocate inner simulation budgets to target the quantile region. The GP framework also yields better uncertainty quantification for the resulting VaR/TVaR estimators that reduces bias and variance compared to existing methods. We illustrate the proposed strategies with two case-studies in two and six dimensions.", "arxiv_id": "1710.05204v2", "research_question": "How well do Gaussian Process (GP) emulators scale to high-dimensional risk-factor spaces when approximating portfolio value functions, and what practical strategies can be used to mitigate the curse of dimensionality in this setting?"}
{"doi": "https://doi.org/10.1016/j.compstruct.2023.117817", "title": "Strain rate-dependent failure modelling of impact damage in laminated CFRP structures", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 9, "authors": ["E. Giannaros", "Jakov Ratković", "Darko Ivančević"], "abstract": "This work proposes a new and flexible unreliable failure detector whose output is related to the trust level of a set of processes. By expressing the relevance of each process of the set by an impact factor value, our approach allows the tuning of the detector output, making possible a softer or stricter monitoring. The idea behind our proposal is that, according to an acceptable margin of failures and the impact factor assigned to processes, in some scenarios, the failure of some low impact processes may not change the user confidence in the set of processes, while the crash of a high impact factor process may seriously affect it. We outline the application scenarios and the proposed unreliable failure detector, giving a detailed account of the concept on which it is based.", "arxiv_id": "1404.6415v1", "research_question": "How can assigning different impact or importance values to processes be used to improve failure detection and system resilience compared to traditional binary (suspected/not suspected) failure detectors?"}
{"doi": "https://doi.org/10.1108/jamr-10-2022-0218", "title": "Social media influencers: literature review, trends and research agenda", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 23, "authors": ["Manish Kumar Srivastava", "Anshika Singh Tanwar", "Harish Chaudhry"], "abstract": "The availability of software which can produce convincing yet synthetic media poses both threats and benefits to tertiary education globally. While other forms of synthetic media exist, this study focuses on deepfakes, which are advanced Generative AI (GenAI) fakes of real people. This conceptual paper assesses the current literature on deepfakes across multiple disciplines by conducting an initial scoping review of 182 peer-reviewed publications.\n  The review reveals three major trends: detection methods, malicious applications, and potential benefits, although no specific studies on deepfakes in the tertiary educational context were found. Following a discussion of these trends, this study applies the findings to postulate the major risks and potential mitigation strategies of deepfake technologies in higher education, as well as potential beneficial uses to aid the teaching and learning of both deepfakes and synthetic media. This culminates in the proposal of a research agenda to build a comprehensive, cross-cultural approach to investigate deepfakes in higher education.", "arxiv_id": "2404.15601v1", "research_question": "What practical steps can universities take to both mitigate the risks of deepfakes and responsibly harness their potential benefits for teaching and learning?"}
{"doi": "https://doi.org/10.1101/2023.10.30.564197", "title": "Gamma sensory stimulation and effects on the brain", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 8, "authors": ["Ho‐Jun Suk", "Annabelle C. Singer", "Li‐Huei Tsai", "Edward S. Boyden", "Simon Hanslmayr", "Thomas J. McHugh", "Emily Niederst", "Vikram Jakkamsetti", "Chinnakkaruppan Adaikkan", "Brennan Jackson", "Martin C. Kahn", "Danying Wang", "Diane Chan", "Ute Geigenmüller", "Mitchell H. Murdock", "Cristina Blanco‐Duque", "Emery N. Brown"], "abstract": "The characterisation of CMB polarisation is one of the next challenge in observationnal cosmology. This is especially true for the so-called B-modes that are at least 3 order of magnitude lower than CMB temperature fluctuations. A precise measurement of the angular power spectrum of these B-modes will give important constraints on inflation parameters. In this talk, I will describe two complementary experiments, BRAIN and CLOVER, dedicated to CMB polarisation measurement. These experiments are proposed to be installed in Dome-C, Antarctica, to take advantage of the extreme dryness of the atmosphere and to allow long integration time.", "arxiv_id": "0412590v2", "research_question": "What are the main sources of contamination and systematic error when trying to measure CMB B-mode polarization, and how can they be mitigated in ground-based experiments?"}
{"doi": "https://doi.org/10.16910/jemr.15.3.5", "title": "Flipping the world upside down: Using eye tracking in virtual reality to study visual search in inverted scenes", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 8, "authors": ["Dejan Draschkow", "Erwan David", "Jason Helbing", "Julia Beitner", "Melissa L.‐H. Võ"], "abstract": "We conducted an eye-tracking study where 30 participants performed searches on the web. We measured their topical knowledge before and after each task. Their eye-fixations were labelled as \"reading\" or \"scanning\". The series of reading fixations in a line, called \"reading-sequences\" were characterized by their length in pixels, fixation duration, and the number of fixations making up the sequence. We hypothesize that differences in knowledge-change of participants are reflected in their eye-tracking measures related to reading. Our results show that the participants with higher change in knowledge differ significantly in terms of their total reading-sequence-length, reading-sequence-duration, and number of reading fixations, when compared to participants with lower knowledge-change.", "arxiv_id": "1805.02399v1", "research_question": "How reliably can eye-tracking metrics (e.g., fixation durations and reading-sequence lengths) be used to predict or assess a user’s learning or knowledge gain during online search tasks?"}
{"doi": "https://doi.org/10.1016/j.jallcom.2023.172768", "title": "The effect of Pr3+ concentration on the microstructure, luminescence and scintillation properties of Pr:LuAG single crystals", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 7, "authors": ["Zeyu Cheng", "Lipeng Huang", "Guozhu Xiong", "Bin Huang", "Weng Deng", "Huiting Zhang", "Hao Ren", "Shoulei Xu", "Zhong-Hua Zhu", "Yan Hao"], "abstract": "The various decay mechanisms of excitons in cuprous oxide (Cu2O) are highly sensitive to defects which can relax selection rules. Here we report cryogenic hyperspectral imaging of exciton luminescence from cuprous oxide crystals grown via the floating zone method showing the samples have few defects. Some locations, however, show strain splitting of the 1s orthoexciton triplet polariton luminescence. Strain is reduced by annealing. In addition, annealing causes annihilation of oxygen and copper vacancies, which leads to a negative correlation between luminescence of unlike vacancies.", "arxiv_id": "1412.2707v1", "research_question": "How do crystal defects (e.g., vacancies) and strain influence exciton decay pathways and luminescence in semiconductors, and in what ways can annealing modify these defects to restore or change excitonic optical properties?"}
{"doi": "https://doi.org/10.1021/acs.analchem.3c03399", "title": "Dipodal Silanes Greatly Stabilize Glass Surface Functionalization for DNA Microarray Synthesis and High-Throughput Biological Assays", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 21, "authors": ["Gisela Ibáñez-Redín", "Arya A. Das", "Timm Michel", "Erika Schaudy", "Santra Santhosh", "Maya Giridhar", "Jory Lietard", "Mark M. Somoza", "Jürgen Behr"], "abstract": "To investigate the effect of wettability on multiphase flow in porous media, hydrophilic glass surfaces are typically modified through a silanization process. This study examines the nanoscale chemical and structural modifications of glass bead surfaces treated with Surfasil, using inverse gas chromatography and atomic force microscopy. The results show that silanization reduces both specific and dispersive components of surface energy, indicating fewer polar groups and lower total energy, leading to decreased hydrophilicity compared to untreated glass beads. BET surface area measurements and AFM images reveal that the surface becomes progressively smoother with increased silanization.\n  Subsequently, this study assessed the stability and extent of surface modifications in silanized samples caused by adsorbed water during storage, using untreated glass beads as a reference. Untreated samples exhibit increases in surface roughness and polar groups, leading to marginal increase in surface energy and hydrophilicity. In contrast, the silanized samples show resistance to water adsorption, with only minor alterations in surface energy and structure, likely occurring in areas where the silanization coating was incomplete. The results suggest that humidity control is crucial during extended storage, as prolonged moisture exposure could still lead to surface modifications, even in silanized samples, potentially affecting wettability consistency in repeated experiments.", "arxiv_id": "2411.14836v1", "research_question": "How does long-term exposure to humidity or adsorbed water change the wettability and surface chemistry of silanized glass surfaces, and what storage conditions best preserve their hydrophobicity?"}
{"doi": "https://doi.org/10.1002/ajoc.202300141", "title": "Synthesis of Thioamides <i>via</i> Decarboxylative/Oxidative Cross‐Coupling Reaction by Visible‐Light Photocatalysis", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 8, "authors": ["Adinath Majee", "Susanta Ghosh", "Subhankar Kumar Sarkar", "Grigory V. Zyryanov", "Sougata Santra", "Satyajit Pal"], "abstract": "We propose a systematic approach to obtain various forms of insulating titanium oxynitrides Ti$_{n}$N$_{2}$O$_{2n-3}$ and we conduct a detailed study on its $n=2$ case, Ti$_{2}$N$_{2}$O. We study the energetics and the electronic structures of Ti$_{2}$N$_{2}$O and compare these results with those of pristine and nitrogen-doped TiO$_{2}$ within the framework of the density-functional theory (DFT) and the GW approximation. We find that Ti$_{2}$N$_{2}$O is semiconducting with the calculated band-gap of 1.81 eV, which is significantly smaller than those of pristine TiO$_{2}$ rutile (3.14 eV) or anatase (3.55 eV). Furthermore, the reduction of the band-gap of Ti$_{2}$N$_{2}$O is realized not by lowering of the conduction-band minimum but by rising the valence-band maximum. Therefore the proposed Ti$_{2}$N$_{2}$O has suitable band-edge alignment for water-splitting photocatalysis. Finally, total energy calculations indicate that Ti$_{2}$N$_{2}$O is potentially easier to synthesize than nitrogen-doped TiO$_{2}$. Based on these results, we propose Ti$_{2}$N$_{2}$O as a promising visible-light photocatalytic material.", "arxiv_id": "1701.06251v1", "research_question": "How does shifting the valence-band maximum upward (instead of lowering the conduction-band minimum) affect a semiconductor’s suitability and efficiency for visible-light-driven water-splitting photocatalysis?"}
{"doi": "https://doi.org/10.3390/coatings13020416", "title": "Measurement of Tire-Pavement Contact Tri-Axial Stress Distribution Based on Sensor Array", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 9, "authors": ["Xinglin Zhou", "Lu Liu", "Maoping Ran", "Jiaxi Guan"], "abstract": "Beamforming is central to the processing function of all phased arrays and becomes particularly challenging with a large number of antenna element (e.g. >100,000). The ability to beamform efficiently with reasonable power requirements is discussed in this paper. Whilst the most appropriate beamforming technology will change over time due to semiconductor and processing developments, we present a hierarchical structure which is technology agnostic and describe both Radio-Frequency (RF) and digital hierarchical beamforming approaches. We present implementations of both RF and digital beamforming systems on two antenna array demonstrators, namely the Electronic Multi Beam Radio Astronomy ConcEpt (EMBRACE) and the dualpolarisation all-digital array (2-PAD). This paper will compare and contrast both digital and analogue implementations without considering the deep system design of these arrays.", "arxiv_id": "1008.4047v1", "research_question": "What are the main practical trade-offs between analogue (RF) and digital hierarchical beamforming approaches for very large phased arrays in terms of power consumption, scalability, and beamforming flexibility?"}
{"doi": "https://doi.org/10.1177/08839115231183487", "title": "Preparation and evaluation of chitosan-alginate/carrageenan hydrogel for oral drug delivery in the treatment of diabetes", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 16, "authors": ["Archana George", "Pranav S. Shrivastav"], "abstract": "This review explores the synthesis, characterization, and therapeutic applications of zinc oxide nanoparticles (ZnO NPs) in the treatment of diabetes mellitus. The study delves into both chemical and green synthesis methods, comparing their impacts on nanoparticle properties. Key characterization techniques such as XRD, FTIR, UV-Vis spectroscopy, and SEM confirm the crystalline structure, optical properties, and morphology of the nanoparticles. ZnO NPs demonstrate significant biological activities, including antibacterial, anti-inflammatory, and antidiabetic effects. These nanoparticles show promise in improving glucose regulation, enhancing insulin sensitivity, and boosting glucose uptake in cells. Despite these benefits, the potential toxicity and long-term effects of ZnO NPs warrant further investigation. Future research should focus on optimizing synthesis methods and conducting comprehensive studies to fully exploit ZnO NPs' potential in diabetes management and other biomedical applications.", "arxiv_id": "2409.04486v1", "research_question": "What are the known mechanisms of toxicity for zinc oxide nanoparticles in the body, and what strategies are used to reduce their potential harmful effects for safe therapeutic use?"}
{"doi": "https://doi.org/10.3389/fgene.2023.1108416", "title": "Multi-environment analysis enhances genomic prediction accuracy of agronomic traits in sesame", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 8, "authors": ["Ye Bi", "Gota Morota", "Zvi Peleg", "Idan Sabag"], "abstract": "Decoding the genome confers the capability to predict characteristics of the organism(phenotype) from DNA (genotype). We describe the present status and future prospects of genomic prediction of complex traits in humans. Some highly heritable complex phenotypes such as height and other quantitative traits can already be predicted with reasonable accuracy from DNA alone. For many diseases, including important common conditions such as coronary artery disease, breast cancer, type I and II diabetes, individuals with outlier polygenic scores (e.g., top few percent) have been shown to have 5 or even 10 times higher risk than average. Several psychiatric conditions such as schizophrenia and autism also fall into this category. We discuss related topics such as the genetic architecture of complex traits, sibling validation of polygenic scores, and applications to adult health, in vitro fertilization (embryo selection), and genetic engineering.", "arxiv_id": "2101.05870v1", "research_question": "How accurate and clinically useful are polygenic risk scores for predicting an individual’s risk of common diseases, and what are the main factors that limit their predictive power?"}
{"doi": "https://doi.org/10.3390/agronomy13112690", "title": "Too Salty or Toxic for Use: A Tale of Starter Fertilizers in Agronomic Cropping Systems", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 9, "authors": ["Lotfi Khiari", "William Makaza"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B meson decays into muon pairs constrain theories beyond the Standard Model, and which types of new physics models are most strongly impacted by such measurements?"}
{"doi": "https://doi.org/10.1002/admt.202201703", "title": "Air‐Permeable Textile Bioelectronics for Wearable Energy Harvesting and Active Sensing", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 15, "authors": ["Shin‐Young Lee", "Ziyuan Che", "Jun Chen", "Pirouz Kavehpour", "Austin Chang", "Justin Li", "Xiao Wan", "Jing Xu", "John Hollister", "Shaolei Wang", "Junyi Yin", "Xun Zhao"], "abstract": "This chapter focuses on vibration energy harvesting using electrostatic converters. It synthesizes the various works carried out on electrostatic devices, from concepts, models and up to prototypes, and covers both standard (electret-free) and electret-based electrostatic vibration energy harvesters (VEH).", "arxiv_id": "1210.5191v1", "research_question": "What are the main advantages and disadvantages of electret-based versus electret-free electrostatic vibration energy harvesters for practical, long-term deployment?"}
{"doi": "https://doi.org/10.1145/3543873.3587534", "title": "Exploration of Framing Biases in Polarized Online Content Consumption", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 8, "authors": ["Markus Reiter-Haas"], "abstract": "Wikipedia and many User-Generated Content (UGC) communities are known for producing reliable, quality content, but also for being vulnerable to false or misleading information. Previous work has shown that many hoaxes on Wikipedia go undetected for extended periods of time. But little is known about the creation of intentionally false or misleading information online. Does collective attention toward a topic increase the likelihood it will spawn disinformation? Here, we measure the relationship between allocation of attention and the production of hoax articles on the English Wikipedia. Analysis of traffic logs reveals that, compared to legitimate articles created on the same day, hoaxes tend to be more associated with traffic spikes preceding their creation. This is consistent with the idea that the supply of false or misleading information on a topic is driven by the attention it receives. These findings improve our comprehension of the determinants of disinformation in UGC communities and could help promote the integrity of knowledge on Wikipedia.", "arxiv_id": "2302.08576v1", "research_question": "What mechanisms might explain why increased public attention to a topic makes it more likely that false or misleading content will be created about it?"}
{"doi": "https://doi.org/10.1016/j.jclepro.2023.137257", "title": "Efficient removal mechanism of an electrical conductivity-enhanced constructed wetlands under particle accumulated conditions", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 7, "authors": ["Lanqian Qin", "Yanan Zhang", "Qin Zhang", "Ronghua Wang", "Shaoyuan Bai", "Yanli Ding", "Mingming Fu", "Xutao Gao", "Jiajun Wang"], "abstract": "The removal of organic micropollutants (OMPs) has been investigated in constructed wetlands (CWs) operated as bioelectrochemical systems (BES). The operation of CWs as BES (CW-BES), either in the form of microbial fuel cells (MFC) or microbial electrolysis cells (MEC), has only been investigated in recent years. The presented experiment used CW meso-scale systems applying a realistic horizontal flow regime and continuous feeding of real urban wastewater spiked with four OMPs (pharmaceuticals), namely carbamazepine (CBZ), diclofenac (DCF), ibuprofen (IBU) and naproxen (NPX). The study evaluated the removal efficiency of conventional CW systems (CW-control) as well as CW systems operated as closed-circuit MFCs (CW-MFCs) and MECs (CW-MECs). Although a few positive trends were identified for the CW-BES compared to the CW-control (higher average CBZ, DCF and NPX removal by 10-17% in CW-MEC and 5% in CW-MFC), these proved to be not statistically significantly different. Mesoscale experiments with real wastewater could thus not confirm earlier positive effects of CW-BES found under strictly controlled laboratory conditions with synthetic wastewaters.", "arxiv_id": "2101.05522v1", "research_question": "What factors typically cause discrepancies in micropollutant removal performance between lab-scale experiments with synthetic wastewater and larger-scale or real-wastewater field systems when using bioelectrochemical constructed wetlands?"}
{"doi": "https://doi.org/10.1016/j.apsusc.2023.157757", "title": "Adsorption and modification behavior of single atoms on the surface of single vacancy graphene: Machine learning accelerated first principle computations", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 16, "authors": ["Yong Liu", "Nan Qu", "Zhonghong Lai", "Jingteng Xue", "Mingwei Li", "Yuan Cheng", "Jingchuan Zhu", "Jingtao Huang", "Fei Zhou", "Jiaying Chen", "Jin Hu"], "abstract": "Data science has become increasingly essential for the production of official statistics, as it enables the automated collection, processing, and analysis of large amounts of data. With such data science practices in place, it enables more timely, more insightful and more flexible reporting. However, the quality and integrity of data-science-driven statistics rely on the accuracy and reliability of the data sources and the machine learning techniques that support them. In particular, changes in data sources are inevitable to occur and pose significant risks that are crucial to address in the context of machine learning for official statistics.\n  This paper gives an overview of the main risks, liabilities, and uncertainties associated with changing data sources in the context of machine learning for official statistics. We provide a checklist of the most prevalent origins and causes of changing data sources; not only on a technical level but also regarding ownership, ethics, regulation, and public perception. Next, we highlight the repercussions of changing data sources on statistical reporting. These include technical effects such as concept drift, bias, availability, validity, accuracy and completeness, but also the neutrality and potential discontinuation of the statistical offering. We offer a few important precautionary measures, such as enhancing robustness in both data sourcing and statistical techniques, and thorough monitoring. In doing so, machine learning-based official statistics can maintain integrity, reliability, consistency, and relevance in policy-making, decision-making, and public discourse.", "arxiv_id": "2306.04338v1", "research_question": "What practical steps can statistical agencies take to detect, monitor, and mitigate the effects of changing data sources (such as concept drift, bias, or reduced availability) when using machine learning for official statistics?"}
{"doi": "https://doi.org/10.1111/jnc.15915", "title": "Hyaluronan hydrates and compartmentalises the <scp>CNS</scp>/<scp>PNS</scp> extracellular matrix and provides niche environments conducive to the optimisation of neuronal activity", "language": "en", "created_date": "2023-07-27T00:00:00", "publication_year": 2023, "cited_by_count": 6, "authors": ["James Melrose"], "abstract": "On 2017 August 17 a binary neutron star coalescence candidate (later designated GW170817) with merger time 12:41:04 UTC was observed through gravitational waves by the Advanced LIGO and Advanced Virgo detectors. The Fermi Gamma-ray Burst Monitor independently detected a gamma-ray burst (GRB 170817A) with a time delay of $\\sim$1.7 s with respect to the merger time. From the gravitational-wave signal, the source was initially localized to a sky region of 31 deg$^2$ at a luminosity distance of $40^{+8}_{-8}$ Mpc and with component masses consistent with neutron stars. The component masses were later measured to be in the range 0.86 to 2.26 Msun. An extensive observing campaign was launched across the electromagnetic spectrum leading to the discovery of a bright optical transient (SSS17a, now with the IAU identification of AT 2017gfo) in NGC 4993 (at $\\sim$40 Mpc) less than 11 hours after the merger by the One-Meter, Two Hemisphere (1M2H) team using the 1 m Swope Telescope. The optical transient was independently detected by multiple teams within an hour. Subsequent observations targeted the object and its environment. Early ultraviolet observations revealed a blue transient that faded within 48 hours. Optical and infrared observations showed a redward evolution over $\\sim$10 days. Following early non-detections, X-ray and radio emission were discovered at the transient's position $\\sim$9 and $\\sim$16 days, respectively, after the merger. Both the X-ray and radio emission likely arise from a physical process that is distinct from the one that generates the UV/optical/near-infrared emission. No ultra-high-energy gamma-rays and no neutrino candidates consistent with the source were found in follow-up searches. (Abridged)", "arxiv_id": "1710.05833v2", "research_question": "What physical mechanisms can produce a time delay between the gravitational-wave signal from a neutron-star merger and the associated short gamma-ray burst?"}
{"doi": "https://doi.org/10.1016/j.jaad.2023.08.103", "title": "Executive summary: Guidelines of care for the management of atopic dermatitis in adults with phototherapy and systemic therapies", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 15, "authors": ["David E. Cohen", "Lindsy Frazer-Green", "Lionel Bercovitch", "Jonathan I. Silverberg", "Anne Marie Singh", "Robert Sidbury", "Ali Alikhan", "Peggy A. Wu", "Jennifer M. Darr", "Aaron M. Drucker", "Lawrence F. Eichenfield", "Dawn Marie R. Davis", "Kathryn Schwarzenberger", "Amy S. Paller"], "abstract": "Demand for health care is constantly increasing due to the ongoing demographic change, while at the same time health service providers face difficulties in finding skilled personnel. This creates pressure on health care systems around the world, such that the efficient, nationwide provision of primary health care has become one of society's greatest challenges. Due to the complexity of health care systems, unforeseen future events, and a frequent lack of data, analyzing and optimizing the performance of health care systems means tackling a wicked problem. To support this task for primary care, this paper introduces the hybrid agent-based simulation model SiM-Care. SiM-Care models the interactions of patients and primary care physicians on an individual level. By tracking agent interactions, it enables modelers to assess multiple key indicators such as patient waiting times and physician utilization. Based on these indicators, primary care systems can be assessed and compared. Moreover, changes in the infrastructure, patient behavior, and service design can be directly evaluated. To showcase the opportunities offered by SiM-Care and aid model validation, we present a case study for a primary care system in Germany. Specifically, we investigate the effects of an aging population, a decrease in the number of physicians, as well as the combined effects.", "arxiv_id": "1910.11027v2", "research_question": "What types of data and validation steps are typically required to ensure an agent-based simulation of primary health care produces reliable, actionable results for policymakers?"}
{"doi": "https://doi.org/10.54153/sjpas.2023.v5i1.445", "title": "Learnheuristics in routing and scheduling problems: A review", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 8, "authors": ["Esam Taha Yaseen", "Ahmed Abdulmunem Hussein", "Ahmed Noori Rashid"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B meson decays to muon pairs constrain or rule out models of new physics beyond the Standard Model?"}
{"doi": "https://doi.org/10.1111/ajps.12771", "title": "Institutions and Political Restraint", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 6, "authors": ["Michael M. Ting", "Giovanna M. Invernizzi"], "abstract": "A lot of business and research effort currently deals with the so called decentralised ledger technology blockchain. Putting it to use carries the tempting promise to make the intermediaries of social interactions superfluous and furthermore keep secure track of all interactions. Currently intermediaries such as banks and notaries are necessary and must be trusted, which creates great dependencies, as the financial crisis of 2008 painfully demonstrated. Especially banks and notaries are said to become dispensable as a result of using the blockchain. But in real-world applications of the blockchain, the power of central actors does not dissolve, it only shifts to new, democratically illegitimate, uncontrolled or even uncontrollable power centers. As interesting as the blockchain technically is, it doesn't efficiently solve any real-world problem and is no substitute for traditional political processes or democratic regulation of power. Research efforts investigating the blockchain should be halted.", "arxiv_id": "2405.06097v1", "research_question": "How can democratic accountability and regulatory oversight be effectively maintained or introduced in decentralized ledger systems to prevent concentration of unregulated power?"}
{"doi": "https://doi.org/10.1016/j.geomorph.2023.108972", "title": "Secular shoreline response to large-scale estuarine shoal migration and welding", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 7, "authors": ["Vincent Mazeiraud", "Alexandre Nicolae Lerma", "Marine Vandenhove", "Stéphane Bujan", "Vincent Hanquiez", "Bruno Castelle", "Cyril Mallet", "Ema Dalet", "Vincent Marieu"], "abstract": "An emerging area of research is the study of macroscale migration patterns as a network of nodes that represent places (e.g., countries, cities, and rural areas) and edges that encode migration ties that connect those places. In this chapter, we first review advances in the study of migration networks and recent work that has employed network analysis to examine such networks at different geographical scales. In our discussion, we focus in particular on global scale migration networks. We then propose ways to leverage network analysis in concert with digital technologies and online geolocated data to examine the structure and dynamics of migration networks. The implementation of such approaches for studying migration networks faces many challenges, including ethical ones, methodological ones, socio-technological ones (e.g., data availability and reuse), and research reproducibility. We detail these challenges, and we then consider possible ways of linking digital geolocated data to administrative and survey data as a way of harnessing new technologies to construct increasingly realistic migration networks (e.g., using multiplex networks). We also briefly discuss new methods (e.g., multilayer network analysis) in network analysis and adjacent fields (e.g., machine learning) that can help advance understanding of macroscale patterns of migration.", "arxiv_id": "2002.10992v2", "research_question": "What are the main ethical and privacy challenges when using geolocated digital data to study large-scale human migration, and what best practices can researchers follow to mitigate those risks?"}
{"doi": "https://doi.org/10.1161/jaha.122.030240", "title": "Association Between Hypertension and Diabetes Control and COVID‐19 Severity: National Patient‐Centered Clinical Research Network, United States, March 2020 to February 2022", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 16, "authors": ["Deborah B. Rolka", "Anuradha Paranjape", "Tegan K. Boehmer", "H. Timothy Bunnell", "Wenke Hwang", "Jihad S. Obeid", "Philip Giordano", "Benjamin D. Horne", "David A. Hanauer", "Alyson B. Goodman", "Abu Saleh Mohammad Mosa", "William E. Trick", "Patricia Robinson", "Michael D. Kappelman", "Deepika Thacker", "Brian Ostasiewski", "Nathan M. Pajor", "Akaki Lekiachvili", "Dimitri Christakis", "Lav P. Patel", "Rebecca C. Woodruff", "Evelyn Twentyman", "Faraz S. Ahmad", "Lindsay G. Cowell", "Sandra L. Jackson", "Mark G. Weiner", "Harold P. Lehmann", "Jason P. Block", "Suchitra Rao", "Bernard Chang", "Joshua L. Denson", "Alexander J. Stoddard", "Daniel Fort", "Thomas W. Carton", "Kenneth H. Mayer", "Samyuktha Nandhakumar", "Jon Puro", "Bridget Nolan", "Saul Blecker", "Janis L. Curtis", "Rachel Hess", "Kshema Nagavedu", "Julia Fearrington", "Elizabeth A. Chrischilles", "William R. Hogan", "Jonathan C. Silverstein", "James C. McClay"], "abstract": "This work presents a simple and realistic approach to handle the available data of COVID-19 patients in India and to forecast the scenario. The model proposed is based on the available facts like the onset of lockdown (as announced by the Government on 25th day, τ0 and the recovery pattern dictated by a mean life recovery time of τ1 ( normally said to be around 14 days). The data of infected COVID-19 patients from March 2, to April 16, 2020 has been used to fit the evolution of infected, recovery and death counts. A slow rising exponential growth, with R0 close to 1/6, is found to represent the infected counts indicating almost a linear rise. The rest of growth, saturation and decay of data is comprehensibly modelled by incorporating lockdown time controlled R0, having a normal error function like behaviour decaying to zero in some time frame of τ2 . The recovery mean life time τ1 dictates the peak and decay. The results predicted for coming days are interesting and optimistic. The introduced time constants based on experimental data for both the recovery rate as well as for determining the time span of activity of R0 after the lockdown are subject of debate and provide possibility to introduce trigger factors to alter these to be more suited to the model. The model can be extended to other communities with their own R0 and recovery time parameters.", "arxiv_id": "2004.09912v1", "research_question": "How does a time-varying reproduction number (R0) that decreases after interventions like lockdowns affect the timing and height of the epidemic peak and the total number of cases?"}
{"doi": "https://doi.org/10.1021/acsami.2c15818", "title": "Studying the Synergistic Effect of Substrate Stiffness and Cyclic Stretch Level on Endothelial Cells Using an Elastomeric Cell Culture Chamber", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 10, "authors": ["Sara Baratchi", "Chanly Chheang", "Karlheinz Peter", "Ngan Nguyen", "Nadia Chandra Sekar", "Ying Zhou", "Scott Needham", "Khashayar Khoshmanesh", "Peter Thurgood", "Sergio Aguilera Suarez", "Austin Lai", "Elena Pirogova"], "abstract": "Optimal characterization of the mechanical properties of both cells and their surrounding is an issue of major interest. Indeed, cell function and development are strongly influenced by external stimuli. Furthermore, a change in cell mechanics might, in some cases, associate with diseases or malfunctioning. In this work, atomic force microscopy (AFM) was applied to examine the mechanical properties of the silicone elastomer polydimethylsiloxane (PDMS) a common substrate in cell culture. Force spectroscopy analysis was done over different specimens of this elastomeric material containing varying ratios of resin to cross-linker in its structure (5:1, 10:1, 20:1, 30:1 and 50:1), which impacts the final material properties (e.g., stiffness, elasticity). To quantify the mechanical properties of the PDMS, factors as the modulus of Young, the maximum adhesive forces as well as both relaxation amplitudes and times upon constant height contact of the tip (dwell time different of zero) were calculated from the different segments forming the force curves. It is demonstrated that the material stiffness is increased by prior oxygen plasma treatment of the sample, required for hydrophilic switching, contrarily to what observed for its adhesiveness. Subsequent incubation of endothelial HUVEC cells on top of these plasma treated PDMS systems yields minor variation in cell mechanics in comparison to those obtained on a glass reference, on which cells show much higher spreading tendency and, by extension, a remarkable membrane hardening. Thus, surface wettability turns a factor of higher relevance than substrate stiffness inducing variations in the cell mechanics.", "arxiv_id": "2210.15038v1", "research_question": "How does substrate surface wettability influence cell mechanical properties (e.g., membrane stiffness and spreading) compared with the effects of substrate stiffness, and what cellular mechanisms mediate these differences?"}
{"doi": "https://doi.org/10.1007/s10723-023-09668-9", "title": "Solar Irradiance Prediction Using an Optimized Data Driven Machine Learning Models", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 20, "authors": ["Gaurav Saini", "Mantosh Kumar", "Nishant Kumar", "Kumari Namrata"], "abstract": "Data science has become increasingly essential for the production of official statistics, as it enables the automated collection, processing, and analysis of large amounts of data. With such data science practices in place, it enables more timely, more insightful and more flexible reporting. However, the quality and integrity of data-science-driven statistics rely on the accuracy and reliability of the data sources and the machine learning techniques that support them. In particular, changes in data sources are inevitable to occur and pose significant risks that are crucial to address in the context of machine learning for official statistics.\n  This paper gives an overview of the main risks, liabilities, and uncertainties associated with changing data sources in the context of machine learning for official statistics. We provide a checklist of the most prevalent origins and causes of changing data sources; not only on a technical level but also regarding ownership, ethics, regulation, and public perception. Next, we highlight the repercussions of changing data sources on statistical reporting. These include technical effects such as concept drift, bias, availability, validity, accuracy and completeness, but also the neutrality and potential discontinuation of the statistical offering. We offer a few important precautionary measures, such as enhancing robustness in both data sourcing and statistical techniques, and thorough monitoring. In doing so, machine learning-based official statistics can maintain integrity, reliability, consistency, and relevance in policy-making, decision-making, and public discourse.", "arxiv_id": "2306.04338v1", "research_question": "What practical steps and monitoring strategies can statistical agencies use to detect, assess, and mitigate the impacts of changing data sources on the accuracy and continuity of official statistics?"}
{"doi": "https://doi.org/10.1021/acs.cgd.3c01016", "title": "Multiple Interactions between Cations and Anions Achieve a High Density of <i>Cyclo</i>-N<sub>5</sub> Salts", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 12, "authors": ["Pengfei Wang", "Yang Du", "Lei Shi", "Chengguo Sun", "Chong Zhang", "Bingcheng Hu", "Chao Gao"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of very rare decays of neutral B mesons into muon pairs constrain physics beyond the Standard Model, and what level of experimental precision or significance is typically needed to rule out or confirm extensions such as supersymmetry or extended Higgs sectors?"}
{"doi": "https://doi.org/10.1109/spin57001.2023.10116311", "title": "Performance Improvement of Water Body Segmentation by DeeplabV3+Using Two Dimensional Variational Mode Decomposition", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 7, "authors": ["Bichu George", "V. V. Variyar Sajith", "Ramesh Sivanpillai", "V. Sowmya"], "abstract": "The segmentation of satellite images is crucial in remote sensing applications. Existing methods face challenges in recognizing small-scale objects in satellite images for semantic segmentation primarily due to ignoring the low-level characteristics of the underlying network and due to containing distinct amounts of information by different feature maps. Thus, in this research, a tri-level attention-based DeepLabv3+ architecture (DeepTriNet) is proposed for the semantic segmentation of satellite images. The proposed hybrid method combines squeeze-and-excitation networks (SENets) and tri-level attention units (TAUs) with the vanilla DeepLabv3+ architecture, where the TAUs are used to bridge the semantic feature gap among encoders output and the SENets used to put more weight on relevant features. The proposed DeepTriNet finds which features are the more relevant and more generalized way by its self-supervision rather we annotate them. The study showed that the proposed DeepTriNet performs better than many conventional techniques with an accuracy of 98% and 77%, IoU 80% and 58%, precision 88% and 68%, and recall of 79% and 55% on the 4-class Land-Cover.ai dataset and the 15-class GID-2 dataset respectively. The proposed method will greatly contribute to natural resource management and change detection in rural and urban regions through efficient and semantic satellite image segmentation", "arxiv_id": "2310.06848v1", "research_question": "How can neural network architectures be designed or adapted to improve the detection and segmentation of small-scale objects in satellite imagery?"}
{"doi": "https://doi.org/10.15379/ijmst.v10i2.1392", "title": "Systemic approach to Smart Urban Transformations Of Cities Case of Muscat- Oman", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 8, "authors": ["Montasser Abdelghani", "Islam Sallam"], "abstract": "Smart cities have been a very active research area in the past 20 years, while continuously adapting to new technological advancements and keeping up with the times regarding sustainability and climate change. In this context, there have been numerous proposals to expand the scope of smart cities, focusing on resilience and sustainability, among other aspects, resulting in terms like smart sustainable cities. At the same time, there is an ongoing discussion regarding the degree in which smart cities put people at their centre. In this work, we argue toward expanding the current smart city definition by integrating the circular economy as one of its central pillars and adopting the term smart (and) circular city. We discuss the ways a smart and circular city encompasses both sustainability and smartness in an integral manner, while also being well-positioned to foster novel business activity and models and helping to place citizens at the heart of the smart city. In this sense, we also argue that previous research in smart cities and technologies, such as those related to Industry 4.0, can serve as a cornerstone to implement circular economy activities within cities, at a scale that exceeds current activities that are based on more conventional approaches. We also outline current open challenges in this domain and research questions that still need to be addressed.", "arxiv_id": "2410.22012v1", "research_question": "What practical steps can city governments take to integrate circular economy principles with smart technologies while ensuring strong citizen engagement and equitable outcomes?"}
{"doi": "https://doi.org/10.1007/s00259-023-06168-6", "title": "An exploration of the feasibility and clinical value of half-dose 5-h total-body 18F-FDG PET/CT scan in patients with Takayasu arteritis", "language": "en", "created_date": "2023-03-04T00:00:00", "publication_year": 2023, "cited_by_count": 10, "authors": ["Hongcheng Shi", "Yushen Gu", "Bing Wu", "Dilibire Adili", "Danjie Cai", "Haojun Yu", "Yiqiu Zhang"], "abstract": "Following the work of Okuyama, Takayasu and Takayasu [Okuyama, Takayasu and Takayasu 1999] we analyze huge databases of Japanese companies' financial figures and confirm that the Zipf's law, a power law distribution with the exponent -1, has been maintained over 30 years in the income distribution of Japanese companies with very high precision. Similar power laws are found not only in income distribution of company's income, but also in the distributions of capital, sales and number of employees. From the data we find an important time evolutionary property that the growth rate of income is approximately independent of the value of income, namely, small companies and large ones have similar statistical chances of growth. This observational fact suggests the applicability of the theory of multiplicative stochastic processes developed in statistical physics. We introduce a discrete version of Langevin equation with additive and multiplicative noises as a simple time evolution model of company's income. We test the validity of the Takayasu-Sato-Takayasu condition [Takayasu, Sato and Takayasu 1997] for having an asymptotic power law distribution as a unique statistically steady solution. Directly estimated power law exponents and theoretically evaluated ones are compared resulting a reasonable fit by introducing a normalization to reduce the effect of gross economic change.", "arxiv_id": "0308365v1", "research_question": "What simple mechanisms or economic processes can explain why company size distributions (income, capital, sales, employees) follow Zipf's law or other power laws?"}
{"doi": "https://doi.org/10.1016/j.bbagen.2023.130416", "title": "4′,7-dihydroxyflavone conjugated carbon nanotube formulation demonstrates improved efficacy against Leishmania parasite", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 6, "authors": ["Santanu Sasidharan", "Prakash Saudagar"], "abstract": "Energetic ion irradiation is an effective method for studying how single and multi-shelled carbon nanotubes break apart. The energy from ions is dissipated through both linear and nonlinear processes in the nanotubes, leading to defect formation. Fragmentation occurs via atomic collision cascades and thermal spikes, each described by different theoretical models. Experiments with Cs-irradiated nanotubes support these models, and an information-theoretic approach further explains the fragmentation mechanisms. Sputtered species yield probability distributions, which are analyzed using Shannon entropy and fractal dimension to assess spatial characteristics. Kullback-Leibler divergence helps identify the diversity of emission mechanisms. Together, thermal and information-theoretic models clarify and distinguish the roles of collision cascades and thermal spikes in nanotube fragmentation.", "arxiv_id": "2511.15467v1", "research_question": "How can information-theoretic measures such as Shannon entropy and Kullback–Leibler divergence be used to distinguish between different physical mechanisms (e.g., collision cascades vs. thermal spikes) responsible for ion-induced fragmentation of carbon nanotubes?"}
{"doi": "https://doi.org/10.3390/math11112516", "title": "Harnack Estimation for Nonlinear, Weighted, Heat-Type Equation along Geometric Flow and Applications", "language": "en", "created_date": "2023-06-01T00:00:00", "publication_year": 2023, "cited_by_count": 6, "authors": ["Shahroud Azami", "Sujit Bhattacharyya", "Apurba Saha", "Yanlin Li", "Shyamal Kumar Hui"], "abstract": "We derive the two-breather solution of the class I infinitely extended nonlinear Schrodinger equation (NLSE). We present a general form of this multi-parameter solution that includes infinitely many free parameters of the equation and free parameters of the two breather components. Particular cases of this solution include rogue wave triplets, and special cases of breather-to-soliton and rogue wave-to-soliton transformations. The presence of many parameters in the solution allows one to describe wave propagation problems with higher accuracy than with the use of the basic NLSE.", "arxiv_id": "2009.09853v1", "research_question": "How do adding higher-order terms or extra free parameters to nonlinear Schrödinger-type models affect the formation, stability, and interactions of breathers and rogue waves in physical systems, and how should one choose those parameters to match observations?"}
{"doi": "https://doi.org/10.1136/tc-2022-057727", "title": "Crowding-out effect of tobacco consumption in Serbia", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 6, "authors": ["Jovan Zubović", "Mihailo Đukić", "Olivera Jovanović", "Marko Vladisavljević"], "abstract": "The progress of some AI paradigms such as deep learning is said to be linked to an exponential growth in the number of parameters. There are many studies corroborating these trends, but does this translate into an exponential increase in energy consumption? In order to answer this question we focus on inference costs rather than training costs, as the former account for most of the computing effort, solely because of the multiplicative factors. Also, apart from algorithmic innovations, we account for more specific and powerful hardware (leading to higher FLOPS) that is usually accompanied with important energy efficiency optimisations. We also move the focus from the first implementation of a breakthrough paper towards the consolidated version of the techniques one or two year later. Under this distinctive and comprehensive perspective, we study relevant models in the areas of computer vision and natural language processing: for a sustained increase in performance we see a much softer growth in energy consumption than previously anticipated. The only caveat is, yet again, the multiplicative factor, as future AI increases penetration and becomes more pervasive.", "arxiv_id": "2109.05472v2", "research_question": "What factors determine whether increasing a model's size or parameter count will lead to a proportional rise in inference energy consumption in real-world deployments?"}
{"doi": "https://doi.org/10.1016/j.triboint.2023.108379", "title": "A synergetic strategy based on texture and Nano-TiO2 grease to improve the tribological and insulating properties of the matrix under current-carrying friction", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 16, "authors": ["Yanqiu Xia", "Kuo Yang", "Xinliang Feng"], "abstract": "The 2023-2032 Planetary Science and Astrobiology Decadal Survey Origins, Worlds, and Life recommended that \"NASA develop scientific exploration strategies, as it has for Mars, in areas of broad scientific importance, e.g., Venus... that have an increasing number of U.S. missions and international collaboration opportunities\" (OWL, p.22-10). In NASA's initial responses to that Decadal Survey, the agency asserted that \"...specific scientific exploration strategies should be community generated by bodies such as the Analysis Groups,\" thus placing the onus on the planetary community to generate and support these exploration strategies. In late 2022, the Venus Exploration Analysis Group began a project to develop a new exploration strategy for Venus, reflecting the 2021 selections of the VERITAS, DAVINCI, and EnVision missions and the sweeping comparative planetology recommendations relevant to Venus in Origins, Worlds, and Life.\n  This is that strategy.\n  Taking a broad look at the scientific, technological, and programmatic advances required to address the key outstanding questions that Venus poses, and predicated on VERITAS, DAVINCI, and EnVision flying as planned in the early 2030s, this report outlines a set of actions available to NASA, VEXAG, and the planetary science community at large to establish a sustained program of Venus exploration in the years and decades ahead. Key to this approach is recognizing Venus as a unique setting where multiple, cross-disciplinary, Decadal-level planetary, Earth, heliophysics, and exoplanet science questions can be addressed, as well as being a worthy target of exploration in its own right.\n  This report offers Assessments of the current state of Venus exploration, and Actions for the U.S. and international Venus community, as well as NASA, to consider. This strategy is a living document and should be updated as warranted.", "arxiv_id": "2412.06830v1", "research_question": "What are the main scientific and technological challenges to establishing a sustained program of Venus exploration, and what types of missions or instruments would best address those challenges?"}
{"doi": "https://doi.org/10.1007/s10389-023-02083-0", "title": "The effect of internet addiction on physical activity and dietary habits in high school students", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 6, "authors": ["Özen Sökmen", "Eda Gülbetekin", "Erkan Güven"], "abstract": "Student attitudes and approaches to problem solving can impact how well they learn physics. Prior research in the US using a validated Attitude and Approaches to Problem Solving (AAPS) survey suggests that there are major differences between students in introductory physics and astronomy courses and physics experts in terms of their attitudes and approaches to physics problem solving. Here we discuss the validation, administration and analysis of data for the Turkish version of the AAPS survey for high school and university students in Turkey. After the validation and administration of the Turkish version of the survey, the analysis of the data was conducted by grouping the data by grade level, school type, and gender. While there are no statistically significant differences between the averages of various groups on the survey, overall, the university students in Turkey were more expert-like than vocational high school students. On an item by item basis, there are statistically differences between the averages of the groups on many items. For example, on average, the university students demonstrated less expert-like attitudes about the role of equations and formulas in problem solving, in solving difficult problems, and in knowing when the solution is not correct, whereas they displayed more expert-like attitudes and approaches on items related to meta-cognition in physics problem solving. A principal component analysis on the data yields item clusters into which the student responses on various survey items can be grouped. A comparison of the responses of the Turkish and American university students enrolled in algebra-based introductory physics courses shows that on more than half of the items, the responses of these two groups were statistically significantly different with the US students on average responding to the items in more expert-like manner.", "arxiv_id": "1604.02620v1", "research_question": "What teaching practices or interventions have been shown to effectively promote more expert-like attitudes and approaches to physics problem solving among high school and university students?"}
{"doi": "https://doi.org/10.3390/ijms241310442", "title": "Synthesis and Biological Activity of Myricetin Derivatives Containing Pyrazole Piperazine Amide", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 13, "authors": ["Xiao Cao", "Wei Xue", "Tao Zhang", "Wei Zeng", "Hui Xin", "Fang Liu", "Zhiling Sun", "Xing Li"], "abstract": "The function of the organism hinges on the performance of its information-processing networks, which convey information via molecular recognition. Many paths within these networks utilize molecular codebooks, such as the genetic code, to translate information written in one class of molecules into another molecular \"language\" . The present paper examines the emergence and evolution of molecular codes in terms of rate-distortion theory and reviews recent results of this approach. We discuss how the biological problem of maximizing the fitness of an organism by optimizing its molecular coding machinery is equivalent to the communication engineering problem of designing an optimal information channel. The fitness of a molecular code takes into account the interplay between the quality of the channel and the cost of resources which the organism needs to invest in its construction and maintenance. We analyze the dynamics of a population of organisms that compete according to the fitness of their codes. The model suggests a generic mechanism for the emergence of molecular codes as a phase transition in an information channel. This mechanism is put into biological context and demonstrated in a simple example.", "arxiv_id": "1007.4471v1", "research_question": "How can concepts from rate-distortion theory be applied to explain why and how molecular coding systems (like the genetic code) emerge and evolve in biological populations?"}
{"doi": "https://doi.org/10.1519/jsc.0000000000004436", "title": "Reliability, Usefulness, and Validity of Field-Based Vertical Jump Measuring Devices", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 21, "authors": ["Dylan O'Leary", "Jennifer Murphy", "Thomas M. Comyns"], "abstract": "This study aimed to assess the reliability and validity of the Polar V800 to measure vertical jump height. Twenty-two physically active healthy men (age: 22.89 +- 4.23 years; body mass: 70.74 +- 8.04 kg; height: 1.74 +- 0.76 m) were recruited for the study. The reliability was evaluated by comparing measurements acquired by the Polar V800 in two identical testing sessions one week apart. Validity was assessed by comparing measurements simultaneously obtained using a force platform (gold standard), high-speed camera and the Polar V800 during squat jump (SJ) and countermovement jump (CMJ) tests. In the test-retest reliability, high intraclass correlation coefficients (ICCs) were observed (mean: 0.90, SJ and CMJ) in the Polar V800. There was no significant systematic bias +- random errors (p > 0.05) between test-retest. Low coefficients of variation (<5%) were detected in both jumps in the Polar V800. In the validity assessment, similar jump height was detected among devices (p > 0.05). There was almost perfect agreement between the Polar V800 compared to a force platform for the SJ and CMJ tests (Mean ICCs = 0.95; no systematic bias +- random errors in SJ mean: -0.38 +- 2.10 cm, p > 0.05). Mean ICC between the Polar V800 versus high-speed camera was 0.91 for the SJ and CMJ tests, however, a significant systematic bias +- random error (0.97 +- 2.60 cm; p = 0.01) was detected in CMJ test. The Polar V800 offers valid, compared to force platform, and reliable information about vertical jump height performance in physically active healthy young men.", "arxiv_id": "2203.16442v1", "research_question": "What factors should be considered when evaluating the validity and reliability of consumer wearable devices for measuring vertical jump height in athletes?"}
{"doi": "https://doi.org/10.3390/vetsci10090565", "title": "Correlating Access to Primary Medical Care and Veterinary Care Providers: A Novel Application of Spatial Gravity Modelling", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 12, "authors": ["Sue M. Neal"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "Why are rare flavor-changing neutral-current decays of B mesons to muon pairs considered especially sensitive probes for physics beyond the Standard Model?"}
{"doi": "https://doi.org/10.1016/j.compositesa.2023.107989", "title": "Reactive extrusion additive manufacturing of a short carbon fiber thermosetting composite via active mixing", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 7, "authors": ["Robert Pavlovic", "Mehran Tehrani", "Pratik Koirala", "Cole Mensch", "Michael J. Fogg", "Carolyn Conner Seepersad", "Athena Aber"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of very rare B-meson decays to muon pairs constrain or rule out extensions of the Standard Model, and what types of new-physics scenarios are most affected?"}
{"doi": "https://doi.org/10.1016/j.fuel.2023.129106", "title": "Effect of diluent N2 addition on NH3/H2/air combustion characteristics", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 26, "authors": ["Fahui Wang", "Shuangshuang Zuo", "Haoxin Deng", "Xiaoping Wen", "Anchao Zhang", "Guoyan Chen"], "abstract": "A 2-dimensional discrete Boltzmann model for combustion is presented. Mathematically, the model is composed of two coupled discrete Boltzmann equations for two species and a phenomenological equation for chemical reaction process. Physically, the model is equivalent to a reactive Navier-Stokes model supplemented by a coarse-grained model for the thermodynamic nonequilibrium behaviours. This model adopts 16 discrete velocities. It works for both subsonic and supersonic combustion phenomena with flexible specific heat ratio. To discuss the physical accuracy of the coarse-grained model for nonequilibrium behaviours, three other discrete velocity models are used for comparisons. Numerical results are compared with analytical solutions based on both the first-order and second-order truncations of the distribution function. It is confirmed that the physical accuracy increases with the increasing moment relations needed by nonequlibrium manifestations. Furthermore, compared with the single distribution function model, this model can simulate more details of combustion.", "arxiv_id": "1405.5500v3", "research_question": "How do discrete kinetic (Boltzmann/lattice) models represent thermodynamic nonequilibrium effects in combustion, and what are their main advantages and limitations compared with conventional reactive Navier–Stokes approaches?"}
{"doi": "https://doi.org/10.1109/tcomm.2023.3255251", "title": "Benchmarking Neural Capacity Estimation: Viability and Reliability", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 6, "authors": ["Nariman Farsad", "Farhad Mirkarimi", "Stefano Rini"], "abstract": "Recently, several methods have been proposed for estimating the mutual information from sample data using deep neural networks. These estimators ar referred to as neural mutual information estimation (NMIE)s. NMIEs differ from other approaches as they are data-driven estimators. As such, they have the potential to perform well on a large class of capacity problems. In order to test the performance across various NMIEs, it is desirable to establish a benchmark encompassing the different challenges of capacity estimation. This is the objective of this paper. In particular, we consider three scenarios for benchmarking:i the classic AWGN channel, ii channels continuous inputs optical intensity and peak-power constrained AWGN channel iii channels with a discrete output, i.e., Poisson channel. We also consider the extension to the multi-terminal case with iv the AWGN and optical MAC models. We argue that benchmarking a certain NMIE across these four scenarios provides a substantive test of performance. In this paper we study the performance of mutual information neural estimator (MINE), smoothed mutual information lower-bound estimator (SMILE), and directed information neural estimator (DINE). and provide insights on the performance of other methods as well. To summarize our benchmarking results, MINE provides the most reliable performance.", "arxiv_id": "2203.11793v2", "research_question": "What are the main practical challenges and failure modes when using neural-network-based estimators to estimate mutual information from finite samples across different channel types (continuous, discrete, and multi-terminal)?"}
{"doi": "https://doi.org/10.1142/s0192415x23500246", "title": "Bufalin Inhibits Tumorigenesis and SREBP-1-Mediated Lipogenesis in Hepatocellular Carcinoma via Modulating the ATP1A1/CA2 Axis", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 12, "authors": ["Changjing Huang", "Ling Qian", "Ying Zhu", "Chenyue Zhang", "Lin Xie", "Zhiqiang Meng", "Dan Wang", "Liping Zhuang", "Yingke Zhao"], "abstract": "The nature and origin of supermassive black holes (SMBHs) remain an open matter of debate within the scientific community. While various theoretical scenarios have been proposed, each with specific observational signatures, the lack of sufficiently sensitive X-ray observations hinders the progress of observational tests. In this white paper, we present how AXIS will contribute to solving this issue. With an angular resolution of 1.5$^{\\prime\\prime}$ on-axis and minimal off-axis degradation, we have designed a deep survey capable of reaching flux limits in the [0.5-2] keV range of approximately 2$\\times$10$^{-18}$ \\fcgs~ over an area of 0.13 deg$^2$ in approximately 7 million seconds (7 Ms). Furthermore, we have planned an intermediate depth survey covering approximately 2 deg$^2$ and reaching flux limits of about 2$\\times$10$^{-17}$ \\fcgs ~ in order to detect a significant number of SMBHs with X-ray luminosities (L$_X$) of approximately 10$^{42}$ \\lx up to z$\\sim$10. These observations will enable AXIS to detect SMBHs with masses smaller than 10$^5$ \\ms, assuming Eddington-limited accretion and a typical bolometric correction for Type II AGN. AXIS will provide valuable information on the seeding and population synthesis models of SMBH, allowing for more accurate constraints on their initial mass function (IMF) and accretion history from z$\\sim$0-10. To accomplish this, AXIS will leverage the unique synergy of survey telescopes such as JWST, Roman, Euclid, LSST, and the new generation of 30m class telescopes. These instruments will provide optical identification and redshift measurements, while AXIS will discover the smoking gun of nuclear activity, particularly in the case of highly obscured AGN or peculiar UV spectra as predicted and recently observed in the early Universe.", "arxiv_id": "2311.07669v2", "research_question": "What observational signatures in X-ray and multiwavelength data would allow us to distinguish between different supermassive black hole seeding scenarios (for example, light versus heavy seeds) at high redshift?"}
{"doi": "https://doi.org/10.1016/j.energy.2023.129682", "title": "Multi-scenarios transferable learning framework with few-shot for early lithium-ion battery lifespan trajectory prediction", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 28, "authors": ["Jinhao Meng", "Ji Wu", "Mingqiang Lin", "Zhengxiang Song", "Yuqiang You"], "abstract": "Effective thermal management is critical for lithium-ion battery packs' safe and efficient operations, particularly in applications such as drones, where compact designs and varying airflow conditions present unique challenges. This study investigates the thermal performance of a 16-cell lithium-ion battery pack by optimizing cooling airflow configurations and integrating phase change materials (PCMs) for enhanced heat dissipation. Seven geometric configurations were evaluated under airflow speeds ranging from 0 to 15 m/s, reflecting the operational conditions of civilian drones. A comprehensive 3D simulation approach was used to analyze the effects of inlet and outlet configurations, airflow dynamics, and PCM phase transition behavior. Results indicate that the trapezoidal (wide-base) configuration, paired with a 5-inlet and 1-outlet setup, achieves the most balanced performance, effectively maintaining optimal operating temperatures across low and high-speed airflow conditions. PCM integration further stabilized thermal behavior, with phase change durations extending to 12.5 min under tested conditions. These findings highlight the importance of geometric optimization and material integration in advancing compact and reliable thermal management systems for energy-dense battery packs. This study provides a foundation for designing efficient cooling strategies tailored to lightweight applications such as drones and portable energy storage systems.", "arxiv_id": "2502.07070v1", "research_question": "What are the practical advantages and limitations of using phase change materials for thermal management in compact lithium-ion battery packs used in drones and other lightweight applications?"}
{"doi": "https://doi.org/10.1093/eurjcn/zvad049", "title": "Response to the letter to the editor – Dr. ChatGPT in cardiovascular nursing: a deeper dive into trustworthiness, value, and potential risk", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 12, "authors": ["Philip Moons", "Liesbet Van Bulck"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B meson decays to muon pairs constrain extensions of the Standard Model, and which types of new-physics models are most strongly affected by these constraints?"}
{"doi": "https://doi.org/10.1103/physrevapplied.19.034005", "title": "Revisiting Neutron Propagation-Based Phase-Contrast Imaging and Tomography: Use of Phase Retrieval to Amplify the Effective Degree of Brilliance", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 7, "authors": ["W. Kockelmann", "S. Schmidt", "Morten Sales", "David M. Paganin", "Henning Friis Poulsen", "Mario A. Beltran", "Peter M. Kadletz"], "abstract": "Propagation-based neutron phase-contrast tomography was demonstrated using the ISIS pulsed spallation source. The proof-of-concept tomogram with Paganin-type phase-retrieval filter applied exhibited an effective net boost of $23\\pm 1$ in the signal-to-noise ratio as compared to an attenuation-based tomogram, implying a boost in the effective degree of neutron brilliance of over two orders of magnitude. This comparison is for phase retrieval versus conventional absorption with no additional collimation in place. Expressions are provided for the optimal phase-contrast geometry as well as conditions for the validity of the method. The underpinning theory is derived under the assumption of the sample being composed of a single material. The effective boost in brilliance may be employed to give reduced acquisition time, or may instead be used to keep exposure times fixed while improving the measured contrast.", "arxiv_id": "1909.11186v3", "research_question": "How does the single-material assumption used in common phase-retrieval approaches (like Paganin-type filters) limit their applicability to heterogeneous samples, and what practical strategies or extensions exist to apply propagation-based neutron phase-contrast tomography to multi-material or compositionally varying specimens?"}
{"doi": "https://doi.org/10.1016/j.corsci.2023.111303", "title": "Film-forming investigation of NbC-modified HfC coating based on oxyacetylene ablation and argon-atmosphere heating above 2000 °C", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 9, "authors": ["Longqi Liu", "Kezhi Li", "Ruixiang He", "Wenhao Zhang"], "abstract": "The Advanced Accelerator Concepts 2000 (AAC2K) Workshop was held in Santa Fe in June, 2000, and included a wide array of conceptual and theoretical advances at the frontier of accelerator physics. This paper reviews the highlights of the workshop, with subjects ranging from acceleration using lasers, plasmas and microstructures, to the beam physics of muon colliders. Particular emphasis is given to the topics which are relevant to research at existing linear accelerator facilities, and the effect of this research on the capabilities of such facilities.", "arxiv_id": "0009022v1", "research_question": "What are the main technical challenges and potential benefits of integrating laser- or plasma-based acceleration techniques with existing linear accelerator facilities?"}
{"doi": "https://doi.org/10.3390/ejihpe13110177", "title": "Assessment and Psychometric Properties of the 21-Item Depression Anxiety Stress Scale (DASS-21) among Portuguese Higher Education Students during the COVID-19 Pandemic", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 27, "authors": ["Carlos Laranjeira", "Maria dos Anjos Dixe", "Pedro Sousa", "Ana Querido"], "abstract": "The COVID-19 pandemic has, worldwide and up to December 2020, caused over 1.7 million deaths, and put the world's most advanced healthcare systems under heavy stress. In many countries, drastic restrictive measures adopted by political authorities, such as national lockdowns, have not prevented the outbreak of new pandemic's waves. In this article, we propose an integrated detection-estimation-forecasting framework that, using publicly available data, is designed to: (i) learn relevant features of the pandemic (e.g., the infection rate); (ii) detect as quickly as possible the onset (or the termination) of an exponential growth of the contagion; and (iii) reliably forecast the pandemic evolution. The proposed solution is validated by analyzing the COVID-19 second and third waves in the USA.", "arxiv_id": "2101.04620v2", "research_question": "What reliable approaches can be used with publicly available case data to detect early onset of exponential growth in an epidemic, and how do these methods balance early detection against false alarms?"}
{"doi": "https://doi.org/10.1016/j.polymdegradstab.2023.110585", "title": "Synthesis of macromolecular antioxidants containing thioether and aromatic secondary amine to improve the anti-oxidation properties of EPDM", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 14, "authors": ["Daigang Peng", "Xiaoyu Meng", "Peng Wei", "Hongjie Song", "Chuanbo Cong", "Qiong Zhou", "Qingkun Liu"], "abstract": "We present the results of processing the effects of the powerful Gamma Ray Burst GRB221009A captured by the charged particle detectors (electrostatic analyzers and solid-state detectors) onboard spacecraft at different points in the heliosphere on October 9, 2022. To follow the GRB221009A propagation through the heliosphere we used the electron and proton flux measurements from solar missions Solar Orbiter and STEREO-A; Earth magnetosphere and the solar wind missions THEMIS and Wind; meteorological satellites POES15, POES19, MetOp3; and MAVEN - a NASA mission orbiting Mars. GRB221009A had a structure of four bursts: less intense Pulse 1 - the triggering impulse - was detected by gamma-ray observatories at 131659 UT (near the Earth); the most intense Pulses 2 and 3 were detected on board all the spacecraft from the list, and Pulse 4 detected in more than 500 s after Pulse 1. Due to their different scientific objectives, the spacecraft, which data was used in this study, were separated by more than 1 AU (Solar Orbiter and MAVEN). This enabled tracking GRB221009A as it was propagating across the heliosphere. STEREO-A was the first to register Pulse 2 and 3 of the GRB, almost 100 seconds before their detection by spacecraft in the vicinity of Earth. MAVEN detected GRB221009A Pulses 2, 3, and 4 at the orbit of Mars about 237 seconds after their detection near Earth. By processing the time delays observed we show that the source location of the GRB221009A was at RA 288.5 degrees, Dec 18.5 degrees (J2000) with an error cone of 2 degrees", "arxiv_id": "2309.05856v1", "research_question": "How can time delays between detections on spacecraft positioned at different locations in the heliosphere be used to determine the sky location of a distant gamma‑ray burst, and what are the main sources of uncertainty in that triangulation?"}
{"doi": "https://doi.org/10.1016/j.jeurceramsoc.2023.07.001", "title": "Strong textured ferroelectric ceramics CaBi2Nb2O9 with superior piezoelectric response via conventional solid-state technique", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 9, "authors": ["Lihua Yin", "Ming Tao", "Shuangming Li", "Yuping Sun", "Jie Yang", "Xingli Wang", "Gaolei Zhao", "Wenhai Song", "Xuebin Zhu", "Caofeng Pan"], "abstract": "A method for evaluating the piezoelectric response of unpoled ferroelectric ceramics from elastic and dielectric measurements is proposed and tested on BaTiO$_3$. The method is based on the observation that the softening in a ferroelectric phase with respect to the paraelectric phase is of piezoelectric origin. The angular averages of the piezoelectric softening in unpoled ceramics are calculated for ferroelectric phases of different symmetries. The expression of the orientational average with the piezoelectric and dielectric constants of single crystal tetragonal BaTiO$_3$ from the literature reproduces well the softening of the Young's modulus of unpoled ceramic BaTiO$_3$, after a correction for the porosity. The agreement is good in the temperature region sufficiently far from the Curie temperature and from the transition to the orthorhombic phase, where the effect of fluctuations should be negligible, but deviations are found outside this region, and the reason for this is discussed. This validates the determination of the piezoelectric response by means of purely elastic measurements on unpoled samples. The method is indirect and, for quantitative assessments, requires the knowledge of the dielectric tensor. On the other hand, it does not require poling of the sample, and therefore is insensitive to inaccuracies from incomplete poling, and can even be used with materials that cannot be poled, for example due to excessive electrical conductivity. While the proposed example of the Young's modulus of a ceramic provides an orientational average of all the single crystal piezoelectric constants, a Resonant Ultrasound Spectroscopy measurement of a single unpoled ceramic sample through the ferroelectric transition can in principle measure all the piezoelectric constants, together with the elastic ones.", "arxiv_id": "1712.01133v1", "research_question": "What are the main limitations and sources of error when determining piezoelectric coefficients from purely elastic measurements on unpoled ceramic samples, and how can they be mitigated?"}
{"doi": "https://doi.org/10.1002/adma.202211197", "title": "Programmed Self‐Assembly of Single Colloidal Gyroids for Chiral Photonic Crystals", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 16, "authors": ["Dwaipayan Chakrabarti", "Angela Demetriadou", "Wesley Flavell", "Tim Albrecht", "Andreas Neophytou"], "abstract": "Chiral crystals consisting of micro-helices have many optical properties while presently available fabrication processes limit their large-scale applications in photonic devices. Here, by using a simplified simulation method, we investigate a bottom-up self-assembly route to build up helical crystals from the smectic monolayer of colloidal helices racemate. With increasing the density, the system undergoes an entropy-driven co-crystallization by forming crystals of various symmetries with different helical shapes. In particular, we identify two crystals of helices arranged in the binary honeycomb and square lattices, which are essentially composed by two sets of opposite-handed chiral crystal. Photonic calculations show that these chiral structures can have large complete photonic bandgaps. In addition, in the self-assembled chiral square crystal, we also find dual polarization bandgaps that selectively forbid the propagation of circularly polarized lights of a specific handedness along the helical axis direction. The self-assembly process in our proposed system is robust, suggesting possibilities of using chiral colloids to assemble photonic metamaterials.", "arxiv_id": "1710.04088v2", "research_question": "What are the key factors that determine whether self-assembly of chiral colloidal particles will produce structures with complete photonic bandgaps, and how can those factors be controlled in experiments?"}
{"doi": "https://doi.org/10.1016/j.cja.2023.03.052", "title": "Effects of swirl and hot streak on thermal performances of a high-pressure turbine", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 17, "authors": ["Shuiting Ding", "Tian Qiu", "Peng Liu", "Shenghui ZHANG"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare meson decays into muon pairs constrain theories beyond the Standard Model, and which classes of new-physics models are most strongly affected?"}
{"doi": "https://doi.org/10.3390/toxins15060380", "title": "Bacterial Toxin-Antitoxin Systems’ Cross-Interactions—Implications for Practical Use in Medicine and Biotechnology", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 18, "authors": ["Lidia Boss", "Barbara Kędzierska"], "abstract": "The last decade has seen an explosion in models that describe phenomena in systems medicine. Such models are especially useful for studying signaling pathways, such as the Wnt pathway. In this chapter we use the Wnt pathway to showcase current mathematical and statistical techniques that enable modelers to gain insight into (models of) gene regulation, and generate testable predictions. We introduce a range of modeling frameworks, but focus on ordinary differential equation (ODE) models since they remain the most widely used approach in systems biology and medicine and continue to offer great potential. We present methods for the analysis of a single model, comprising applications of standard dynamical systems approaches such as nondimensionalization, steady state, asymptotic and sensitivity analysis, and more recent statistical and algebraic approaches to compare models with data. We present parameter estimation and model comparison techniques, focusing on Bayesian analysis and coplanarity via algebraic geometry. Our intention is that this (non exhaustive) review may serve as a useful starting point for the analysis of models in systems medicine.", "arxiv_id": "1502.01902v2", "research_question": "What are the main challenges in estimating parameters for ordinary differential equation models in systems medicine, and how can Bayesian approaches help address them?"}
{"doi": "https://doi.org/10.3390/ijms24031941", "title": "Impact of Mephedrone on Fear Memory in Adolescent Rats: Involvement of Matrix Metalloproteinase-9 (MMP-9) and N-Methyl-D-aspartate (NMDA) Receptor", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 9, "authors": ["Ewa Kędzierska", "Joanna Listos", "Karolina Wydra", "Ewa Gibuła‐Tarłowska", "Marta Marszałek‐Grabska", "Jolanta H. Kotlińska", "T Słowik", "Małgorzata Filip", "Paweł Grochecki", "Irena Smaga"], "abstract": "Xenon can produce general anesthesia. Its main protein target is the N-methyl-D-aspartate receptor, a ionotropic channel playing a pivotal role in the function of the central nervous system.\n  The molecular mechanisms allowing this noble gas to have such a specific effect remain obscure, probably as a consequence of the lack of structural data at the atomic level of detail. Herein, as a result of five independent molecular dynamics simulations, three different binding sites were found for xenon in the glycine binding domain of the N-methyl-D-aspartate receptor. The absolute binding free energy of xenon in these sites ranges between -8 and -14 kJ/mole. However, it depends significantly upon the protein conformer chosen for performing the calculation, suggesting that larger values could probably be obtained, if other conformers were considered.\n  These three sites are next to each other, one of them being next to the glycine site. This could explain why the F758W and F758Y mutations can prevent competitive inhibition by xenon without affecting glycine binding.", "arxiv_id": "2203.02219v1", "research_question": "How can an inert, nonpolar atom like xenon selectively bind to and modulate the function of an ionotropic receptor in the central nervous system?"}
{"doi": "https://doi.org/10.3389/fendo.2023.1305378", "title": "Prevalence and predictors of developing vision-threatening diabetic retinopathy within the first three years of type 2 diabetes", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 6, "authors": ["Chuandi Zhou", "Zhi Zheng", "Bo Li", "Qin Zhang", "Ye Chen", "Guosheng Dai", "Shuzhi Zhao", "Dawei Luo", "Chufeng Gu", "Yan Jia"], "abstract": "Diabetic retinopathy (DR) is a diabetes complication that affects eyes. DR is a primary cause of blindness in working-age people and it is estimated that 3 to 4 million people with diabetes are blinded by DR every year worldwide. Early diagnosis have been considered an effective way to mitigate such problem. The ultimate goal of our research is to develop novel machine learning techniques to analyze the DR images generated by the fundus camera for automatically DR diagnosis. In this paper, we focus on identifying small lesions on DR fundus images. The results from our analysis, which include the lesion category and their exact locations in the image, can be used to facilitate the determination of DR severity (indicated by DR stages). Different from traditional object detection for natural images, lesion detection for fundus images have unique challenges. Specifically, the size of a lesion instance is usually very small, compared with the original resolution of the fundus images, making them diffcult to be detected. We analyze the lesion-vs-image scale carefully and propose a large-size feature pyramid network (LFPN) to preserve more image details for mini lesion instance detection. Our method includes an effective region proposal strategy to increase the sensitivity. The experimental results show that our proposed method is superior to the original feature pyramid network (FPN) method and Faster RCNN.", "arxiv_id": "1911.08588v1", "research_question": "What are the main challenges and recommended best practices for developing and validating machine learning systems that reliably detect very small lesions in high-resolution retinal fundus images across different cameras and patient populations?"}
{"doi": "https://doi.org/10.5194/adgeo-59-59-2023", "title": "Sources of Contaminants of Emerging Concern in Groundwater of Barcelona Urban Area", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 6, "authors": ["Laura Scheiber", "Diego Schmidlin", "Rotman Criollo", "Silvia Burdons", "Mónica Enrich", "Marc Teixidó", "Anna Jurado", "Enric Vázquez‐Suñé", "Diana Puigserver"], "abstract": "We present the results of processing the effects of the powerful Gamma Ray Burst GRB221009A captured by the charged particle detectors (electrostatic analyzers and solid-state detectors) onboard spacecraft at different points in the heliosphere on October 9, 2022. To follow the GRB221009A propagation through the heliosphere we used the electron and proton flux measurements from solar missions Solar Orbiter and STEREO-A; Earth magnetosphere and the solar wind missions THEMIS and Wind; meteorological satellites POES15, POES19, MetOp3; and MAVEN - a NASA mission orbiting Mars. GRB221009A had a structure of four bursts: less intense Pulse 1 - the triggering impulse - was detected by gamma-ray observatories at 131659 UT (near the Earth); the most intense Pulses 2 and 3 were detected on board all the spacecraft from the list, and Pulse 4 detected in more than 500 s after Pulse 1. Due to their different scientific objectives, the spacecraft, which data was used in this study, were separated by more than 1 AU (Solar Orbiter and MAVEN). This enabled tracking GRB221009A as it was propagating across the heliosphere. STEREO-A was the first to register Pulse 2 and 3 of the GRB, almost 100 seconds before their detection by spacecraft in the vicinity of Earth. MAVEN detected GRB221009A Pulses 2, 3, and 4 at the orbit of Mars about 237 seconds after their detection near Earth. By processing the time delays observed we show that the source location of the GRB221009A was at RA 288.5 degrees, Dec 18.5 degrees (J2000) with an error cone of 2 degrees", "arxiv_id": "2309.05856v1", "research_question": "How can time-of-arrival differences measured by widely separated spacecraft be used to determine the sky location of a gamma‑ray burst, and what are the main sources of error or uncertainty in that triangulation method?"}
{"doi": "https://doi.org/10.1007/s00330-023-09727-5", "title": "Prediction of Ki-67 expression in gastrointestinal stromal tumors using radiomics of plain and multiphase contrast-enhanced CT", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 22, "authors": ["Li Peng", "Feng Shi", "Weidong Fang", "Ronggui Zhang", "Qing Zhou", "Yun Liu", "ChangYin He", "Chuanming Li", "Yuwei Xia"], "abstract": "Distinguishing gastrointestinal stromal tumors (GISTs) from other intra-abdominal tumors and GISTs molecular analysis is necessary for treatment planning, but challenging due to its rarity. The aim of this study was to evaluate radiomics for distinguishing GISTs from other intra-abdominal tumors, and in GISTs, predict the c-KIT, PDGFRA,BRAF mutational status and mitotic index (MI). All 247 included patients (125 GISTS, 122 non-GISTs) underwent a contrast-enhanced venous phase CT. The GIST vs. non-GIST radiomics model, including imaging, age, sex and location, had a mean area under the curve (AUC) of 0.82. Three radiologists had an AUC of 0.69, 0.76, and 0.84, respectively. The radiomics model had an AUC of 0.52 for c-KIT, 0.56 for c-KIT exon 11, and 0.52 for the MI. Hence, our radiomics model was able to distinguish GIST from non-GISTS with a performance similar to three radiologists, but was not able to predict the c-KIT mutation or MI.", "arxiv_id": "2010.06824v2", "research_question": "Can CT-based radiomics reliably predict tumor-specific genetic mutations and proliferation indices, or is biopsy-based molecular testing still required for accurate treatment planning?"}
{"doi": "https://doi.org/10.1039/d3tc00147d", "title": "Achieving highly efficient bipolar near-ultraviolet emitters <i>via</i> regulating the energy levels of the excited states by a co-acceptor system", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 8, "authors": ["Ling Yue", "Yuling Sun", "Guijiang Zhou", "Xiaolong Yang", "Siqi Liu", "Daokun Zhong", "Xuming Deng", "Feng Zhao", "Xi Chen"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "Why are measurements of rare neutral B-meson decays to muon pairs considered sensitive probes of physics beyond the Standard Model, and what types of new-physics scenarios can such measurements constrain?"}
{"doi": "https://doi.org/10.1016/j.matpr.2023.03.226", "title": "Smart materials – A state-of-the-art-review", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 10, "authors": ["Shubham Nandu Ovhal", "Vaibhav Patil", "Nidhi Jain", "Kaveti Nani Kartik"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B meson decays to muon pairs constrain or rule out extensions to the Standard Model?"}
{"doi": "https://doi.org/10.1186/s12955-023-02115-z", "title": "Experience-based health state valuation using the EQ VAS: a register-based study of the EQ-5D-3L among nine patient groups in Sweden", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 20, "authors": ["Mikael Landén", "Allan Abbott", "Nancy Devlin", "Kristina Burström", "Malin Regardt", "Annette W‐Dahl", "Magnus Forssblad", "Marcus Schmitt‐Egenolf", "Fitsum Sebsibe Teni", "Johanna Vinblad", "Ola Rolfson", "Magnus Ekström", "Peter Fritzell", "Åsa Jönsson", "Michael Möller", "Björn E. Rosengren", "David Parkin"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of the rare decays of neutral B mesons into muon pairs test the Standard Model and constrain possible new physics scenarios?"}
{"doi": "https://doi.org/10.1108/whatt-03-2023-0054", "title": "Sailing the tide of over consumption: applying a business history approach to explore the rising demand of luxury yachts and travel since 1979", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 6, "authors": ["Duncan Philip Connors"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B meson decays into muon pairs constrain physics beyond the Standard Model, and which types of new-physics scenarios are most strongly affected by these constraints?"}
{"doi": "https://doi.org/10.3847/1538-4357/acb13d", "title": "A Multiwavelength Study of Active Galactic Nuclei in Post-merger Remnants", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 20, "authors": ["Russell E. Ryan", "Shobita Satyapal", "Karen L. Masters", "Amy Jones", "Niv Drory", "Jimmy A. Irwin", "Kavya Mukundan", "William C. Keel", "Sara L. Ellison", "Preethi Nair", "Wenhao Li", "David V. Stark"], "abstract": "Observations give strong support for the unification scheme of active galactic nuclei. The scheme is premised on toroidal obscuration of the central engine by dusty clouds that are individually very optically thick. These lectures summarize the torus properties, describe the handling and implications of its clumpy nature and present speculations about its dynamic origin.", "arxiv_id": "0805.3699v1", "research_question": "How does a clumpy dusty torus affect the observed spectra, polarization, and variability of active galactic nuclei compared with a smooth, continuous torus model?"}
{"doi": "https://doi.org/10.14450/2318-9312.v35.e1.a2023.pp64-75", "title": "PERFIL E COMPETÊNCIAS ADQUIRIDAS POR EGRESSOS DE UM CURSO DE FARMÁCIA, NO BRASIL", "language": "pt", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 14, "authors": ["Aylma Lima Carneiro", "Gizelly Braga Pires", "Bruno Rodrigues Alencar", "Tatiane de Oliveira Silva Alencar"], "abstract": "An angular analysis of $B^0\\rightarrow K^{*0}e^{+}e^{-}$ decays is presented using proton-proton collision data collected by the LHCb experiment at centre-of-mass energies of 7, 8 and 13 TeV, corresponding to an integrated luminosity of 9 fb$^{-1}$. The analysis is performed in the region of the dilepton invariant mass squared of 1.1-6.0 GeV$^{2}/c^{4}$. In addition, a test of lepton flavour universality is performed by comparing the obtained angular observables with those measured in $B^0\\rightarrow K^{*0}μ^{+}μ^{-}$ decays. In general, the angular observables are found to be consistent with the Standard Model expectations as well as with global analyses of other $b \\rightarrow s \\ell^{+} \\ell^{-}$ processes, where $\\ell$ is either a muon or an electron. No sign of lepton-flavour-violating effects is observed.", "arxiv_id": "2502.10291v2", "research_question": "How do angular analyses of rare B→K*ℓ+ℓ− decays test lepton flavor universality and what kinds of new-physics effects can they help reveal or constrain?"}
{"doi": "https://doi.org/10.1002/sce.21810", "title": "Learning by doing: A multi‐level analysis of the impact of citizen science education", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 15, "authors": ["Elizabeth Y. Zhang", "Thea V. Kristensen", "Calista Hundley", "Zachary L. Watson", "Sarah Bunnell", "Fariya Farah"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B-meson decays to muon pairs constrain theories beyond the Standard Model, and which classes of new-physics models are most strongly affected?"}
{"doi": "https://doi.org/10.1186/s12906-023-04081-x", "title": "Cardioprotective effects of Schisantherin A against isoproterenol-induced acute myocardial infarction through amelioration of oxidative stress and inflammation via modulation of PI3K-AKT/Nrf2/ARE and TLR4/MAPK/NF-κB pathways in rats", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 20, "authors": ["Ren Yun-xia", "Zheng Xu", "Jinfang Cheng", "Zhijun Zhang", "Kaiyi Zhu", "Xiaolong Mi"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B meson decays into muon pairs constrain extensions of the Standard Model, and which classes of new-physics models are most strongly affected by these results?"}
{"doi": "https://doi.org/10.1016/j.landurbplan.2023.104893", "title": "Ecosystem engineers enter the city: Habitat characteristics influencing the distribution of Eurasian beavers Castor fiber in a human-transformed landscape", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 7, "authors": ["Dariusz Wrazidło", "Michał Ciach", "Izabela Fedyń"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare neutral B meson decays into muon pairs constrain theories beyond the Standard Model, and why are these particular decays so sensitive to new physics?"}
{"doi": "https://doi.org/10.21512/tw.v23i2.8140", "title": "Role of Threshold of Free Shipping Promotion and Product Type on Impulsive Buying Behaviour in E-Commerce Platform", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 6, "authors": ["Nila Armelia Windasari", "Readdy Aria Yendola"], "abstract": "This report details our methodology and results developed for the Multilingual E-commerce Search Competition. The problem aims to recognize relevance between user queries versus product items in a multilingual context and improve recommendation performance on e-commerce platforms. Utilizing Large Language Models (LLMs) and their capabilities in other tasks, our data-centric method achieved the highest score compared to other solutions during the competition. Final leaderboard is publised at https://alibaba-international-cikm2025.github.io. The source code for our project is published at https://github.com/nhtlongcs/e-commerce-product-search.", "arxiv_id": "2510.25428v1", "research_question": "What are effective strategies for improving relevance matching between multilingual user queries and product listings on e-commerce platforms, especially when labeled data is limited?"}
{"doi": "https://doi.org/10.1021/acssuschemeng.3c01970", "title": "Process Mechanism for Production of Green Lixiviant Thiosulfate at Atmospheric Pressure", "language": "en", "created_date": "2023-07-04T00:00:00", "publication_year": 2023, "cited_by_count": 11, "authors": ["Lin Wang", "Yongbin Yang", "Tao Jiang", "Ke Li", "Yan Zhang", "Yang Ou", "Qian Li", "Wei Gao"], "abstract": "We discuss the use of commercial high-power light emitting diodes (LEDs) as a light source for fluorescence pressure measurements. A relatively broad light emitting spectra of single color LEDs (in comparison with lasers) do not prevent producing narrow fluorescence lines at least for two widely used pressure indicator materials, namely ruby (Cr$^{3+}$:Al$_2$O$_3$) and strontium tetraborate (Sm$^{2+}$:SrB$_4$O$_7$). Strongest responses of both pressure indicators were detected for the green color LEDs with the average wavelength $λ_{\\rm av}\\sim 530$ nm. LEDs might be easily implemented for producing fiber coupled, as well as the parallel light sources. LEDs were found to be efficient to replace laser sources in piston-cylinder cell and diamond anvil cell fluorescence pressure measurement setups.", "arxiv_id": "2305.08243v1", "research_question": "What are the main advantages and limitations of using high-power LEDs instead of lasers for fluorescence-based pressure measurements in high‑pressure cells?"}
{"doi": "https://doi.org/10.1016/j.lwt.2023.115273", "title": "Dynamic changes in the major chemical and volatile components during the “Ziyan” tea wine processing", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 21, "authors": ["Xiaoxuan Wang", "Keke Li", "Wei Xu", "Yang Yang", "Wenbao Jia", "Yiqiao Zhao", "Yao Zou", "Tunyaluk Bouphun", "Binhan Wen", "Yajie Hua", "Siyu Liao", "Qian Tang"], "abstract": "On 2017 August 17 a binary neutron star coalescence candidate (later designated GW170817) with merger time 12:41:04 UTC was observed through gravitational waves by the Advanced LIGO and Advanced Virgo detectors. The Fermi Gamma-ray Burst Monitor independently detected a gamma-ray burst (GRB 170817A) with a time delay of $\\sim$1.7 s with respect to the merger time. From the gravitational-wave signal, the source was initially localized to a sky region of 31 deg$^2$ at a luminosity distance of $40^{+8}_{-8}$ Mpc and with component masses consistent with neutron stars. The component masses were later measured to be in the range 0.86 to 2.26 Msun. An extensive observing campaign was launched across the electromagnetic spectrum leading to the discovery of a bright optical transient (SSS17a, now with the IAU identification of AT 2017gfo) in NGC 4993 (at $\\sim$40 Mpc) less than 11 hours after the merger by the One-Meter, Two Hemisphere (1M2H) team using the 1 m Swope Telescope. The optical transient was independently detected by multiple teams within an hour. Subsequent observations targeted the object and its environment. Early ultraviolet observations revealed a blue transient that faded within 48 hours. Optical and infrared observations showed a redward evolution over $\\sim$10 days. Following early non-detections, X-ray and radio emission were discovered at the transient's position $\\sim$9 and $\\sim$16 days, respectively, after the merger. Both the X-ray and radio emission likely arise from a physical process that is distinct from the one that generates the UV/optical/near-infrared emission. No ultra-high-energy gamma-rays and no neutrino candidates consistent with the source were found in follow-up searches. (Abridged)", "arxiv_id": "1710.05833v2", "research_question": "How do the different electromagnetic signals (prompt gamma-rays, early blue optical/UV emission, later red optical/IR evolution, and delayed X-ray/radio emission) arise from a neutron star merger, and what does each component tell us about the ejecta composition, outflow geometry, and remnant?"}
{"doi": "https://doi.org/10.1021/acsami.3c00047", "title": "Impact of π-Expanded Boron-Carbonyl Hybrid Acceptors on TADF Properties: Controlling Local Triplet Excited States and Unusual Emission Tuning", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 6, "authors": ["Chaerin Kim", "Taehwan Lee", "Jeong‐Hwan Lee", "Young Hoon Lee", "Min Hyung Lee", "Ina Nur Istiqomah", "Jaehoon Jung", "Jee‐Hun Jang"], "abstract": "Organic light emitting diodes (OLEDs) based on thermally activated delayed fluorescence (TADF) utilize molecular systems with a small energy splitting between singlet and triplet states. This can either be realized in intramolecular charge transfer states of molecules with near-orthogonal donor and acceptor moieties or in intermolecular exciplex states formed between a suitable combination of individual donor and acceptor materials. Here, we investigate 4,4'-(9H,9'H-[3,3'-bicarbazole]-9,9'-diyl)bis(3-(trifluoromethyl) benzonitrile) (pCNBCzoCF3), which shows intramolecular TADF but can also form exciplex states in combination with 4,4',4''-tris[phenyl(m-tolyl)amino]triphenylamine (m-MTDATA). Orange emitting exciplex-based OLEDs additionally generate a sky-blue emission from the intramolecular emitter with an intensity that can be voltage-controlled. We apply electroluminescence detected magnetic resonance (ELDMR) to study the thermally activated spin-dependent triplet to singlet up-conversion in operating devices. Thereby, we can investigate intermediate excited states involved in OLED operation and derive the corresponding activation energy for both, intra- and intermolecular based TADF. Furthermore, we give a lower estimate for the extent of the triplet wavefunction to be >1.2 nm. Photoluminescence detected magnetic resonance (PLDMR) reveals the population of molecular triplets in optically excited thin films. Overall, our findings allow us to draw a comprehensive picture of the spin-dependent emission from intra- and intermolecular TADF OLEDs.", "arxiv_id": "2006.15653v2", "research_question": "How can magnetic resonance techniques (such as electroluminescence-detected and photoluminescence-detected magnetic resonance) be used to distinguish and characterize intra- versus intermolecular thermally activated delayed fluorescence processes in operating OLEDs?"}
{"doi": "https://doi.org/10.1021/acsapm.3c01627", "title": "Robust Self-Healable and Three-Dimensional Printable Thermoplastic Elastomeric Waterborne Polyurethane for Artificial Muscle and Biomedical Scaffold Applications", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 11, "authors": ["Niranjan Karak", "Jay Hind Rajput", "Atharva A. Poundarik", "Biman B. Mandal", "Samiran Morang", "Ashutosh Bandyopadhyay"], "abstract": "The advent of dedicated Deep Learning (DL) accelerators and neuromorphic processors has brought on new opportunities for applying both Deep and Spiking Neural Network (SNN) algorithms to healthcare and biomedical applications at the edge. This can facilitate the advancement of medical Internet of Things (IoT) systems and Point of Care (PoC) devices. In this paper, we provide a tutorial describing how various technologies including emerging memristive devices, Field Programmable Gate Arrays (FPGAs), and Complementary Metal Oxide Semiconductor (CMOS) can be used to develop efficient DL accelerators to solve a wide variety of diagnostic, pattern recognition, and signal processing problems in healthcare. Furthermore, we explore how spiking neuromorphic processors can complement their DL counterparts for processing biomedical signals. The tutorial is augmented with case studies of the vast literature on neural network and neuromorphic hardware as applied to the healthcare domain. We benchmark various hardware platforms by performing a sensor fusion signal processing task combining electromyography (EMG) signals with computer vision. Comparisons are made between dedicated neuromorphic processors and embedded AI accelerators in terms of inference latency and energy. Finally, we provide our analysis of the field and share a perspective on the advantages, disadvantages, challenges, and opportunities that various accelerators and neuromorphic processors introduce to healthcare and biomedical domains.", "arxiv_id": "2007.05657v2", "research_question": "What are the main trade-offs between using neuromorphic processors and conventional deep learning accelerators for real-time biomedical signal processing on edge/point-of-care devices?"}
{"doi": "https://doi.org/10.4018/ijsi.319314", "title": "Road Rage and Aggressive Driving Behaviour Detection in Usage-Based Insurance Using Machine Learning", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 12, "authors": ["Subramanian Arumugam", "R. Bhargavi"], "abstract": "Data science has become increasingly essential for the production of official statistics, as it enables the automated collection, processing, and analysis of large amounts of data. With such data science practices in place, it enables more timely, more insightful and more flexible reporting. However, the quality and integrity of data-science-driven statistics rely on the accuracy and reliability of the data sources and the machine learning techniques that support them. In particular, changes in data sources are inevitable to occur and pose significant risks that are crucial to address in the context of machine learning for official statistics.\n  This paper gives an overview of the main risks, liabilities, and uncertainties associated with changing data sources in the context of machine learning for official statistics. We provide a checklist of the most prevalent origins and causes of changing data sources; not only on a technical level but also regarding ownership, ethics, regulation, and public perception. Next, we highlight the repercussions of changing data sources on statistical reporting. These include technical effects such as concept drift, bias, availability, validity, accuracy and completeness, but also the neutrality and potential discontinuation of the statistical offering. We offer a few important precautionary measures, such as enhancing robustness in both data sourcing and statistical techniques, and thorough monitoring. In doing so, machine learning-based official statistics can maintain integrity, reliability, consistency, and relevance in policy-making, decision-making, and public discourse.", "arxiv_id": "2306.04338v1", "research_question": "What practical steps can organizations that use machine learning for official statistics take to detect, monitor, and mitigate the impacts of changing data sources on the validity and reliability of their outputs?"}
{"doi": "https://doi.org/10.1007/s00431-023-04837-0", "title": "Hydroxychloroquine in children with proliferative lupus nephritis: a randomized clinical trial", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 16, "authors": ["Hend Hassan Abdelnabi", "Shymaa Mohamed Elrifaey", "Fatma Sayed Gheet", "Waleed Ahmed El-Shahaby", "Heba Dawoud"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of extremely rare particle decays into muon pairs constrain extensions of the Standard Model, and which classes of new-physics models are most strongly affected by such measurements?"}
{"doi": "https://doi.org/10.1016/j.eiar.2022.107029", "title": "Evaluating citizens' satisfaction on the urban environmental management through a multi-criteria approach: An application experience in Sicily", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 12, "authors": ["Salvatore Corrente", "Carlo Ingrao", "Agata Matarazzo", "Antonio Punzo"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of the branching fractions for rare decays of neutral B mesons into muon pairs constrain or rule out extensions of the Standard Model, and which classes of new-physics models are most affected?"}
{"doi": "https://doi.org/10.1016/j.jobe.2023.107429", "title": "A numerical framework for the ITZ percolation, effective fraction and diffusivity of concrete systems considering the nonuniform ITZ", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 10, "authors": ["Mingqi Li", "Jianjun Lin", "Lili Yuan", "Huisu Chen", "Qingxin Zhao"], "abstract": "On 2017 August 17 a binary neutron star coalescence candidate (later designated GW170817) with merger time 12:41:04 UTC was observed through gravitational waves by the Advanced LIGO and Advanced Virgo detectors. The Fermi Gamma-ray Burst Monitor independently detected a gamma-ray burst (GRB 170817A) with a time delay of $\\sim$1.7 s with respect to the merger time. From the gravitational-wave signal, the source was initially localized to a sky region of 31 deg$^2$ at a luminosity distance of $40^{+8}_{-8}$ Mpc and with component masses consistent with neutron stars. The component masses were later measured to be in the range 0.86 to 2.26 Msun. An extensive observing campaign was launched across the electromagnetic spectrum leading to the discovery of a bright optical transient (SSS17a, now with the IAU identification of AT 2017gfo) in NGC 4993 (at $\\sim$40 Mpc) less than 11 hours after the merger by the One-Meter, Two Hemisphere (1M2H) team using the 1 m Swope Telescope. The optical transient was independently detected by multiple teams within an hour. Subsequent observations targeted the object and its environment. Early ultraviolet observations revealed a blue transient that faded within 48 hours. Optical and infrared observations showed a redward evolution over $\\sim$10 days. Following early non-detections, X-ray and radio emission were discovered at the transient's position $\\sim$9 and $\\sim$16 days, respectively, after the merger. Both the X-ray and radio emission likely arise from a physical process that is distinct from the one that generates the UV/optical/near-infrared emission. No ultra-high-energy gamma-rays and no neutrino candidates consistent with the source were found in follow-up searches. (Abridged)", "arxiv_id": "1710.05833v2", "research_question": "How do neutron star mergers produce emissions across such different wavelengths (gamma-rays, optical/infrared, X-rays, radio), and why do these signals appear at different times after the merger?"}
{"doi": "https://doi.org/10.1016/j.enconman.2023.117942", "title": "Cascading latent heat thermal energy storage in parabolic trough solar collector as a promising solution: An experimental investigation", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 25, "authors": ["Vahid Piroozmand", "Rouhollah Ahmadi", "Kourosh Bagherzadeh"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B meson decays into muon pairs constrain physics beyond the Standard Model, and which classes of new-physics models are most strongly affected?"}
{"doi": "https://doi.org/10.2174/18744478-v17-e230120-2022-8", "title": "Parametric Study on the Influence of Pedestrians' Road Crossing Pattern on Safety", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 6, "authors": ["Sathya Prakash", "Krishnamurthy Karuppanagounder"], "abstract": "A novel head-mounted virtual immersive/interactive reality environment (VIRE) is utilized to evaluate the behaviour of participants in three pedestrian road crossing conditions while 1) not distracted, 2) distracted with a smartphone, and 3) distracted with a smartphone with a virtually implemented safety measure on the road. Forty-two volunteers participated in our research who completed thirty successful (complete crossing) trials in blocks of ten trials for each crossing condition. For the two distracted conditions, pedestrians are engaged in a maze-solving game on a virtual smartphone, while at the same time checking the traffic for a safe crossing gap. For the proposed safety measure, smart flashing and color changing LED lights are simulated on the crosswalk to warn the distracted pedestrian who initiates crossing. Surrogate safety measures as well as speed information and distraction attributes such as direction and orientation of participant's head were collected and evaluated by employing a Multinomial Logit (MNL) model. Results from the model indicate that females have more dangerous crossing behaviour especially in distracted conditions; however, the smart LED treatment reduces this negative impact. Moreover, the number of times and the percentage of duration the head was facing the smartphone during a trial and a waiting time respectively increase the possibility of unsafe crossings; though, the proposed treatment reduces the safety crossing rate. Hence, our study shows that the smart LED light safety treatment indeed improves the safety of distracted pedestrians and enhances the successful crossing rate.", "arxiv_id": "1806.06454v1", "research_question": "How effective are smart, flashing or color-changing LED crosswalk lights at reducing smartphone-related pedestrian distraction in real-world street environments, and what factors influence their success?"}
{"doi": "https://doi.org/10.1088/2631-8695/acebb8", "title": "On the devolvement of fractal antenna for IoT applications", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 12, "authors": ["Arpit Khandelwal", "Kapil Shrivastava", "Basudha Dewan", "Ashwini Kumar"], "abstract": "Intelligence Everywhere is predicated on the seamless integration of IoT networks transporting a vast amount of data streams through many computing resources across an edge-to-cloud continuum, relying on the orchestration of distributed machine learning models. The result is an interconnected and collective intelligent ecosystem where devices, systems, services, and users work together to support IoT applications. This paper discusses the state-of-the-art research and the principles of the Intelligence Everywhere framework for enhancing IoT applications in vertical sectors such as Digital Health, Infrastructure, and Transportation/Mobility in the context of intelligent society (Society 5.0). It also introduces a novel perspective for the development of horizontal IoT applications, capable of running across various IoT networks while fostering collective intelligence across diverse sectors. Finally, this paper provides comprehensive insights into the challenges and opportunities for harnessing collective knowledge from real-time insights, leading to optimised processes and better overall collaboration across different IoT sectors.", "arxiv_id": "2310.00346v1", "research_question": "What are the main technical and organizational challenges to achieving interoperability and collective intelligence across heterogeneous IoT networks, and what strategies can be used to address them?"}
{"doi": "https://doi.org/10.3390/en16134994", "title": "Microfluidic Studies on Minimum Miscibility Pressure for n-Decane and CO2", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 12, "authors": ["Vladislav Krutko", "Michael Tarkhov", "Desmond Batsa Dorhjie", "Аlexey Cheremisin", "Alexandre I. Rykov", "Е.В. Зенова", "Evgeny Shilov", "Ivan Filippov", "Dmitrii Pereponov"], "abstract": "We discuss the use of commercial high-power light emitting diodes (LEDs) as a light source for fluorescence pressure measurements. A relatively broad light emitting spectra of single color LEDs (in comparison with lasers) do not prevent producing narrow fluorescence lines at least for two widely used pressure indicator materials, namely ruby (Cr$^{3+}$:Al$_2$O$_3$) and strontium tetraborate (Sm$^{2+}$:SrB$_4$O$_7$). Strongest responses of both pressure indicators were detected for the green color LEDs with the average wavelength $λ_{\\rm av}\\sim 530$ nm. LEDs might be easily implemented for producing fiber coupled, as well as the parallel light sources. LEDs were found to be efficient to replace laser sources in piston-cylinder cell and diamond anvil cell fluorescence pressure measurement setups.", "arxiv_id": "2305.08243v1", "research_question": "What are the practical advantages and limitations of using high-power LEDs instead of lasers for fluorescence-based pressure measurements in high-pressure apparatus like diamond anvil cells?"}
{"doi": "https://doi.org/10.1016/j.cjtee.2023.05.002", "title": "Advance in hyperbaric oxygen therapy in spinal cord injury", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 10, "authors": ["Antonio Romanelli", "C. Ferraiuoli", "Salvatore Palmese", "Antonio Siglioccolo", "Alessandro Calicchio", "Renato Gammaldi", "Alessio Galardo", "Vittorio Caterino", "Veronica Vicinanza"], "abstract": "Background: Spinal cord injury triggers complex pathological cascades, resulting in destructive tissue damage and incomplete tissue repair. Scar formation is generally considered as a barrier for regeneration in central nervous system (CNS), while the intrinsic mechanism of scar-forming after spinal cord injury has not been completed deciphered. Methods: We assessed cholesterol hemostasis in spinal cord lesions and injured peripheral nerves using confocal reflection microscopy and real-time PCR analyses. The involvement of the proteins, which were predicted to promote cholesterol efflux in spinal cord lesions, were assessed with Liver X receptor (LXR) agonist and Apolipoprotein E (APOE) deficiency. The role of reverse cholesterol transport (RCT) in cholesterol clearance was examined in APOE KO mice injured sciatic nerves and myelin-overloaded macrophages in vitro. Finally, we determined the consequence of excess cholesterol accumulation in CNS by transplantation of myelin into neonatal spinal cord lesions. Results: We found that excess cholesterol accumulates in phagocytes and is inefficiently removed in spinal cord lesions in young-adult mice. Interestingly, we observed that excessive cholesterol also accumulates in injured peripheral nerves, but is subsequently removed by RCT. Meanwhile, preventing RCT led to macrophage accumulation and fibrosis in injured peripheral nerves. Furthermore, the neonatal mouse spinal cord lesions are devoid of myelin-derived lipids, and able to heal without excess cholesterol accumulation. We found that transplantation of myelin into neonatal lesions disrupts healing with excessive cholesterol accumulation, persistent macrophage activation and fibrosis, indicating myelin-derived cholesterol plays a critical role in impaired wound healing.", "arxiv_id": "2209.09700v1", "research_question": "How does cholesterol accumulation within phagocytes affect inflammation and scar formation after nervous system injury, and what strategies could be used to promote cholesterol clearance to improve healing?"}
{"doi": "https://doi.org/10.1007/s00604-023-05812-0", "title": "Copper nanoparticle-decorated nitrogen-doped carbon nanosheets for electrochemical determination of paraquat", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 9, "authors": ["Xue Zhao", "Thomas Wågberg", "Sam Toan", "Zongshan Zhao", "Lei Wang", "Zhou Jie", "Guangzhi Hu"], "abstract": "Here, we report the synthesis of hard carbon materials(CSH) made from corn starch and their application as an anode in lithium-ion batteries. The study shows that the Microstructure and electrochemical properties of CSHs are affected by nitrogen doping. It is found that nitrogen is embedded in the carbon layer with graphite nitrogen, pyridine nitrogen, and pyrrole nitrogen, so as to the surface morphology was changed and reduced the disorder of the materials. The electrochemical test results show that the introduction of nitrogen elements can increase the reversible capacity of the material, with the first discharge capacity reaching above 426.35 mAh g-1, and the rate performance also improves. When triethylenetetramine and pre-carbonized corn starch are carbonized at a mass ratio of 1:9, the obtained material has a reversible capacity of 122.04 mAh g-1 at a rate of 2 C. During the carbonization process, the nitrogen in triethylenetetramine is doped into the carbon materials, improving the electrochemical performance of the material. Keywords: Lithium-ion battery; Hard carbon; Corn starch; Nitrogen doping;", "arxiv_id": "2407.20255v1", "research_question": "How does nitrogen doping alter the structure and lithium-storage mechanisms of biomass-derived hard carbon anodes, and why does that typically improve their capacity and rate performance?"}
{"doi": "https://doi.org/10.3390/su151410811", "title": "The Role of Managers in Corporate Change Management: A Bibliometric Review", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 8, "authors": ["Ángel Gutiérrez-Iñiguez", "Jesús Collado Agudo", "Josep Rialp Criado"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B meson decays into muon pairs constrain theories beyond the Standard Model, and which types of new-physics scenarios are most strongly affected?"}
{"doi": "https://doi.org/10.1016/j.colsurfa.2023.133096", "title": "Photocatalytic hydrogen production and simultaneous tetracycline degradation by selectively depositing growth of MoS2 on the PbTiO3 (00<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" altimg=\"si0001.svg\"><mml:mover accent=\"true\"><mml:mrow><mml:mn mathvariant=\"bold-italic\">1</mml:mn></mml:mrow><mml:mo></mml:mo></mml:mover></mml:math>) surface", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 7, "authors": ["INAAM ULLAH", "Shaolin Xue", "Hange Feng", "Tiantian Yang", "Lingwei Li", "Zibo Dong", "Zhaoyang Wang"], "abstract": "Studies of muonic hydrogen atoms and molecules have been performed traditionally in bulk targets of gas, liquid or solid. At TRIUMF, Canada's meson facility, we have developed a new type of target system using multilayer thin films of solid hydrogen, which provides a beam of muonic hydrogen atoms in vacuum. Using the time-of-flight of the muonic atoms, the energy-dependent information of muonic reactions are obtained in direct manner. We discuss some unique measurements enabled by the new technique, with emphasis on processes relevant to muon catalyzed fusion.", "arxiv_id": "0101007v1", "research_question": "What are the advantages and potential challenges of studying muonic hydrogen atoms and their reactions in vacuum compared with using bulk gas, liquid, or solid targets?"}
{"doi": "https://doi.org/10.4269/ajtmh.22-0717", "title": "Co-Infection between Dengue Virus and SARS-CoV-2 in Cali, Colombia", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 7, "authors": ["Luis Gabriel Parra‐Lara", "Olga Lucía Agudelo Rojas", "Alejandro Tejada Vega", "Fernando Rosso", "Sebastián Robles-Castillo", "Isabel L Zapata-Vasquez", "Julio Llanos-Torres", "David Esteban Rebellón-Sánchez", "Sarita Rodríguez"], "abstract": "In the present study, we have described how by using molecular docking and molecular dynamic (MD) simulation studies the combination drug of ivermectin and doxycycline can be used as a potential inhibitor for SARS-CoV-2 virus. In lieu of unavailability of specific cure of COVID-19 till now various possibilities for individual and combination drugs have been explored by the medical practitioners/scientists for the remedial purpose of CoV-2 infections. $3CL^{pro}$ is the main protease of SARS-CoV-2 virus which plays an essential role in mediating viral replication in the human body. $3CL^{pro}$ protein can serve as an attractive drug target. In this work, we have studied drug: $3CL^{pro}$ interactions by in silico molecular docking and MD simulation approaches. Common and easily available antiviral drugs ivermectin, doxycycline and their combination have been proved their valid candidature to be used as potential drug candidates against SARS-CoV-2 infections.", "arxiv_id": "2012.00653v1", "research_question": "What experimental and clinical steps are required to validate a candidate drug or drug combination that is predicted to inhibit a viral protease before it can be recommended for patient treatment?"}
{"doi": "https://doi.org/10.1038/s41559-023-02232-4", "title": "Chase-away evolution maintains imperfect mimicry in a brood parasite–host system despite rapid evolution of mimics", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 9, "authors": ["Kuan-Chi Chen", "Lazaro Hamusikili", "Claire N. Spottiswoode", "J. F. R. Colebrook-Robjent", "Tanmay Dixit", "Anthony J. C. Fulford", "William E. Feeney", "Andrei L. Apostol", "Jess Lund", "Wenfei Tong", "Christopher Town"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of branching fractions for rare decays like B_s→μ+μ− and B^0→μ+μ− constrain physics beyond the Standard Model, and which types of new-physics scenarios are most strongly affected?"}
{"doi": "https://doi.org/10.3390/w15213817", "title": "Municipal Solid Waste Fly Ash-Derived Zeolites as Adsorbents for the Recovery of Nutrients and Heavy Metals—A Review", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 9, "authors": ["Muhammad Umar", "Christian Vogelsang"], "abstract": "Waste pollution is one of the most significant environmental issues in the modern world. The importance of recycling is well known, either for economic or ecological reasons, and the industry demands high efficiency. Our team conducted comprehensive research on Artificial Intelligence usage in waste detection and classification to fight the world's waste pollution problem. As a result an open-source framework that enables the detection and classification of litter was developed. The final pipeline consists of two neural networks: one that detects litter and a second responsible for litter classification. Waste is classified into seven categories: bio, glass, metal and plastic, non-recyclable, other, paper and unknown. Our approach achieves up to 70% of average precision in waste detection and around 75% of classification accuracy on the test dataset. The code used in the studies is publicly available online.", "arxiv_id": "2105.06808v1", "research_question": "What are the main challenges and practical considerations when deploying AI-based systems for detecting and classifying litter in real-world outdoor environments, and how can their accuracy and robustness be improved?"}
{"doi": "https://doi.org/10.1016/j.heliyon.2023.e13263", "title": "Testosterone and persistent organic pollutants in East Greenland male polar bears (Ursus maritimus)", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 7, "authors": ["Christian Sonne", "Bjarne Styrishave", "Daniel J. Hitchcock", "Gro D. Villanger", "Robert J. Letcher", "Runé Dietz", "Tomasz Maciej Ciesielski", "Bjørn Munro Jenssen", "A. Smette"], "abstract": "POLAR is a dedicated Gamma-Ray Burst polarimeter making use of Compton-scattering which took data from the second Chinese spacelab, the Tiangong-2 from September 2016 to April 2017. It has a wide Field of View of $\\sim6$ steradians and an effective area of $\\sim400\\ cm^2$ at 300 keV. These features make it one of the most sensitive instruments in its energy range (15-500 keV), and therefore capable of almost continuously monitoring persistent sources such as pulsars. Significant folded pulsation from both PSR B0531+21 (the Crab Pulsar) and PSR B1509-58 has been observed. Observations of the Crab Pulsar with POLAR have previously been used for phase-resolved spectroscopy of the Crab Pulsar to calibrate the instrumental responses of POLAR. In this work, we investigate a polarimetric joint-fitting method for observations of the Crab Pulsar with POLAR. Unlike a GRB observation with POLAR, the observations of the Crab Pulsar are complicated by multiple observational datasets during which the polarization plane rotates as well. So before fitting, we have to correct the modulation curves under different datasets, by taking into account the rotations of the Crab Pulsar's relative position in the detctor's local coordinate, and the changes of detector response in different datasets. Despite these difficulties and the low signal to background for such sources constraining, polarization measurements were possible with the POLAR data. We will present the methodology briefly, which could be applied to any wide FoV polarimeter, and polarization results of the Crab pulsar with POLAR. Finally, the inferred ability of pulsar detection with POLAR-2 (the successor of POLAR) will also be discussed.", "arxiv_id": "2109.03142v1", "research_question": "What are the main challenges in performing phase-resolved hard X-ray/gamma-ray polarimetry of pulsars with wide-field Compton-scattering polarimeters, and what general techniques are used to correct for detector rotation and time-varying instrument response?"}
{"doi": "https://doi.org/10.5281/zenodo.7588256", "title": "Compound flood risk analysis in CONUS", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 8, "authors": ["Dongyu Feng"], "abstract": "Flood risk managers seek to optimise Blue-Green Infrastructure (BGI) designs to maximise return on investment. Current systems often use optimisation algorithms and detailed flood models to maximise benefit-cost ratios for single rainstorm return periods. However, these schemes may lack robustness in mitigating flood risks across different storm magnitudes. For example, a BGI scheme optimised for a 100-year return period may differ from one optimised for a 10-year return period. This study introduces a novel methodology incorporating five return periods (T = 10, 20, 30, 50, and 100 years) into a multi-objective BGI optimisation framework. The framework combines a Non-dominated Sorting Genetic Algorithm II (NSGA-II) with a fully distributed hydrodynamic model to optimise the spatial placement and combined size of BGI features. For the first time, direct damage cost (DDC) and expected annual damage (EAD), calculated for various building types, are used as risk objective functions, transforming a many-objective problem into a multi-objective one. Performance metrics such as Median Risk Difference (MedRD), Maximum Risk Difference (MaxRD), and Area Under Pareto Front (AUPF) reveal that a 100-year optimised BGI design performs poorly when evaluated for other return periods, particularly shorter ones. In contrast, a BGI design optimised using composite return periods enhances performance metrics across all return periods, with the greatest improvements observed in MedRD (22%) and AUPF (73%) for the 20-year return period, and MaxRD (23%) for the 50-year return period. Furthermore, climate uplift stress testing confirms the robustness of the proposed design to future rainfall extremes. This study advocates a paradigm shift in flood risk management, moving from single maximum to multiple rainstorm return period-based designs to enhance resilience and adaptability to future climate extremes.", "arxiv_id": "2502.12174v2", "research_question": "How should decision-makers choose which storm return periods to include and how to weight them when optimising blue–green infrastructure design under limited budget and uncertainty about future climate change?"}
{"doi": "https://doi.org/10.1016/j.cclet.2023.109131", "title": "Enhanced interfacial charge transfer on Bi metal@defective Bi2Sn2O7 quantum dots towards improved full-spectrum photocatalysis: A combined experimental and theoretical investigation", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 15, "authors": ["Minghua Zhou", "Zhongzheng Hu", "Ruiheng Liang", "Ge Song", "Huizhong Wu", "Xuyang Zhang"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "Why are decays of neutral B mesons into muon pairs so rare, and how do precise measurements of their branching fractions constrain or rule out theories beyond the Standard Model?"}
{"doi": "https://doi.org/10.1002/soej.12660", "title": "<scp>AACSB</scp> accreditation and student demand", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 6, "authors": ["Marisa Cameron", "Katherine Starr", "Bryan C. McCannon"], "abstract": "A classical problem in power systems is to allocate in-coming (elastic or inelastic) demands without violating the operating constraints of electric networks in an online fashion. Although online decision problems have been well-studied in the literature, a unique challenge arising in power systems is the presence of non-linear constraints, a departure from the traditional settings. A particular example is the capacity constraint of apparent power, which gives rise to a quadratic constraint, rather than typical linear constraints. In this paper, we present a competitive randomized online algorithm for deciding whether a sequence of inelastic demands can be allocated for the requested intervals, subject to the total satisfiable apparent power within a time-varying capacity constraint. We also consider an alternative setting with nodal voltage constraint, using a variant of the online algorithm. Finally, simulation studies are provided to evaluate the algorithms empirically.", "arxiv_id": "1611.00559v2", "research_question": "How do nonlinear constraints such as apparent power (quadratic) or nodal voltage limits change the design and performance guarantees of online algorithms for allocating electrical demands compared to problems with only linear constraints?"}
{"doi": "https://doi.org/10.3390/biomedicines11051282", "title": "Efficacy and Safety of Anticoagulant Therapy in COVID-19-Related Pulmonary Embolism with Different Extension", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 6, "authors": ["Claudio Picariello", "Emanuele Nicastri", "Andrea Antinori", "Alessandra Oliva", "Maria Chiara Gatto", "Andrea Garascia", "Enrico Girardi", "Claudia Palazzolo"], "abstract": "The clinical symptoms of pulmonary embolism (PE) are very diverse and non-specific, which makes it difficult to diagnose. In addition, pulmonary embolism has multiple triggers and is one of the major causes of vascular death. Therefore, if it can be detected and treated quickly, it can significantly reduce the risk of death in hospitalized patients. In the detection process, the cost of computed tomography pulmonary angiography (CTPA) is high, and angiography requires the injection of contrast agents, which increase the risk of damage to the patient. Therefore, this study will use a deep learning approach to detect pulmonary embolism in all patients who take a CT image of the chest using a convolutional neural network. With the proposed pulmonary embolism detection system, we can detect the possibility of pulmonary embolism at the same time as the patient's first CT image, and schedule the CTPA test immediately, saving more than a week of CT image screening time and providing timely diagnosis and treatment to the patient.", "arxiv_id": "2206.01344v1", "research_question": "What are the main limitations, potential risks, and clinical pitfalls of using an AI-based system to screen for pulmonary embolism on non-contrast chest CT images before confirmatory testing?"}
{"doi": "https://doi.org/10.1007/s43207-023-00297-2", "title": "A DFT study of electronic structure and optical properties of the pure, doped and co-doped CaZrO3 perovskite for photovoltaic applications", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 13, "authors": ["A. El Badraoui", "S. Dahbi", "H. Ez‐Zahraouy", "N. Tahiri", "O. El Bounagui"], "abstract": "This study investigates the novelty of the crystalline and electronic structure of (Mg,Ti)-doped ZnO and the co-doped Zn1-x-yMgxTiyO structures using Gaussian and plane-wave basis sets, as implemented in the CP2K code. The goal of incorporating low concentration of Mg and Ti into ZnO is to influence its electronic properties without significantly altering its geometrical and crystalline structure. Within the framework of density functional theory (DFT), we analyze various doped and co-doped configurations. Our results show that Ti-doped ZnO exhibits an indirect band gap, while Mg doping preserves the direct semiconductor behavior of ZnO structure, with an increase in band gap energy. Additionally, the co-doped Zn1-x-yMgxTiyO system, at varying concentrations of Ti and Mg, displays minimal lattice deformation. These findings suggest that this material could be a promising candidate for transparent electronic devices, highlighting the importance of understanding the electronic structure of ZnO to optimize its physical properties.", "arxiv_id": "2503.02731v1", "research_question": "How do Mg and Ti dopants typically affect the crystal structure, band gap (direct vs indirect), and electrical/optical properties of ZnO, and what physical mechanisms control those changes?"}
{"doi": "https://doi.org/10.1080/17439760.2023.2218341", "title": "The value of social media language for the assessment of wellbeing: A systematic review and meta-analysis", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 11, "authors": ["Selim Sametoğlu", "Dirk H. M. Pelt", "Johannes C. Eichstaedt", "Lyle Ungar", "Meike Bartels"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare neutral B meson decays into muon pairs constrain extensions of the Standard Model, and which classes of new-physics models are most strongly affected by these results?"}
{"doi": "https://doi.org/10.3390/ijms242115818", "title": "Structural Variation Evolution at the 15q11-q13 Disease-Associated Locus", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 9, "authors": ["Alberto L’Abbate", "Mario Ventura", "Gerardina Chirico", "Annalisa Paparella", "Claudia Rita Catacchio", "Francesca Antonacci", "Donato Palmisano", "Flavia Angela Maria Maggiolini", "David Porubskỳ", "Evan E. Eichler"], "abstract": "Although mutations drive the evolutionary process, the rates at which the mutations occur are themselves subject to evolutionary forces. Our purpose here is to understand the role of selection and random genetic drift in the evolution of mutation rates, and we address this question in asexual populations at mutation-selection equilibrium neglecting selective sweeps. Using a multitype branching process, we calculate the fixation probability of a rare nonmutator in a large asexual population of mutators, and find that a nonmutator is more likely to fix when the deleterious mutation rate of the mutator population is high. Compensatory mutations in the mutator population are found to decrease the fixation probability of a nonmutator when the selection coefficient is large. But, surprisingly, the fixation probability changes nonmonotonically with increasing compensatory mutation rate when the selection is mild. Using these results for the fixation probability and a drift-barrier argument, we find a novel relationship between the mutation rates and the population size. We also discuss the time to fix the nonmutator in an adapted population of asexual mutators, and compare our results with experiments.", "arxiv_id": "1501.03632v2", "research_question": "How do population size, natural selection, and genetic drift together determine the evolutionarily stable mutation rate in asexual populations?"}
{"doi": "https://doi.org/10.1007/s00145-023-09458-2", "title": "Unbounded Predicate Inner Product Functional Encryption from Pairings", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 10, "authors": ["Tapas Pal", "Subhranil Dutta", "Uddipana Dowerah", "Sayantan Mukherjee", "Aikaterini Mitrokotsa"], "abstract": "Pairing-based inner product functional encryption provides an efficient theoretical construction for privacy-preserving edge computing secured by widely deployed elliptic curve cryptography. In this work, an efficient software implementation framework for pairing-based function-hiding inner product encryption (FHIPE) is presented using the recently proposed and widely adopted BLS12-381 pairing-friendly elliptic curve. Algorithmic optimizations provide $\\approx 2.6 \\times$ and $\\approx 3.4 \\times$ speedup in FHIPE encryption and decryption respectively, and extensive performance analysis is presented using a Raspberry Pi 4B edge device. The proposed optimizations enable this implementation framework to achieve performance and ciphertext size comparable to previous work despite being implemented on an edge device with a slower processor and supporting a curve at much higher security level with a larger prime field. Practical privacy-preserving edge computing applications such as encrypted biomedical sensor data classification and secure wireless fingerprint-based indoor localization are also demonstrated using the proposed implementation framework.", "arxiv_id": "2504.02068v1", "research_question": "What are the main trade-offs to consider when deploying pairing-based function-hiding inner product encryption on resource-constrained edge devices, in terms of computational cost, ciphertext size, and chosen elliptic curve security level?"}
{"doi": "https://doi.org/10.1016/j.envres.2023.116119", "title": "Exploration of the mechanism of 2-CP degradation by Acinetobacter sp. stimulated by Lactobacillus plantarum fermentation waste: A bio-waste reuse", "language": "en", "created_date": "2023-05-13T00:00:00", "publication_year": 2023, "cited_by_count": 9, "authors": ["Guotao Chen", "Yuan Ren", "Zhen Zhang", "Han Lin"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B meson decays to muon pairs constrain physics beyond the Standard Model, and which types of new-physics models are most strongly affected by these constraints?"}
{"doi": "https://doi.org/10.1016/j.jde.2023.03.015", "title": "Stability and bifurcation of a reaction-diffusion-advection model with nonlinear boundary condition", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 10, "authors": ["Xingfu Zou", "Zhenzhen Li", "Binxiang Dai"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B-meson decays into muon pairs constrain theories beyond the Standard Model, and can you give concrete examples of the types of new physics that are limited by these measurements?"}
{"doi": "https://doi.org/10.48550/arxiv.2305.09858", "title": "Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 8, "authors": ["Jianpeng Xu", "Jiao Chen", "Nikhil Thakurdesai", "Evren Körpeoğlu", "Kaushiki Nag", "Jason H. D. Cho", "Xiaohan Li", "Luyi Ma", "Kannan Achan", "Sushant Kumar"], "abstract": "Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system performance by providing structured information about entities and their relationships, such as complementary or substitutable relations between products or product types, which can be utilized in recommender systems. However, relation labeling in KGs remains a challenging task due to the dynamic nature of e-commerce domains and the associated cost of human labor. Recently, breakthroughs in Large Language Models (LLMs) have shown surprising results in numerous natural language processing tasks. In this paper, we conduct an empirical study of LLMs for relation labeling in e-commerce KGs, investigating their powerful learning capabilities in natural language and effectiveness in predicting relations between product types with limited labeled data. We evaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets, demonstrating their ability to achieve competitive performance compared to humans on relation labeling tasks using just 1 to 5 labeled examples per relation. Additionally, we experiment with different prompt engineering techniques to examine their impact on model performance. Our results show that LLMs significantly outperform existing KG completion models in relation labeling for e-commerce KGs and exhibit performance strong enough to replace human labeling.", "arxiv_id": "2305.09858v1", "research_question": "What best practices should practitioners follow when using large language models with few-shot prompts to label relations in dynamic e-commerce knowledge graphs, in order to ensure accuracy, scalability, and maintainability?"}
{"doi": "https://doi.org/10.3390/photonics10020121", "title": "Enriching Capacity and Transmission of Hybrid WDM-FSO Link for 5G Mobility", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 21, "authors": ["Yutao Shi", "Ammar Armghan", "Meshari Alsharari", "Khaled Aliqab", "Farman Ali"], "abstract": "The previous link adaptation algorithms on ofdm based systems use equal modulation order for all sub carrier index within a block. For multimedia transmission using ofdm as the modulation technique, unequal constellation is used within one ofdm subcarrier block, a set of subcarriers for audio and another set for video transmissions. A generic model has been shown for such a transmission and link adaptation algorithm has been proposed using EESM (Effective Exponential SNR mapping) method as basic method. Mathematical model has been derived for the channel based on bivariate Gaussian distribution in which the amplitude varies two dimensionally in the same envelope. From the Moment generating function of bivariate distribution, Probability of error has been theoretically derived. Results have been shown for BER performance of an ofdm system using unequal constellation. BER performances have been shown for different values of correlation parameter and fading figure.", "arxiv_id": "1002.1198v1", "research_question": "What are the trade-offs, benefits, and practical considerations of using different modulation orders for subcarriers within an OFDM block (e.g., separating audio and video), and how should link adaptation and error-rate prediction account for varying correlation and fading across those subcarriers?"}
{"doi": "https://doi.org/10.1016/j.ab.2023.115264", "title": "A mechanical HSA biosensor based on multi-field-coupling-mediated magnetic sensitization strategy", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 6, "authors": ["Shengbo Sang", "Yang Ge", "Kun Yang", "Honglie Chen", "Xing Guo", "Haoyu Wang", "Xiushan Dong", "Pengli Xiao", "Dong Zhao"], "abstract": "The 2023-2032 Planetary Science and Astrobiology Decadal Survey Origins, Worlds, and Life recommended that \"NASA develop scientific exploration strategies, as it has for Mars, in areas of broad scientific importance, e.g., Venus... that have an increasing number of U.S. missions and international collaboration opportunities\" (OWL, p.22-10). In NASA's initial responses to that Decadal Survey, the agency asserted that \"...specific scientific exploration strategies should be community generated by bodies such as the Analysis Groups,\" thus placing the onus on the planetary community to generate and support these exploration strategies. In late 2022, the Venus Exploration Analysis Group began a project to develop a new exploration strategy for Venus, reflecting the 2021 selections of the VERITAS, DAVINCI, and EnVision missions and the sweeping comparative planetology recommendations relevant to Venus in Origins, Worlds, and Life.\n  This is that strategy.\n  Taking a broad look at the scientific, technological, and programmatic advances required to address the key outstanding questions that Venus poses, and predicated on VERITAS, DAVINCI, and EnVision flying as planned in the early 2030s, this report outlines a set of actions available to NASA, VEXAG, and the planetary science community at large to establish a sustained program of Venus exploration in the years and decades ahead. Key to this approach is recognizing Venus as a unique setting where multiple, cross-disciplinary, Decadal-level planetary, Earth, heliophysics, and exoplanet science questions can be addressed, as well as being a worthy target of exploration in its own right.\n  This report offers Assessments of the current state of Venus exploration, and Actions for the U.S. and international Venus community, as well as NASA, to consider. This strategy is a living document and should be updated as warranted.", "arxiv_id": "2412.06830v1", "research_question": "What technological advancements and mission capabilities are most critical to enable a sustained, long-term program of surface and atmospheric exploration of Venus?"}
{"doi": "https://doi.org/10.28945/5105", "title": "Key Factors for a Creative Environment in Saudi Arabian Higher Education Institutions", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 9, "authors": ["Osama Sohaib", "Mamdouh Qahl"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B meson decays into muon pairs constrain or inform models of physics beyond the Standard Model?"}
{"doi": "https://doi.org/10.61796/ejheaa.v1i2.106", "title": "The Views of the Faculty on the Effectiveness of Teacher Education Programs in Developing Lifelong Learning Competence", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 28, "authors": ["Jayson R. Miñoza", "Osias Kit T. Kilag", "Millaine Mariel R. Ledesma", "Ann Bernasyl E. Vestal", "Marsha H. Malbas", "John Michael Sasan"], "abstract": "During the past twelve years, NASA's Fermi Gamma-ray Space Telescope has supported a wide range of Education and Public Outreach (E/PO) activities, targeting K-14 students and the general public. The purpose of the Fermi E/PO program is to increase student and public understanding of the science of the high-energy Universe, through inspiring, engaging and educational activities linked to the mission's science objectives. The E/PO program has additional more general goals, including increasing the diversity of students in the Science, Technology, Engineering and Mathematics (STEM) pipeline, and increasing public awareness and understanding of Fermi science and technology. Fermi's multi-faceted E/PO program includes elements in each major outcome category: Higher Education; Elementary and Secondary Education; Informal Education and Public Outreach.", "arxiv_id": "1303.0042v1", "research_question": "What are effective methods for evaluating the impact of space-science education and public-outreach programs on student interest, learning outcomes, and diversity in the STEM pipeline?"}
{"doi": "https://doi.org/10.1063/5.0140014", "title": "Abnormal strain-dependent thermal conductivity in biphenylene monolayer using machine learning interatomic potential", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 19, "authors": ["Gang Zhang", "Dengfeng Li", "Guangyu Yang", "Ping Zhou", "Yanxiao Hu", "Zhanjun Qiu", "Bo-Lin Li"], "abstract": "Data science has become increasingly essential for the production of official statistics, as it enables the automated collection, processing, and analysis of large amounts of data. With such data science practices in place, it enables more timely, more insightful and more flexible reporting. However, the quality and integrity of data-science-driven statistics rely on the accuracy and reliability of the data sources and the machine learning techniques that support them. In particular, changes in data sources are inevitable to occur and pose significant risks that are crucial to address in the context of machine learning for official statistics.\n  This paper gives an overview of the main risks, liabilities, and uncertainties associated with changing data sources in the context of machine learning for official statistics. We provide a checklist of the most prevalent origins and causes of changing data sources; not only on a technical level but also regarding ownership, ethics, regulation, and public perception. Next, we highlight the repercussions of changing data sources on statistical reporting. These include technical effects such as concept drift, bias, availability, validity, accuracy and completeness, but also the neutrality and potential discontinuation of the statistical offering. We offer a few important precautionary measures, such as enhancing robustness in both data sourcing and statistical techniques, and thorough monitoring. In doing so, machine learning-based official statistics can maintain integrity, reliability, consistency, and relevance in policy-making, decision-making, and public discourse.", "arxiv_id": "2306.04338v1", "research_question": "What practical strategies can statistical agencies use to detect, monitor, and mitigate the effects of changes in external data sources on machine-learning models and published statistics?"}
{"doi": "https://doi.org/10.1016/j.cma.2023.116233", "title": "Towards higher-order accurate mass lumping in explicit isogeometric analysis for structural dynamics", "language": "en", "created_date": "2023-07-28T00:00:00", "publication_year": 2023, "cited_by_count": 21, "authors": ["Thi‐Hoa Nguyen", "Dominik Schillinger", "René R. Hiemstra", "Sascha Eisenträger"], "abstract": "We present a mass lumping approach based on an isogeometric Petrov-Galerkin method that preserves higher-order spatial accuracy in explicit dynamics calculations irrespective of the polynomial degree of the spline approximation. To discretize the test function space, our method uses an approximate dual basis, whose functions are smooth, have local support and satisfy approximate bi-orthogonality with respect to a trial space of B-splines. The resulting mass matrix is ``close'' to the identity matrix. Specifically, a lumped version of this mass matrix preserves all relevant polynomials when utilized in a Galerkin projection. Consequently, the mass matrix can be lumped (via row-sum lumping) without compromising spatial accuracy in explicit dynamics calculations. We address the imposition of Dirichlet boundary conditions and the preservation of approximate bi-orthogonality under geometric mappings. In addition, we establish a link between the exact dual and approximate dual basis functions via an iterative algorithm that improves the approximate dual basis towards exact bi-orthogonality. We demonstrate the performance of our higher-order accurate mass lumping approach via convergence studies and spectral analyses of discretized beam, plate and shell models.", "arxiv_id": "2305.12916v3", "research_question": "How does using lumped mass matrices with high-order spline-based discretizations affect the stability limits and allowable time step size in explicit dynamic simulations compared to consistent mass matrices?"}
{"doi": "https://doi.org/10.1021/acs.jmedchem.3c00573", "title": "Imaging Cholinergic Receptors in the Brain by Positron Emission Tomography", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 12, "authors": ["Hualong Fu", "Jing‐Jing Zhang", "Chongzhao Ran", "Ahmed Haider", "Jian Rong", "Zhen Chen", "Steven H. Liang", "Nehal H. Elghazawy", "Jingyin Zhou", "Jiahui Chen", "Yinlong Li", "Weiwei Fang", "Thomas Collier", "Ruofan Lin"], "abstract": "The characterisation of CMB polarisation is one of the next challenge in observationnal cosmology. This is especially true for the so-called B-modes that are at least 3 order of magnitude lower than CMB temperature fluctuations. A precise measurement of the angular power spectrum of these B-modes will give important constraints on inflation parameters. In this talk, I will describe two complementary experiments, BRAIN and CLOVER, dedicated to CMB polarisation measurement. These experiments are proposed to be installed in Dome-C, Antarctica, to take advantage of the extreme dryness of the atmosphere and to allow long integration time.", "arxiv_id": "0412590v2", "research_question": "What makes Antarctic sites like Dome C particularly well-suited for long-duration CMB polarization observations, and what practical logistical or environmental challenges do they present?"}
{"doi": "https://doi.org/10.1016/j.biopha.2023.115394", "title": "Triptolide attenuates pulmonary fibrosis by inhibiting fibrotic extracellular matrix remodeling mediated by MMPs/LOX/integrin", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 19, "authors": ["Jiahui Yan", "Liang Han", "Yaqin Song", "Kai Qin", "Shenghao Tu", "Zhe Chen", "Xin Ba", "Weiji Lin", "Yu Wang", "Ruiyuan Zhang", "Yao Huang", "Tingting Li", "Ying Huang"], "abstract": "We present a multiphase mathematical model for tumor growth which incorporates the remodeling of the extracellular matrix and describes the formation of fibrotic tissue by tumor cells. We also detail a full qualitative analysis of the spatially homogeneous problem, and study the equilibria of the system in order to characterize the conditions under which fibrosis may occur.", "arxiv_id": "0910.4591v1", "research_question": "What are the main biological mechanisms by which tumor cells remodel the extracellular matrix to produce fibrotic tissue, and how can such fibrosis influence tumor progression and response to treatment?"}
{"doi": "https://doi.org/10.1109/tns.2023.3242644", "title": "Total-Ionizing-Dose Effects and Low-Frequency Noise in N-Type Carbon Nanotube Field-Effect Transistors With HfO₂ Gate Dielectrics", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 7, "authors": ["L. W. Massengill", "Andrew M. Aaron", "Patrick K. Darmawi-Iskandar", "J. L. Davidson", "Michael L. Alles", "Jeffery S. Kauppila", "En Xia Zhang", "B. L. Bhuva", "Daniel M. Fleetwood"], "abstract": "Energetic ion irradiation is an effective method for studying how single and multi-shelled carbon nanotubes break apart. The energy from ions is dissipated through both linear and nonlinear processes in the nanotubes, leading to defect formation. Fragmentation occurs via atomic collision cascades and thermal spikes, each described by different theoretical models. Experiments with Cs-irradiated nanotubes support these models, and an information-theoretic approach further explains the fragmentation mechanisms. Sputtered species yield probability distributions, which are analyzed using Shannon entropy and fractal dimension to assess spatial characteristics. Kullback-Leibler divergence helps identify the diversity of emission mechanisms. Together, thermal and information-theoretic models clarify and distinguish the roles of collision cascades and thermal spikes in nanotube fragmentation.", "arxiv_id": "2511.15467v1", "research_question": "How do collision cascades and thermal spikes differ in the physical mechanisms by which energetic ions cause defects and fragmentation in carbon nanotubes, and what experimental signatures can be used to distinguish between these two processes?"}
{"doi": "https://doi.org/10.1007/s12325-023-02476-3", "title": "Proton Pump Inhibitors, Kidney Damage, and Mortality: An Updated Narrative Review", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 23, "authors": ["Chelsi J. Flanagan", "Katelyn Parker", "Elyse M. Cornett", "Edwin Dudossat", "Olga Willett", "Natalie W. Wu", "Alan D. Kaye", "Hirni Patel", "Anam Dharani", "Amber N. Edinoff", "Adam M. Kaye", "Lauren Linquest"], "abstract": "Automated segmentation of kidneys and kidney tumors is an important step in quantifying the tumor's morphometrical details to monitor the progression of the disease and accurately compare decisions regarding the kidney tumor treatment. Manual delineation techniques are often tedious, error-prone and require expert knowledge for creating unambiguous representation of kidneys and kidney tumors segmentation. In this work, we propose an end-to-end boundary aware fully Convolutional Neural Networks (CNNs) for reliable kidney and kidney tumor semantic segmentation from arterial phase abdominal 3D CT scans. We propose a segmentation network consisting of an encoder-decoder architecture that specifically accounts for organ and tumor edge information by devising a dedicated boundary branch supervised by edge-aware loss terms. We have evaluated our model on 2019 MICCAI KiTS Kidney Tumor Segmentation Challenge dataset and our method has achieved dice scores of 0.9742 and 0.8103 for kidney and tumor repetitively and an overall composite dice score of 0.8923.", "arxiv_id": "1909.06684v1", "research_question": "How do boundary-aware or edge-sensitive segmentation techniques improve the accuracy of automated kidney and tumor delineation, especially for small or irregularly shaped tumors, compared to standard segmentation approaches?"}
{"doi": "https://doi.org/10.1021/acs.jcim.3c00194", "title": "The Conformational Transitions and Dynamics of <i>Burkholderia cepacia</i> Lipase Regulated by Water–Oil Interfaces", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 13, "authors": ["Wanqian Dong", "Jiamin Gao", "Zhenhao Liu", "Kuan Liang", "Rui Zhou", "Zhengyu Shu", "Mojie Duan"], "abstract": "Hydrophobic interfaces have unique physicochemical properties and are used in various chemical products such as food, cosmetics, soap, and medicine and technologies such as pan coating and ski wax. In this chapter, we describe the fundamental concept of hydrophobic interfaces and explain their ion adsorption and zeta potential by using experimental data from the literature. Thus far, these electrical properties are considered universal for solid/water, liquid/water, and gas/water interfaces; however, a careful comparison in this chapter will reveal significant differences among them. To confirm that the affinity of H$^+$ ions for all hydrophobic interfaces is stronger than that of OH$^-$ ions, more experimental data on hydrophobic liquid/water and solid/water interfaces are required.", "arxiv_id": "2508.13737v1", "research_question": "What experimental techniques and measurement approaches are best suited to determine and compare ion adsorption and zeta potential at solid/water, liquid/water, and gas/water hydrophobic interfaces, and what are the main limitations of those methods?"}
{"doi": "https://doi.org/10.33073/pjm-2023-023", "title": "The Molecular Approaches and Challenges of <i>Streptococcus pneumoniae</i> Serotyping for Epidemiological Surveillance in the Vaccine Era", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 13, "authors": ["Niazlin Mohd Taib", "Hazmin Hazman", "Nurshahira Sulaiman", "Siti Norbaya Masri", "James John", "Nurul Asyikin Abdul Rahman", "Mohd Nasir Mohd Desa"], "abstract": "We aim to characterize the U-band variability of young brown dwarfs in the Taurus Molecular Cloud and discuss its origin. We used the XMM-Newton Extended Survey of the Taurus Molecular Cloud, where a sample of 11 young bona fide brown dwarfs (spectral type later than M6) were observed simultaneously in X-rays with XMM-Newton and in the U-band with the XMM-Newton Optical/UV Monitor (OM). We obtained upper limits to the U-band emission of 10 brown dwarfs (U>19.6-20.6 mag), whereas 2MASSJ04141188+2811535 was detected in the U-band. Remarkably, the magnitude of this brown dwarf increased regularly from U~19.5 mag at the beginning of the observation, peaked 6h later at U~18.4 mag, and then decreased to U~18.65 mag in the next 2h. The first OM U-band measurement is consistent with the quiescent level observed about one year later thanks to ground follow-up observations. This brown dwarf was not detected in X-rays by XMM-Newton during the OM observation. We discuss the possible sources of U-band variability for this young brown dwarf, namely a magnetic flare, non-steady accretion onto the substellar surface, and rotational modulation of a hot spot. We conclude that this event is related to accretion from a circumsubstellar disk, where the mass accretion rate was about a factor of 3 higher than during the quiescent level.", "arxiv_id": "0609027v1", "research_question": "How can astronomers distinguish whether U-band/UV variability in young brown dwarfs is caused by magnetic flares, changes in accretion rate, or rotational modulation of hot spots, using observational data?"}
{"doi": "https://doi.org/10.1097/ijg.0000000000002247", "title": "Zonulopathy Identified During Cataract Extraction in Patients With Primary Angle Closure Disease", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 8, "authors": ["Shuo Zhang", "Mugen Liu", "Jing Song", "Chunyan Qiao", "Hui Zhang", "Kai Cao", "Dongjun Li", "Ningli Wang", "Jing Liang", "Ye Zhang"], "abstract": "Primary angle-closure disease (PACD) is a severe retinal disease, which might cause irreversible vision loss. In clinic, accurate identification of angle-closure and localization of the scleral spur's position on anterior segment optical coherence tomography (AS-OCT) is essential for the diagnosis of PACD. However, manual delineation might confine in low accuracy and low efficiency. In this paper, we propose an efficient and accurate end-to-end architecture for angle-closure classification and scleral spur localization. Specifically, we utilize a revised ResNet152 as our backbone to improve the accuracy of the angle-closure identification. For scleral spur localization, we adopt EfficientNet as encoder because of its powerful feature extraction potential. By combining the skip-connect module and pyramid pooling module, the network is able to collect semantic cues in feature maps from multiple dimensions and scales. Afterward, we propose a novel keypoint registration loss to constrain the model's attention to the intensity and location of the scleral spur area. Several experiments are extensively conducted to evaluate our method on the angle-closure glaucoma evaluation (AGE) Challenge dataset. The results show that our proposed architecture ranks the first place of the classification task on the test dataset and achieves the average Euclidean distance error of 12.00 pixels in the scleral spur localization task.", "arxiv_id": "1910.10414v1", "research_question": "What are the main challenges in accurately localizing the scleral spur in anterior segment OCT images, and how do those challenges impact the reliability of automated angle-closure diagnosis in clinical practice?"}
{"doi": "https://doi.org/10.3390/land12051004", "title": "Factors Influencing Four Decades of Forest Change in Guizhou Province, China", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 8, "authors": ["Qiang Li", "Zilong Xia", "Ruishan Chen", "Xiaona Guo", "Michael E. Meadows", "Zhenzhen Pan"], "abstract": "Understanding historical forest dynamics, specifically changes in forest biomass and carbon stocks, has become critical for assessing current forest climate benefits and projecting future benefits under various policy, regulatory, and stewardship scenarios. Carbon accounting frameworks based exclusively on national forest inventories are limited to broad-scale estimates, but model-based approaches that combine these inventories with remotely sensed data can yield contiguous fine-resolution maps of forest biomass and carbon stocks across landscapes over time. Here we describe a fundamental step in building a map-based stock-change framework: mapping historical forest biomass at fine temporal and spatial resolution (annual, 30m) across all of New York State (USA) from 1990 to 2019, using freely available data and open-source tools.\n  Using Landsat imagery, US Forest Service Forest Inventory and Analysis (FIA) data, and off-the-shelf LiDAR collections we developed three modeling approaches for mapping historical forest aboveground biomass (AGB): training on FIA plot-level AGB estimates (direct), training on LiDAR-derived AGB maps (indirect), and an ensemble averaging predictions from the direct and indirect models. Model prediction surfaces (maps) were tested against FIA estimates at multiple scales. All three approaches produced viable outputs, yet tradeoffs were evident in terms of model complexity, map accuracy, saturation, and fine-scale pattern representation. The resulting map products can help identify where, when, and how forest carbon stocks are changing as a result of both anthropogenic and natural drivers alike. These products can thus serve as inputs to a wide range of applications including stock-change assessments, monitoring reporting and verification frameworks, and prioritizing parcels for protection or enrollment in improved management programs.", "arxiv_id": "2304.02632v1", "research_question": "What are the main sources of uncertainty when producing fine-resolution, multi-decadal maps of forest biomass using combinations of satellite imagery, inventory plots, and LiDAR, and how can those uncertainties be quantified and reduced for use in carbon accounting?"}
{"doi": "https://doi.org/10.1098/rstb.2022.0222", "title": "Early life impacts of maternal obesity: a window of opportunity to improve the health of two generations", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 13, "authors": ["Susan E. Ozanne", "Laura Dearden"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare neutral B meson decays into muon pairs constrain physics beyond the Standard Model, and which types of new-physics scenarios are most strongly affected by these results?"}
{"doi": "https://doi.org/10.1136/bmjopen-2022-061023", "title": "Optimal duration of antibiotic treatment for community-acquired pneumonia in adults: a systematic review and duration-effect meta-analysis", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 20, "authors": ["Tasnim Hamza", "Edoardo G. Ostinelli", "Satoshi Funada", "Yuki Kataoka", "Yan Luo", "Toshi A. Furukawa", "Akira Ōnishi", "Yuki Furukawa"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare B meson decays to muon pairs constrain theories beyond the Standard Model, and what would a significant deviation from the predicted branching fractions imply for new physics?"}
{"doi": "https://doi.org/10.1515/opis-2022-0141", "title": "Social Unrest Prediction Through Sentiment Analysis on Twitter Using Support Vector Machine: Experimental Study on Nigeria’s #EndSARS", "language": "en", "created_date": "2023-03-10T00:00:00", "publication_year": 2023, "cited_by_count": 13, "authors": ["Eniafe Festus Ayetiran", "Temidayo Michael Oladele"], "abstract": "We present work on sentiment analysis in Twitter for Macedonian. As this is pioneering work for this combination of language and genre, we created suitable resources for training and evaluating a system for sentiment analysis of Macedonian tweets. In particular, we developed a corpus of tweets annotated with tweet-level sentiment polarity (positive, negative, and neutral), as well as with phrase-level sentiment, which we made freely available for research purposes. We further bootstrapped several large-scale sentiment lexicons for Macedonian, motivated by previous work for English. The impact of several different pre-processing steps as well as of various features is shown in experiments that represent the first attempt to build a system for sentiment analysis in Twitter for the morphologically rich Macedonian language. Overall, our experimental results show an F1-score of 92.16, which is very strong and is on par with the best results for English, which were achieved in recent SemEval competitions.", "arxiv_id": "2109.13725v1", "research_question": "What are the main challenges in performing sentiment analysis on morphologically rich languages in social media texts, and what general strategies can be used to address them?"}
{"doi": "https://doi.org/10.1038/s41420-023-01696-4", "title": "Secreted miR-210-3p, miR-183-5p and miR-96-5p reduce sensitivity to docetaxel in prostate cancer cells", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 10, "authors": ["Alberto Mercatanti", "Letizia Pitto", "Monica Evangelista", "Maristella Canovai", "Marco Pellegrini", "Francesca Marrocolo", "Valentina Casieri", "Romina D’Aurizio", "Sergio Bracarda", "Vincenzo Lionetti", "Milena Rizzo"], "abstract": "Prostate cancer (PCa) is the most prevalent cancer among men in the United States, accounting for nearly 300,000 cases, 29\\% of all diagnoses and 35,000 total deaths in 2024. Traditional screening methods such as prostate-specific antigen (PSA) testing and magnetic resonance imaging (MRI) have been pivotal in diagnosis, but have faced limitations in specificity and generalizability. In this paper, we explore the potential of enhancing PCa gland segmentation using a novel MRI modality called synthetic correlated diffusion imaging (CDI$^s$). We employ several state-of-the-art deep learning models, including U-Net, SegResNet, Swin UNETR, Attention U-Net, and LightM-UNet, to segment prostate glands from a 200 CDI$^s$ patient cohort. We find that SegResNet achieved superior segmentation performance with a Dice-Sorensen coefficient (DSC) of $76.68 \\pm 0.8$. Notably, the Attention U-Net, while slightly less accurate (DSC $74.82 \\pm 2.0$), offered a favorable balance between accuracy and computational efficiency. Our findings demonstrate the potential of deep learning models in improving prostate gland segmentation using CDI$^s$ to enhance PCa management and clinical support.", "arxiv_id": "2501.09185v2", "research_question": "How could more accurate automatic prostate gland segmentation from MRI change clinical workflow, diagnosis, and treatment planning for prostate cancer?"}
{"doi": "https://doi.org/10.4324/9781003366782-7", "title": "EU border technologies and the co-production of security ‘problems’ and ‘solutions’", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 9, "authors": ["Maria Gabrielsen Jumbert", "Bruno Oliveira Martins"], "abstract": "Cloning spare parts and entities of mass products is an old and serious unsolved problem for the automotive industry. The economic losses in addition to a loss of know-how and IP theft as well as security and safety threats are huge in all dimensions. This presentation gives an overview of the traditional state of the art on producing clone resistant electronic units in the last two decades. A survey is attempting to demonstrate the techniques so far known as Physically Unclonable Functions PUFs showing their advantages and drawbacks. The necessity for fabricating mechatronic-security in the vehicular environment is emerging to become a vital requirement for new automotive security regulations (legal regulations) in the near future. The automotive industry is facing a challenge to produce low-cost and highly safe and secure networked automotive systems. The emerging networked smart traffic environment is offering new safety services and creating at the same time new needs and threats in a highly networked world. There is a crying need for automotive security that approaches the level of the robust biological security for cars as dominating mobility actors in the modern smart life environment. Possible emerging technologies allowing embedding practical mechatronic-security modules as a low-cost digital alternative are presented. Such digital clone-resistant mechatronic-units (as Electronic Control Units ECUs) may serve as smart security anchors for the automotive environment in the near future. First promising initial results are also presented.", "arxiv_id": "1805.07570v1", "research_question": "What are the main technical and practical challenges in designing low-cost, clone-resistant hardware modules for vehicles, and what strategies can be used to address them?"}
{"doi": "https://doi.org/10.1016/j.pharmthera.2023.108428", "title": "17β-hydroxysteroid dehydrogenases in the progression of nonalcoholic fatty liver disease", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 18, "authors": ["Zong‐Gen Peng", "Meixi Wang"], "abstract": "Integrating deep learning with clinical expertise holds great potential for addressing healthcare challenges and empowering medical professionals with improved diagnostic tools. However, the need for annotated medical images is often an obstacle to leveraging the full power of machine learning models. Our research demonstrates that by combining synthetic images, generated using diffusion models, with real images, we can enhance nonalcoholic fatty liver disease (NAFLD) classification performance even in low-data regime settings. We evaluate the quality of the synthetic images by comparing two metrics: Inception Score (IS) and Fréchet Inception Distance (FID), computed on diffusion- and generative adversarial network (GAN)-generated images. Our results show superior performance for the diffusion-generated images, with a maximum IS score of $1.90$ compared to $1.67$ for GANs, and a minimum FID score of $69.45$ compared to $100.05$ for GANs. Utilizing a partially frozen CNN backbone (EfficientNet v1), our synthetic augmentation method achieves a maximum image-level ROC AUC of $0.904$ on a NAFLD prediction task.", "arxiv_id": "2307.06507v2", "research_question": "How can synthetic medical images generated by deep generative models be validated and regulated to ensure they are safe, unbiased, and clinically useful when used to augment training data for diagnostic AI systems?"}
{"doi": "https://doi.org/10.1109/lmwt.2023.3279573", "title": "Broadband Asymmetric GaAs MMIC Doherty Power Amplifiers With Simplified Peaking Matching Network and Output Capacitance Compensation", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 10, "authors": ["Yana Zheng", "Yongle Wu", "Weimin Wang", "Weijuan Chen"], "abstract": "In this paper, a quasi-asymmetric Doherty power amplifier (PA) is designed without load modulation using the GaAs 0.25μm pHEMT technology to reach an enlarged output power back-off (OPBO) with circuitry solutions in order to overcome technology restrictions. To prevent power leakage in auxiliary PA (PAaux) due to its extremely large off-state impedance, a Wilkinson power combiner is added to the output. Moreover, an input asymmetric power divider is designed to guarantee that no considerable power is delivered to main PA (PAmain) in the high-power region to make it saturated. A two-section matching network is proposed for PAmain, which simultaneously compensates for phase differences of the main and auxiliary amplification paths. To control the significant impedance variation of PAaux versus sweeping power and the different impedance trajectories of the main and auxiliary amplification paths, the bias and dimension selection of PAaux are analyzed to reach the desired output power profile versus input power. These methods overcome impedance variations and linearity degradation. To achieve the aimed 10% fractional bandwidth, appropriate low-quality LC-networks are selected as matching networks. The simulation results indicate the utility of the proposed structure for microwave link applications. Continuous-wave simulations imply that the Doherty PA has a 33dBm maximum output power and a 13.5dB power gain with less than 1dB power gain compression in the desired frequency range (7.6-8.4GHz). The drain efficiency of 30% at the highest input power, minimum of second and third harmonic powers of -140dBm and -130dBm, respectively, and OPBO of 7.5dB are also obtained.", "arxiv_id": "2005.02715v1", "research_question": "What circuit-level techniques are commonly used to extend the output power back-off of Doherty power amplifiers while preserving efficiency and linearity?"}
{"doi": "https://doi.org/10.53346/wjapls.2023.4.1.0054", "title": "Long COVID: An unpredicted multisystem syndrome of COVID-19 disease", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 10, "authors": ["GAJANAN GODHALI", "NEEL TANDEL", "SHITAL VISHNU PATIL"], "abstract": "Clinicians in the frontline need to assess quickly whether a patient with symptoms indeed has COVID-19 or not. The difficulty of this task is exacerbated in low resource settings that may not have access to biotechnology tests. Furthermore, Tuberculosis (TB) remains a major health problem in several low- and middle-income countries and its common symptoms include fever, cough and tiredness, similarly to COVID-19. In order to help in the detection of COVID-19, we propose the extraction of deep features (DF) from chest X-ray images, a technology available in most hospitals, and their subsequent classification using machine learning methods that do not require large computational resources. We compiled a five-class dataset of X-ray chest images including a balanced number of COVID-19, viral pneumonia, bacterial pneumonia, TB, and healthy cases. We compared the performance of pipelines combining 14 individual state-of-the-art pre-trained deep networks for DF extraction with traditional machine learning classifiers. A pipeline consisting of ResNet-50 for DF computation and ensemble of subspace discriminant classifier was the best performer in the classification of the five classes, achieving a detection accuracy of 91.6+ 2.6% (accuracy + 95% Confidence Interval). Furthermore, the same pipeline achieved accuracies of 98.6+1.4% and 99.9+0.5% in simpler three-class and two-class classification problems focused on distinguishing COVID-19, TB and healthy cases; and COVID-19 and healthy images, respectively. The pipeline was computationally efficient requiring just 0.19 second to extract DF per X-ray image and 2 minutes for training a traditional classifier with more than 2000 images on a CPU machine. The results suggest the potential benefits of using our pipeline in the detection of COVID-19, particularly in resource-limited settings and it can run with limited computational resources.", "arxiv_id": "2007.08223v1", "research_question": "How reliable are chest X-ray–based machine learning systems at distinguishing COVID-19 from other respiratory diseases (such as tuberculosis and bacterial/viral pneumonia) in low-resource clinical settings, and what are the common sources of diagnostic errors and limitations?"}
{"doi": "https://doi.org/10.1007/s13205-023-03525-y", "title": "Vitamin B6, B12 and folate modulate deregulated pathways and protein aggregation in yeast model of Huntington disease", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 11, "authors": ["Venketesh Sivaramakrishnan", "Bibha Choudhary", "R. Saiswaroop", "Sai Sanwid Pradhan", "Durga Prasad Patnana", "Meghana Manjunath", "Sai Phalguna Kanikaram", "K. Raksha Rao"], "abstract": "The presence of an expanded polyglutamine produces a toxic gain of function in huntingtin. Protein aggregation resulting from this gain of function is likely to be the cause of neuronal death. Two main mechanisms of aggregation have been proposed: hydrogen bonding by polar-zipper formation and covalent bonding by transglutaminase-catalyzed cross-linking. In cell culture models of Huntington's disease, aggregates are mostly stabilized by hydrogen bonds, but covalent bonds are also likely to occur. Nothing is known about the nature of the bonds that stabilize the aggregates in the brain of patients with Huntington's disease. It seems that the nature of the bond stabilizing the aggregates is one of the most important questions, as the answer would condition the therapeutic approach to Huntington's disease.", "arxiv_id": "2511.04174v1", "research_question": "What experimental methods can be used to determine whether protein aggregates in human neurodegenerative disease are stabilized mainly by hydrogen bonds (e.g., polar zippers) or by covalent cross-links (e.g., transglutaminase-mediated)?"}
{"doi": "https://doi.org/10.3390/rs15174266", "title": "High Spatial Resolution Fractional Vegetation Coverage Inversion Based on UAV and Sentinel-2 Data: A Case Study of Alpine Grassland", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 14, "authors": ["Guoqing Zhou", "Renjie Huang", "Shuhua Yi", "Haotian You", "Yu Qin", "Guangrui Zhong", "Xiaowen Han", "Jianjun Chen"], "abstract": "A joint measurement is presented of the branching fractions $B^0_s\\toμ^+μ^-$ and $B^0\\toμ^+μ^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\toμ^+μ^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\toμ^+μ^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.", "arxiv_id": "1411.4413v2", "research_question": "How do precise measurements of rare decays like Bs→μ+μ− and B0→μ+μ− constrain extensions of the Standard Model, and which classes of new-physics models are most strongly affected by such measurements?"}
{"doi": "https://doi.org/10.1007/s12562-022-01666-2", "title": "The impact of fish farming on phosphorus loading of surface sediment in coastal complex aquaculture", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 8, "authors": ["Kuninao Tada", "Kazuhiko Ichimi", "Hitomi Yamaguchi", "Masatoshi Nakakuni", "Jidapa Koomklang"], "abstract": "Accurate phenotypic analysis in aquaculture breeding necessitates the quantification of subtle morphological phenotypes. Existing datasets suffer from limitations such as small scale, limited species coverage, and inadequate annotation of keypoints for measuring refined and complex morphological phenotypes of fish body parts. To address this gap, we introduce FishPhenoKey, a comprehensive dataset comprising 23,331 high-resolution images spanning six fish species. Notably, FishPhenoKey includes 22 phenotype-oriented annotations, enabling the capture of intricate morphological phenotypes. Motivated by the nuanced evaluation of these subtle morphologies, we also propose a new evaluation metric, Percentage of Measured Phenotype (PMP). It is designed to assess the accuracy of individual keypoint positions and is highly sensitive to the phenotypes measured using the corresponding keypoints. To enhance keypoint detection accuracy, we further propose a novel loss, Anatomically-Calibrated Regularization (ACR), that can be integrated into keypoint detection models, leveraging biological insights to refine keypoint localization. Our contributions set a new benchmark in fish phenotype analysis, addressing the challenges of precise morphological quantification and opening new avenues for research in sustainable aquaculture and genetic studies. Our dataset and code are available at https://github.com/WeizhenLiuBioinform/Fish-Phenotype-Detect.", "arxiv_id": "2405.12476v2", "research_question": "What are effective ways to incorporate anatomical priors or biological knowledge into keypoint detection models to improve localization of subtle morphological landmarks in fish?"}
{"doi": "https://doi.org/10.1016/j.fuel.2023.128965", "title": "Comparison of probabilistic jet fuel property models for the fuel screening and design", "language": "en", "created_date": "2025-10-10T00:00:00", "publication_year": 2023, "cited_by_count": 9, "authors": ["Bastian Rauch", "Uwe Bauder", "Manfred Aigner", "Clemens Hall"], "abstract": "Liquid transportation fuels require costly and time-consuming tests to characterize metrics, such as Research Octane Number (RON) for gasoline. If fuel sale restrictions requiring use of standard Cooperative Fuel Research testing procedures do not apply, these tests may be avoided by using multivariate statistical models to predict RON and other quantities. Here we show that an accurate statistical model for the RON of gasoline and gasoline-like fuels can be constructed by ensuring the representation of key functional groups in the spectroscopic data set are used to train the model. We found that a principal component regression model for RON based on IR absorbance and informed using neat and 134 mixtures of n-heptane, isooctane, toluene, ethanol, methylcyclohexane, and 1-hexene could predict RON for the 10 Coordinating Research Council Fuels for Advanced Combustion Engine (FACE) gasolines and 12 FACE gasoline blends with ethanol within 34.8+/-36.1 on average and 51.2 in the worst case. We next studied the effect of adding 28 additional minor components found in the FACE gasolines to the statistical model, and determined that it was necessary to add additional representatives of the branched alkane and aromatics classes to reduce model error. For example, adding 2,3-dimethylpentane and xylene to the previous model allowed it to predict RON for the 22 target fuels within 0.3+/-4.4 on average and 7.9 in the worst case. However, we determined that the specific choice of fuel in those classes mattered less than ensuring the representation of the relevant functional group. This work builds upon previous efforts by creating models informed by neat and surrogate fuels---rather than complex real fuels---that could predict the performance of complex unknown fuels.", "arxiv_id": "1606.07122v1", "research_question": "How can one select a minimal set of representative surrogate compounds and spectroscopic features to reliably train multivariate models that predict octane rating (RON) for complex gasoline blends, and what practical limitations or sources of error should be expected when applying such models to unknown fuels?"}
